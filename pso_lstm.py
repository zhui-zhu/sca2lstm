# PSO-LSTMé€‚é…ç‰ˆæœ¬ - å•æµåŸŸ22001ï¼Œä»…ä½¿ç”¨discharge_vol
# ç›´æ¥è¯»å–CSVæ–‡ä»¶ï¼Œæ— éœ€å½’ä¸€åŒ–ï¼Œä¼˜åŒ–LSTMè¶…å‚æ•°
# Usage: python pso_lstm.py --lead_time 1

import os
import sys
import argparse
import json
import datetime as dt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_squared_error
from typing import Tuple
import matplotlib.pyplot as plt

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------- Utilities -------------------------

def set_seed(seed: int = 42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def nse(obs, sim):
    obs = np.asarray(obs).flatten()
    sim = np.asarray(sim).flatten()
    denom = np.sum((obs - np.mean(obs)) ** 2)
    if denom == 0:
        return -np.inf
    return 1 - np.sum((obs - sim) ** 2) / denom

def rmse(obs, sim):
    return float(np.sqrt(mean_squared_error(np.asarray(obs).flatten(),
                                            np.asarray(sim).flatten())))

def bias_pct(obs, sim):
    obs = np.asarray(obs).flatten()
    sim = np.asarray(sim).flatten()
    s_obs = np.sum(obs)
    if s_obs == 0:
        return np.nan
    return float((np.sum(sim - obs) / s_obs) * 100.0)

def log(level: str, message: str):
    """ç®€å•çš„æ—¥å¿—å‡½æ•°"""
    timestamp = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {level}: {message}")

# ------------------------- æ•°æ®åŠ è½½å‡½æ•° -------------------------

def load_discharge_data(basin_id: str = "22001") -> np.ndarray:
    """
    ç›´æ¥ä»CSVæ–‡ä»¶åŠ è½½æŒ‡å®šæµåŸŸçš„discharge_volæ•°æ®
    Args:
        basin_id: æµåŸŸID, é»˜è®¤ä¸º22001
    Returns:
        discharge_series: discharge_volæ—¶é—´åºåˆ—æ•°æ®
    """
    csv_path = f"./datasets/CAMELS_GB/CAMELS_GB_timeseries/CAMELS_GB_hydromet_timeseries_{basin_id}_19701001-20150930.csv"
    
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
    
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(csv_path, parse_dates=['date'])
    df = df.sort_values('date')  # æŒ‰æ—¶é—´æ’åº
    
    # æå–discharge_volåˆ—
    discharge_series = df['discharge_vol'].values
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    if np.isnan(discharge_series).any():
        print(f"è­¦å‘Šï¼šå‘ç°{np.isnan(discharge_series).sum()}ä¸ªç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‰å‘å¡«å……å¤„ç†")
        # ç®€å•çš„å‰å‘å¡«å……å¤„ç†ç¼ºå¤±å€¼
        discharge_series = pd.Series(discharge_series).ffill().values
    
    return discharge_series

def create_sequences(discharge_series: np.ndarray, time_steps: int, lead_time: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    åˆ›å»ºLSTMè®­ç»ƒåºåˆ—
    Args:
        discharge_series: discharge_volæ—¶é—´åºåˆ—
        time_steps: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå†å²æ—¶é—´æ­¥ï¼‰
        lead_time: é¢„æµ‹æ­¥é•¿ï¼ˆæœªæ¥æ—¶é—´æ­¥ï¼‰
    Returns:
        X: è¾“å…¥åºåˆ—ï¼Œshape=(N_samples, time_steps, 1)
        y: è¾“å‡ºåºåˆ—ï¼Œshape=(N_samples, lead_time)
    """
    X, y = [], []
    
    # æ€»åºåˆ—é•¿åº¦éœ€æ±‚
    total_length = time_steps + lead_time
    
    for i in range(len(discharge_series) - total_length + 1):
        # è¾“å…¥åºåˆ—ï¼šè¿‡å»time_stepsä¸ªæ—¶é—´æ­¥
        x_seq = discharge_series[i:i + time_steps]
        # è¾“å‡ºåºåˆ—ï¼šæœªæ¥lead_timeä¸ªæ—¶é—´æ­¥
        y_seq = discharge_series[i + time_steps:i + time_steps + lead_time]
        
        X.append(x_seq.reshape(-1, 1))  # æ·»åŠ ç‰¹å¾ç»´åº¦
        y.append(y_seq)
    
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)

def split_train_val(X: np.ndarray, y: np.ndarray, train_ratio: float = 0.8) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    Args:
        X: è¾“å…¥åºåˆ—
        y: è¾“å‡ºåºåˆ—  
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
    Returns:
        X_train, X_val, y_train, y_val
    """
    split_idx = int(len(X) * train_ratio)
    
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    return X_train, X_val, y_train, y_val

# ------------------------- Model -------------------------

class RunoffLSTM(nn.Module):
    """ç®€åŒ–ç‰ˆLSTM - ä»…ä½¿ç”¨discharge_volå†å²æ•°æ®é¢„æµ‹æœªæ¥"""
    def __init__(self, time_steps: int, hidden_size: int, lead_time: int = 1, num_layers: int = 1):
        super().__init__()
        self.lead_time = lead_time
        # è¾“å…¥ç»´åº¦ä¸º1ï¼ˆåªæœ‰discharge_volä¸€ä¸ªç‰¹å¾ï¼‰
        self.lstm = nn.LSTM(input_size=1,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True)
        self.fc = nn.Linear(hidden_size, lead_time)  # è¾“å‡ºç»´åº¦ä¸ºlead_time

    def forward(self, x):
        # x: [B, T, 1]
        out, _ = self.lstm(x)      # [B, T, H]
        out = out[:, -1, :]        # last time step
        out = self.fc(out)         # [B, lead_time]
        return out

def train_eval_lstm(X_train, y_train, X_val, y_val,
                    batch_size: int = 64,
                    hidden_size: int = 64,
                    max_epochs: int = 25,
                    verbose: bool = False) -> float:
    """Return validation NSE (higher is better). (ç”¨äº PSO é˜¶æ®µï¼Œä¸ç”»å›¾)"""
    if len(X_train) < 5 or len(X_val) < 3:
        return -9999

    # ç¡®ä¿yæ˜¯2Dæ•°ç»„ (N_samples, 1)
    if y_train.ndim == 1:
        y_train = y_train.reshape(-1, 1)
    if y_val.ndim == 1:
        y_val = y_val.reshape(-1, 1)

    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                             torch.tensor(y_train, dtype=torch.float32))
    val_ds   = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                             torch.tensor(y_val, dtype=torch.float32))

    bs_train = max(4, min(batch_size, len(train_ds)))
    bs_val   = max(4, min(batch_size, len(val_ds)))

    train_loader = DataLoader(train_ds, batch_size=bs_train, shuffle=True, drop_last=False)
    val_loader   = DataLoader(val_ds,   batch_size=bs_val,   shuffle=False, drop_last=False)

    model = RunoffLSTM(time_steps=X_train.shape[1], hidden_size=hidden_size, lead_time=y_train.shape[1]).to(DEVICE)
    crit = nn.MSELoss()
    opt  = torch.optim.Adam(model.parameters(), lr=1e-3)

    best_val_nse = -np.inf
    train_losses, val_losses = [], []
    
    for epoch in range(max_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            opt.zero_grad()
            pred = model(xb)
            loss = crit(pred, yb)
            loss.backward()
            opt.step()
            train_loss += loss.item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                pred = model(xb)
                val_loss += crit(pred, yb).item()
                y_true.append(yb.cpu().numpy())
                y_pred.append(pred.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        val_nse = nse(y_true, y_pred)
        
        if verbose:
            if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == max_epochs - 1:
                print(f"  Epoch {epoch+1:02d}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val NSE: {val_nse:.4f}")
        
        best_val_nse = max(best_val_nse, val_nse)

    return best_val_nse

def train_evaluate(X_train, y_train, X_val, y_val,
                   time_steps, batch_size, hidden_size,
                   epochs, plot=True, save_path=None, verbose=True):
    """
    è®­ç»ƒå¹¶è¯„ä¼°LSTMæ¨¡å‹ï¼Œæ”¯æŒå¤šlead_timeè¾“å‡º
    """
    # ç¡®ä¿yæ˜¯2Dæ•°ç»„ (N_samples, lead_time)
    if y_train.ndim == 1:
        y_train = y_train.reshape(-1, 1)
    if y_val.ndim == 1:
        y_val = y_val.reshape(-1, 1)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                             torch.tensor(y_train, dtype=torch.float32))
    val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                           torch.tensor(y_val, dtype=torch.float32))

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)

    # åˆå§‹åŒ–æ¨¡å‹ - è¾“å…¥ç»´åº¦ä¸º1ï¼ˆåªæœ‰discharge_volï¼‰
    model = RunoffLSTM(time_steps=time_steps, hidden_size=hidden_size, lead_time=y_train.shape[1]).to(DEVICE)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # è®­ç»ƒå¾ªç¯
    train_losses, val_losses = [], []
    best_val_rmse = np.inf

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                pred = model(xb)
                val_loss += criterion(pred, yb).item()
                y_true.append(yb.cpu().numpy())
                y_pred.append(pred.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_losses.append(train_loss)
        val_losses.append(val_loss)

        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        val_rmse = rmse(y_true, y_pred)
        val_nse = nse(y_true, y_pred)

        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            if save_path:
                torch.save(model.state_dict(), save_path)

        if verbose and ((epoch + 1) % 10 == 0 or epoch == 0 or epoch == epochs - 1):
            print(f"Epoch {epoch+1:03d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.3f} | Val NSE: {val_nse:.3f}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.title(f'Training Curve - Lead Time: {y_train.shape[1]}')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path.replace('.pth', '_training_curve.png'))
        plt.show()

    return best_val_rmse, val_nse

# ------------------------- PSO -------------------------

class Particle:
    def __init__(self, dim, lb, ub):
        self.dim = dim
        self.lb = np.array(lb)
        self.ub = np.array(ub)
        self.position = np.random.uniform(self.lb, self.ub)
        self.velocity = np.zeros(dim)
        self.best_position = self.position.copy()
        self.best_score = -np.inf
        self.score = -np.inf

def pso_optimize(objective_func, dim, lb, ub, n_particles=15, max_iter=20, verbose=True):
    """æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°"""
    particles = [Particle(dim, lb, ub) for _ in range(n_particles)]
    global_best_position = None
    global_best_score = -np.inf

    for iter_idx in range(max_iter):
        # è¯„ä¼°æ‰€æœ‰ç²’å­
        iter_scores = []
        for p_idx, p in enumerate(particles):
            p.score = objective_func(p.position)
            iter_scores.append(p.score)
            
            # æ›´æ–°ä¸ªä½“æœ€ä½³
            if p.score > p.best_score:
                p.best_score = p.score
                p.best_position = p.position.copy()
            
            # æ›´æ–°å…¨å±€æœ€ä½³
            if p.score > global_best_score:
                global_best_score = p.score
                global_best_position = p.position.copy()

        # è®¡ç®—æœ¬è½®ç»Ÿè®¡ä¿¡æ¯
        iter_scores = np.array(iter_scores)
        mean_score = np.mean(iter_scores)
        std_score = np.std(iter_scores)
        min_score = np.min(iter_scores)
        max_score = np.max(iter_scores)

        # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
        w, c1, c2 = 0.5, 1.5, 1.5
        for p in particles:
            r1, r2 = np.random.rand(dim), np.random.rand(dim)
            p.velocity = (w * p.velocity +
                          c1 * r1 * (p.best_position - p.position) +
                          c2 * r2 * (global_best_position - p.position))
            p.position = p.position + p.velocity
            p.position = np.clip(p.position, lb, ub)

        if verbose:
            # è·å–å½“å‰æœ€ä½³å‚æ•°
            best_params_str = ""
            if global_best_position is not None:
                ts, bs, hs = int(global_best_position[0]), int(global_best_position[1]), int(global_best_position[2])
                best_params_str = f"| Best: ts={ts}, bs={bs}, hs={hs}"
            
            print(f"ğŸ”„ PSO ç¬¬ {iter_idx+1:02d}/{max_iter} è½® | "
                  f"å¹³å‡: {mean_score:.4f} Â± {std_score:.4f} | "
                  f"èŒƒå›´: [{min_score:.4f}, {max_score:.4f}] | "
                  f"å…¨å±€æœ€ä½³: {global_best_score:.4f} {best_params_str}")

    return global_best_position, global_best_score

# ------------------------- Main -------------------------

def main():
    parser = argparse.ArgumentParser(description="PSO-LSTM for discharge_vol prediction (basin 22001)")
    parser.add_argument("--basin_id", type=str, default="22001", help="Basin ID (default: 22001)")
    parser.add_argument("--time_steps", type=int, default=None, help="Input sequence length (use PSO if None)")
    parser.add_argument("--lead_time", type=int, default=1, help="Lead time (forecast horizon)")
    parser.add_argument("--batch_size", type=int, default=None, help="Batch size (use PSO if None)")
    parser.add_argument("--hidden_size", type=int, default=None, help="Hidden size (use PSO if None)")
    parser.add_argument("--pso_particles", type=int, default=10, help="PSO particles")
    parser.add_argument("--pso_iters", type=int, default=5, help="PSO iterations")
    parser.add_argument("--epochs", type=int, default=20, help="Training epochs (PSO phase)")
    parser.add_argument("--final_epochs", type=int, default=50, help="Final training epochs")
    parser.add_argument("--plot", action="store_true", help="Plot results")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")

    args = parser.parse_args()

    set_seed(args.seed)

    # è®¾ç½®ç®—æ³•åç§°å’Œè¾“å‡ºç›®å½•
    algo_name = "PSO_LSTM"
    
    # åˆ›å»ºè¾“å‡ºç›®å½• - ä½¿ç”¨ä¸å¸¦å†’å·çš„æ—¶é—´æ ¼å¼
    current_time = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "model_output", f"{current_time}_{algo_name}")
    os.makedirs(output_dir, exist_ok=True)
    
    log("INFO", f"å¼€å§‹è¿è¡Œ {algo_name} ç®—æ³•")
    log("INFO", f"è¾“å‡ºç›®å½•: {output_dir}")

    # 1. åŠ è½½æ•°æ®
    log("INFO", f"åŠ è½½æµåŸŸ {args.basin_id} çš„ discharge_vol æ•°æ®...")
    discharge_series = load_discharge_data(basin_id=args.basin_id)
    log("INFO", f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(discharge_series)} ä¸ªæ—¶é—´æ­¥")

    # 2. å‚æ•°ä¼˜åŒ–ï¼ˆPSOï¼‰
    if args.time_steps is None or args.batch_size is None or args.hidden_size is None:
        log("INFO", "å¼€å§‹ PSO è¶…å‚æ•°ä¼˜åŒ–...")
        log("INFO", f"PSO é…ç½®: ç²’å­æ•°={args.pso_particles}, è¿­ä»£æ¬¡æ•°={args.pso_iters}")
        log("INFO", "æœç´¢ç©ºé—´: time_stepsâˆˆ[2,48], batch_sizeâˆˆ[4,256], hidden_sizeâˆˆ[8,128]")

        def objective(params):
            ts, bs, hs = int(params[0]), int(params[1]), int(params[2])
            
            # å‚æ•°æœ‰æ•ˆæ€§æ£€æŸ¥
            if ts < 2 or bs < 4 or hs < 8:
                print(f"âš ï¸  å‚æ•°æ— æ•ˆ: ts={ts}, bs={bs}, hs={hs} (å°äºæœ€å°å€¼)")
                return -9999
            
            print(f"ğŸ§ª æµ‹è¯•å‚æ•°ç»„åˆ: time_steps={ts:2d}, batch_size={bs:3d}, hidden_size={hs:3d}", end="")
            
            # åˆ›å»ºåºåˆ—æ•°æ®
            X, y = create_sequences(discharge_series, ts, args.lead_time)
            if len(X) < 100:
                print(f" â†’ æ•°æ®ä¸è¶³: {len(X)} < 100")
                return -9999
            
            # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
            X_train, X_val, y_train, y_val = split_train_val(X, y)
            
            # è®­ç»ƒå¹¶è¯„ä¼°
            val_nse = train_eval_lstm(X_train, y_train, X_val, y_val,
                                      batch_size=bs, hidden_size=hs,
                                      max_epochs=args.epochs, verbose=True)
            
            print(f" â†’ NSE: {val_nse:.4f}")
            return val_nse

        # PSOå‚æ•°èŒƒå›´
        dim = 3
        lb = [2, 4, 8]     # time_steps, batch_size, hidden_size
        ub = [48, 256, 128]
        best_params, best_score = pso_optimize(objective, dim, lb, ub,
                                               n_particles=args.pso_particles,
                                               max_iter=args.pso_iters, verbose=True)
        best_time_steps, best_batch_size, best_hidden_size = map(int, best_params)
        log("INFO", f"ğŸ¯ PSO ä¼˜åŒ–å®Œæˆ!")
        log("INFO", f"   æœ€ä½³å‚æ•°: time_steps={best_time_steps}, batch_size={best_batch_size}, hidden_size={best_hidden_size}")
        log("INFO", f"   æœ€ä½³éªŒè¯NSE: {best_score:.4f}")
        log("INFO", f"   æ€»è¯„ä¼°æ¬¡æ•°: {args.pso_particles * args.pso_iters}")
    else:
        best_time_steps = args.time_steps
        best_batch_size = args.batch_size
        best_hidden_size = args.hidden_size
        log("INFO", f"ä½¿ç”¨æŒ‡å®šå‚æ•°: time_steps={best_time_steps}, batch_size={best_batch_size}, hidden_size={best_hidden_size}")

    # 3. æœ€ç»ˆè®­ç»ƒ
    log("INFO", "å¼€å§‹æœ€ç»ˆæ¨¡å‹è®­ç»ƒ...")
    X, y = create_sequences(discharge_series, best_time_steps, args.lead_time)
    X_train, X_val, y_train, y_val = split_train_val(X, y)
    
    log("INFO", f"è®­ç»ƒé›†: {X_train.shape}, éªŒè¯é›†: {X_val.shape}")

    # æœ€ç»ˆæ¨¡å‹è®­ç»ƒï¼ˆå¸¦ç»˜å›¾ï¼‰
    model_path = os.path.join(output_dir, f"lstm_basin{args.basin_id}_lead{args.lead_time}.pth")
    print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒè¯¦ç»†æŸå¤±è¾“å‡º:")
    best_rmse, best_nse = train_evaluate(X_train, y_train, X_val, y_val,
                                         best_time_steps, best_batch_size, best_hidden_size,
                                         epochs=args.final_epochs, plot=args.plot, save_path=model_path, verbose=True)

    # 4. ä¿å­˜ç»“æœ
    results = {
        "basin_id": args.basin_id,
        "lead_time": args.lead_time,
        "best_time_steps": best_time_steps,
        "best_batch_size": best_batch_size,
        "best_hidden_size": best_hidden_size,
        "best_val_rmse": float(best_rmse),
        "best_val_nse": float(best_nse),
        "train_samples": len(X_train),
        "val_samples": len(X_val),
        "timestamp": current_time
    }

    with open(os.path.join(output_dir, "results.json"), "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    log("INFO", f"è¿è¡Œå®Œæˆï¼æœ€ä½³éªŒè¯RMSE: {best_rmse:.3f}, NSE: {best_nse:.3f}")
    log("INFO", f"ç»“æœå·²ä¿å­˜è‡³: {output_dir}")

if __name__ == "__main__":
    main()