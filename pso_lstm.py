# PSO-LSTMé€‚é…ç‰ˆæœ¬ - å•æµåŸŸ22001ï¼Œä»…ä½¿ç”¨discharge_vol
# ç›´æ¥è¯»å–CSVæ–‡ä»¶ï¼Œæ— éœ€å½’ä¸€åŒ–ï¼Œä¼˜åŒ–LSTMè¶…å‚æ•°
# Usage: python pso_lstm.py --lead_time 1

import os
import sys
import argparse
import json
import datetime as dt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_squared_error
from typing import Tuple
import matplotlib.pyplot as plt

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------- Utilities -------------------------

def set_seed(seed: int = 42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def nse(obs, sim):
    obs = np.asarray(obs).flatten()
    sim = np.asarray(sim).flatten()
    denom = np.sum((obs - np.mean(obs)) ** 2)
    if denom == 0:
        return -np.inf
    return 1 - np.sum((obs - sim) ** 2) / denom

def rmse(obs, sim):
    return float(np.sqrt(mean_squared_error(np.asarray(obs).flatten(),
                                            np.asarray(sim).flatten())))

def bias_pct(obs, sim):
    """
    è®¡ç®—ç™¾åˆ†æ¯”åå·®ï¼ˆBias%ï¼‰
    Args:
        obs: è§‚æµ‹å€¼æ•°ç»„
        sim: æ¨¡æ‹Ÿå€¼æ•°ç»„
    Returns:
        bias_pct: ç™¾åˆ†æ¯”åå·®å€¼
    """
    obs = np.asarray(obs).flatten()
    sim = np.asarray(sim).flatten()
    s_obs = np.sum(obs)
    if s_obs == 0:
        return np.nan
    return float((np.sum(sim - obs) / s_obs) * 100.0)

def log(level: str, message: str):
    """ç®€å•çš„æ—¥å¿—å‡½æ•°"""
    timestamp = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {level}: {message}")

# ------------------------- æ•°æ®åŠ è½½å‡½æ•° -------------------------

def load_discharge_data(basin_id: str = "22001") -> Tuple[np.ndarray, pd.DatetimeIndex]:
    """
    ç›´æ¥ä»CSVæ–‡ä»¶åŠ è½½æŒ‡å®šæµåŸŸçš„discharge_volæ•°æ®å’Œæ—¥æœŸä¿¡æ¯
    Args:
        basin_id: æµåŸŸID, é»˜è®¤ä¸º22001
    Returns:
        discharge_series: discharge_volæ—¶é—´åºåˆ—æ•°æ®
        dates: å¯¹åº”çš„æ—¥æœŸç´¢å¼•
    """
    csv_path = f"./datasets/CAMELS_GB/CAMELS_GB_timeseries/CAMELS_GB_hydromet_timeseries_{basin_id}_19701001-20150930.csv"
    
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
    
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(csv_path, parse_dates=['date'])
    
    # æå–discharge_volåˆ—å’Œæ—¥æœŸ
    discharge_series = df['discharge_vol'].values
    dates = pd.DatetimeIndex(df['date'])
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    if np.isnan(discharge_series).any():
        print(f"è­¦å‘Šï¼šå‘ç°{np.isnan(discharge_series).sum()}ä¸ªç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‰å‘å¡«å……å¤„ç†")
        # ç®€å•çš„å‰å‘å¡«å……å¤„ç†ç¼ºå¤±å€¼
        discharge_series = pd.Series(discharge_series).ffill().values
    
    return discharge_series, dates

def create_sequences(discharge_series: np.ndarray, time_steps: int, lead_time: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    åˆ›å»ºLSTMè®­ç»ƒåºåˆ—
    Args:
        discharge_series: discharge_volæ—¶é—´åºåˆ—
        time_steps: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå†å²æ—¶é—´æ­¥ï¼‰
        lead_time: é¢„æµ‹æ­¥é•¿ï¼ˆæœªæ¥æ—¶é—´æ­¥ï¼‰
    Returns:
        X: è¾“å…¥åºåˆ—ï¼Œshape=(N_samples, time_steps, 1)
        y: è¾“å‡ºåºåˆ—ï¼Œshape=(N_samples, lead_time)
    """
    X, y = [], []
    
    # æ€»åºåˆ—é•¿åº¦éœ€æ±‚
    total_length = time_steps + lead_time
    
    for i in range(len(discharge_series) - total_length + 1):
        # è¾“å…¥åºåˆ—ï¼šè¿‡å»time_stepsä¸ªæ—¶é—´æ­¥
        x_seq = discharge_series[i:i + time_steps]
        # è¾“å‡ºåºåˆ—ï¼šæœªæ¥lead_timeä¸ªæ—¶é—´æ­¥
        y_seq = discharge_series[i + time_steps:i + time_steps + lead_time]
        
        X.append(x_seq.reshape(-1, 1))  # æ·»åŠ ç‰¹å¾ç»´åº¦
        y.append(y_seq)
    
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)

def split_train_val(X: np.ndarray, y: np.ndarray, train_ratio: float = 0.8) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    Args:
        X: è¾“å…¥åºåˆ—
        y: è¾“å‡ºåºåˆ—  
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
    Returns:
        X_train, X_val, y_train, y_val
    """
    split_idx = int(len(X) * train_ratio)
    
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    return X_train, X_val, y_train, y_val

# ------------------------- Model -------------------------

class RunoffLSTM(nn.Module):
    """ç®€åŒ–ç‰ˆLSTM - ä»…ä½¿ç”¨discharge_volå†å²æ•°æ®é¢„æµ‹æœªæ¥"""
    def __init__(self, time_steps: int, hidden_size: int, lead_time: int = 1, num_layers: int = 1):
        super().__init__()
        self.lead_time = lead_time
        # è¾“å…¥ç»´åº¦ä¸º1ï¼ˆåªæœ‰discharge_volä¸€ä¸ªç‰¹å¾ï¼‰
        self.lstm = nn.LSTM(input_size=1,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True) # è¾“å…¥å½¢çŠ¶ä¸º [B, T, 1]
        self.fc = nn.Linear(hidden_size, lead_time)  # è¾“å‡ºç»´åº¦ä¸ºlead_time

    def forward(self, x):
        # x: [B, T, 1]
        out, _ = self.lstm(x)      # [B, T, H]
        out = out[:, -1, :]        # last time step, [B, H]
        out = self.fc(out)         # [B, lead_time]
        return out

def train_eval_lstm(X_train, y_train, X_val, y_val,
                    batch_size: int = 64,
                    hidden_size: int = 64,
                    max_epochs: int = 25,
                    verbose: bool = False) -> float:
    """Return validation NSE (higher is better). (ç”¨äº PSO é˜¶æ®µï¼Œä¸ç”»å›¾)"""
    if len(X_train) < 5 or len(X_val) < 3:
        return -9999

    # ç¡®ä¿yæ˜¯2Dæ•°ç»„ (N_samples, 1)
    if y_train.ndim == 1:
        y_train = y_train.reshape(-1, 1)
    if y_val.ndim == 1:
        y_val = y_val.reshape(-1, 1)

    # åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                             torch.tensor(y_train, dtype=torch.float32))
    val_ds   = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                             torch.tensor(y_val, dtype=torch.float32))
    # åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°
    bs_train = max(4, min(batch_size, len(train_ds)))
    bs_val   = max(4, min(batch_size, len(val_ds)))

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_ds, batch_size=bs_train, shuffle=True, drop_last=False)
    val_loader   = DataLoader(val_ds,   batch_size=bs_val,   shuffle=False, drop_last=False)

    model = RunoffLSTM(time_steps=X_train.shape[1], hidden_size=hidden_size, lead_time=y_train.shape[1]).to(DEVICE)
    crit = nn.MSELoss()
    opt  = torch.optim.Adam(model.parameters(), lr=1e-3)

    best_val_nse = -np.inf
    train_losses, val_losses = [], []
    
    for epoch in range(max_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            opt.zero_grad()
            pred = model(xb)
            loss = crit(pred, yb)
            loss.backward()
            opt.step()
            train_loss += loss.item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                pred = model(xb)
                val_loss += crit(pred, yb).item()
                y_true.append(yb.cpu().numpy())
                y_pred.append(pred.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        val_nse = nse(y_true, y_pred)
        
        if verbose:
            if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == max_epochs - 1:
                print(f"  Epoch {epoch+1:02d}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val NSE: {val_nse:.4f}")
        
        best_val_nse = max(best_val_nse, val_nse)

    return best_val_nse

def train_evaluate(X_train, y_train, X_val, y_val,
                   time_steps, batch_size, hidden_size,
                   epochs, plot=True, save_path=None, verbose=True):
    """
    è®­ç»ƒå¹¶è¯„ä¼°LSTMæ¨¡å‹ï¼Œæ”¯æŒå¤šlead_timeè¾“å‡º
    """
    # ç¡®ä¿yæ˜¯2Dæ•°ç»„ (N_samples, lead_time)
    if y_train.ndim == 1:
        y_train = y_train.reshape(-1, 1)
    if y_val.ndim == 1:
        y_val = y_val.reshape(-1, 1)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                             torch.tensor(y_train, dtype=torch.float32))
    val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                           torch.tensor(y_val, dtype=torch.float32))

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)

    # åˆå§‹åŒ–æ¨¡å‹ - è¾“å…¥ç»´åº¦ä¸º1ï¼ˆåªæœ‰discharge_volï¼‰
    model = RunoffLSTM(time_steps=time_steps, hidden_size=hidden_size, lead_time=y_train.shape[1]).to(DEVICE)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # è®­ç»ƒå¾ªç¯
    train_losses, val_losses = [], []
    best_val_rmse = np.inf

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                pred = model(xb)
                val_loss += criterion(pred, yb).item()
                y_true.append(yb.cpu().numpy())
                y_pred.append(pred.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_losses.append(train_loss)
        val_losses.append(val_loss)

        y_true = np.concatenate(y_true)
        y_pred = np.concatenate(y_pred)
        val_rmse = rmse(y_true, y_pred)
        val_nse = nse(y_true, y_pred)

        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            if save_path:
                torch.save(model.state_dict(), save_path)

        if verbose and ((epoch + 1) % 10 == 0 or epoch == 0 or epoch == epochs - 1):
            print(f"Epoch {epoch+1:03d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.3f} | Val NSE: {val_nse:.3f}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if plot:
        plt.figure(figsize=(8, 4))
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.title(f'Training Curve - Lead Time: {y_train.shape[1]}')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path.replace('.pth', '_training_curve.png'))
        plt.show()

    return best_val_rmse, val_nse

def plot_prediction_comparison(dates, actual, predicted, basin_id, lead_time, start_date=None, end_date=None, save_path=None):
    """
    ç»˜åˆ¶é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å¯¹æ¯”å›¾
    Args:
        dates: æ—¥æœŸç´¢å¼•
        actual: å®é™…å€¼æ•°ç»„
        predicted: é¢„æµ‹å€¼æ•°ç»„  
        basin_id: æµåŸŸID
        lead_time: é¢„æµ‹æ­¥é•¿
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        save_path: ä¿å­˜è·¯å¾„
    """
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=(15, 10))
    
    # æ—¶é—´èŒƒå›´ç­›é€‰
    mask = np.ones(len(dates), dtype=bool)
    if start_date:
        start_dt = pd.to_datetime(start_date)
        mask = mask & (dates >= start_dt)
    if end_date:
        end_dt = pd.to_datetime(end_date)
        mask = mask & (dates <= end_dt)
    
    filtered_dates = dates[mask]
    filtered_actual = actual[mask]
    filtered_predicted = predicted[mask]
    
    if len(filtered_dates) == 0:
        print("âš ï¸  æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— æ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
        filtered_dates = dates
        filtered_actual = actual
        filtered_predicted = predicted
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    nse_score = nse(filtered_actual, filtered_predicted)
    rmse_score = rmse(filtered_actual, filtered_predicted)
    bias_score = bias_pct(filtered_actual, filtered_predicted)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plt.subplot(2, 1, 1)
    plt.plot(filtered_dates, filtered_actual, label='Actual Discharge', color='blue', alpha=0.8, linewidth=1.5)
    plt.plot(filtered_dates, filtered_predicted, label='Predicted Discharge', color='red', alpha=0.8, linewidth=1.5)
    plt.title(f'Basin {basin_id} Discharge Prediction Comparison (Lead Time: {lead_time})', fontsize=14, fontweight='bold')
    plt.ylabel('Discharge (mÂ³/s)', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ æŒ‡æ ‡ä¿¡æ¯
    textstr = f'NSE: {nse_score:.4f} | RMSE: {rmse_score:.2f} | Bias: {bias_score:.2f}%'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    plt.text(0.02, 0.95, textstr, transform=plt.gca().transAxes, fontsize=12,
             verticalalignment='top', bbox=props)
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    plt.subplot(2, 1, 2)
    plt.scatter(filtered_actual, filtered_predicted, alpha=0.6, s=20)
    
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
    min_val = min(np.min(filtered_actual), np.min(filtered_predicted))
    max_val = max(np.max(filtered_actual), np.max(filtered_predicted))
    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=2, label='Perfect Prediction Line')
    
    plt.xlabel('Actual Discharge (mÂ³/s)', fontsize=12)
    plt.ylabel('Predicted Discharge (mÂ³/s)', fontsize=12)
    plt.title(f'Predicted vs Actual Discharge Scatter Plot (NSE: {nse_score:.4f})', fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    
    # æ—¶é—´èŒƒå›´ä¿¡æ¯
    time_range_info = f"Time Range: {filtered_dates[0].strftime('%Y-%m-%d')} to {filtered_dates[-1].strftime('%Y-%m-%d')}"
    if start_date or end_date:
        time_range_info += f" (Filtered: {len(filtered_dates)} data points)"
    else:
        time_range_info += f" (All Data: {len(filtered_dates)} data points)"
    
    plt.figtext(0.5, 0.01, time_range_info, ha='center', fontsize=10, style='italic')
    
    plt.tight_layout(rect=[0, 0.03, 1, 1])  # è°ƒæ•´å¸ƒå±€ï¼Œä¸ºåº•éƒ¨æ–‡æœ¬ç•™å‡ºç©ºé—´
    
    # ä¿å­˜å›¾ç‰‡ï¼ˆåœ¨showä¹‹å‰ä¿å­˜ï¼‰
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Prediction comparison plot saved to: {save_path}")
    
    plt.show()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š Prediction Statistics:")
    print(f"   Data Points: {len(filtered_actual)}")
    print(f"   Actual Range: [{np.min(filtered_actual):.2f}, {np.max(filtered_actual):.2f}]")
    print(f"   Predicted Range: [{np.min(filtered_predicted):.2f}, {np.max(filtered_predicted):.2f}]")
    print(f"   NSE: {nse_score:.4f}")
    print(f"   RMSE: {rmse_score:.2f}")
    print(f"   Bias: {bias_score:.2f}%")
    
    return nse_score, rmse_score, bias_score

# ------------------------- PSO -------------------------

class Particle:
    def __init__(self, dim, lb, ub):
        self.dim = dim
        self.lb = np.array(lb)
        self.ub = np.array(ub)
        self.position = np.random.uniform(self.lb, self.ub)
        self.velocity = np.zeros(dim)
        self.best_position = self.position.copy()
        self.best_score = -np.inf
        self.score = -np.inf

def pso_optimize(objective_func, dim, lb, ub, n_particles=15, max_iter=20, verbose=True):
    """æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°"""
    particles = [Particle(dim, lb, ub) for _ in range(n_particles)]
    global_best_position = None
    global_best_score = -np.inf

    for iter_idx in range(max_iter):
        # è¯„ä¼°æ‰€æœ‰ç²’å­
        iter_scores = []
        for p_idx, p in enumerate(particles):
            p.score = objective_func(p.position)
            iter_scores.append(p.score)
            
            # æ›´æ–°ä¸ªä½“æœ€ä½³
            if p.score > p.best_score:
                p.best_score = p.score
                p.best_position = p.position.copy()
            
            # æ›´æ–°å…¨å±€æœ€ä½³
            if p.score > global_best_score:
                global_best_score = p.score
                global_best_position = p.position.copy()

        # è®¡ç®—æœ¬è½®ç»Ÿè®¡ä¿¡æ¯
        iter_scores = np.array(iter_scores)
        mean_score = np.mean(iter_scores)
        std_score = np.std(iter_scores)
        min_score = np.min(iter_scores)
        max_score = np.max(iter_scores)

        # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
        w, c1, c2 = 0.5, 1.5, 1.5
        for p in particles:
            r1, r2 = np.random.rand(dim), np.random.rand(dim)
            p.velocity = (w * p.velocity +
                          c1 * r1 * (p.best_position - p.position) +
                          c2 * r2 * (global_best_position - p.position))
            p.position = p.position + p.velocity
            p.position = np.clip(p.position, lb, ub)

        if verbose:
            # è·å–å½“å‰æœ€ä½³å‚æ•°
            best_params_str = ""
            if global_best_position is not None:
                ts, bs, hs = int(global_best_position[0]), int(global_best_position[1]), int(global_best_position[2])
                best_params_str = f"| Best: ts={ts}, bs={bs}, hs={hs}"
            
            print(f"ğŸ”„ PSO ç¬¬ {iter_idx+1:02d}/{max_iter} è½® | "
                  f"å¹³å‡: {mean_score:.4f} Â± {std_score:.4f} | "
                  f"èŒƒå›´: [{min_score:.4f}, {max_score:.4f}] | "
                  f"å…¨å±€æœ€ä½³: {global_best_score:.4f} {best_params_str}")

    return global_best_position, global_best_score

# ------------------------- Main -------------------------
# python .\pso_lstm.py --basin_id 32006 --pso_particles 10 --pso_iters 5 --epochs 30 --final_epochs 50 --plot --plot_prediction --start 2000-01-01 --end 2000-12-31
def main():
    parser = argparse.ArgumentParser(description="PSO-LSTM for discharge_vol prediction (basin 22001)")
    parser.add_argument("--basin_id", type=str, default="22001", help="Basin ID (default: 22001)")
    parser.add_argument("--time_steps", type=int, default=None, help="Input sequence length (use PSO if None)")
    parser.add_argument("--lead_time", type=int, default=1, help="Lead time (forecast horizon)")
    parser.add_argument("--batch_size", type=int, default=None, help="Batch size (use PSO if None)")
    parser.add_argument("--hidden_size", type=int, default=None, help="Hidden size (use PSO if None)")
    parser.add_argument("--pso_particles", type=int, default=10, help="PSO particles")
    parser.add_argument("--pso_iters", type=int, default=5, help="PSO iterations")
    parser.add_argument("--epochs", type=int, default=20, help="Training epochs (PSO phase)")
    parser.add_argument("--final_epochs", type=int, default=50, help="Final training epochs")
    parser.add_argument("--plot", action="store_true", help="Plot results")
    parser.add_argument("--plot_prediction", action="store_true", help="ç»˜åˆ¶é¢„æµ‹å¯¹æ¯”å›¾")
    parser.add_argument("--start", type=str, help="é¢„æµ‹å¼€å§‹æ—¶é—´ (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, help="é¢„æµ‹ç»“æŸæ—¶é—´ (YYYY-MM-DD)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")

    args = parser.parse_args()

    set_seed(args.seed)

    # è®¾ç½®ç®—æ³•åç§°å’Œè¾“å‡ºç›®å½•
    algo_name = "PSO_LSTM"
    
    # åˆ›å»ºè¾“å‡ºç›®å½• - ä½¿ç”¨ä¸å¸¦å†’å·çš„æ—¶é—´æ ¼å¼
    current_time = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "model_output", algo_name, f"{current_time}")
    os.makedirs(output_dir, exist_ok=True)
    
    log("INFO", f"å¼€å§‹è¿è¡Œ {algo_name} ç®—æ³•")
    log("INFO", f"è¾“å‡ºç›®å½•: {output_dir}")

    # 1. åŠ è½½æ•°æ®
    log("INFO", f"åŠ è½½æµåŸŸ {args.basin_id} çš„ discharge_vol æ•°æ®...")
    discharge_series, dates = load_discharge_data(basin_id=args.basin_id)
    log("INFO", f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(discharge_series)} ä¸ªæ—¶é—´æ­¥")

    # 2. å‚æ•°ä¼˜åŒ–ï¼ˆPSOï¼‰
    if args.time_steps is None or args.batch_size is None or args.hidden_size is None:
        log("INFO", "å¼€å§‹ PSO è¶…å‚æ•°ä¼˜åŒ–...")
        log("INFO", f"PSO é…ç½®: ç²’å­æ•°={args.pso_particles}, è¿­ä»£æ¬¡æ•°={args.pso_iters}")
        log("INFO", "æœç´¢ç©ºé—´: time_stepsâˆˆ[2,48], batch_sizeâˆˆ[4,256], hidden_sizeâˆˆ[8,128]")

        def objective(params):
            ts, bs, hs = int(params[0]), int(params[1]), int(params[2])
            
            # å‚æ•°æœ‰æ•ˆæ€§æ£€æŸ¥
            if ts < 2 or bs < 4 or hs < 8:
                print(f"âš ï¸  å‚æ•°æ— æ•ˆ: ts={ts}, bs={bs}, hs={hs} (å°äºæœ€å°å€¼)")
                return -9999
            
            print(f"ğŸ§ª æµ‹è¯•å‚æ•°ç»„åˆ: time_steps={ts:2d}, batch_size={bs:3d}, hidden_size={hs:3d}", end="")
            
            # åˆ›å»ºåºåˆ—æ•°æ®
            X, y = create_sequences(discharge_series, ts, args.lead_time)
            if len(X) < 100:
                print(f" â†’ æ•°æ®ä¸è¶³: {len(X)} < 100")
                return -9999
            
            # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
            X_train, X_val, y_train, y_val = split_train_val(X, y)
            
            # è®­ç»ƒå¹¶è¯„ä¼°
            val_nse = train_eval_lstm(X_train, y_train, X_val, y_val,
                                      batch_size=bs, hidden_size=hs,
                                      max_epochs=args.epochs, verbose=True)
            
            print(f" â†’ NSE: {val_nse:.4f}")
            return val_nse

        # PSOå‚æ•°èŒƒå›´
        dim = 3
        lb = [2, 4, 8]     # time_steps, batch_size, hidden_size
        ub = [48, 256, 128]
        best_params, best_score = pso_optimize(objective, dim, lb, ub,
                                               n_particles=args.pso_particles,
                                               max_iter=args.pso_iters, verbose=True)
        best_time_steps, best_batch_size, best_hidden_size = map(int, best_params)
        log("INFO", f"ğŸ¯ PSO ä¼˜åŒ–å®Œæˆ!")
        log("INFO", f"   æœ€ä½³å‚æ•°: time_steps={best_time_steps}, batch_size={best_batch_size}, hidden_size={best_hidden_size}")
        log("INFO", f"   æœ€ä½³éªŒè¯NSE: {best_score:.4f}")
        log("INFO", f"   æ€»è¯„ä¼°æ¬¡æ•°: {args.pso_particles * args.pso_iters}")
    else:
        best_time_steps = args.time_steps
        best_batch_size = args.batch_size
        best_hidden_size = args.hidden_size
        log("INFO", f"ä½¿ç”¨æŒ‡å®šå‚æ•°: time_steps={best_time_steps}, batch_size={best_batch_size}, hidden_size={best_hidden_size}")

    # 3. æœ€ç»ˆè®­ç»ƒ
    log("INFO", "å¼€å§‹æœ€ç»ˆæ¨¡å‹è®­ç»ƒ...")
    X, y = create_sequences(discharge_series, best_time_steps, args.lead_time)
    X_train, X_val, y_train, y_val = split_train_val(X, y)
    
    log("INFO", f"è®­ç»ƒé›†: {X_train.shape}, éªŒè¯é›†: {X_val.shape}")

    # æœ€ç»ˆæ¨¡å‹è®­ç»ƒï¼ˆå¸¦ç»˜å›¾ï¼‰
    model_path = os.path.join(output_dir, f"lstm_basin{args.basin_id}_lead{args.lead_time}.pth")
    print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒè¯¦ç»†æŸå¤±è¾“å‡º:")
    best_rmse, best_nse = train_evaluate(X_train, y_train, X_val, y_val,
                                         best_time_steps, best_batch_size, best_hidden_size,
                                         epochs=args.final_epochs, plot=args.plot, save_path=model_path, verbose=True)

    # 4. ä¿å­˜ç»“æœ
    results = {
        "basin_id": args.basin_id,
        "lead_time": args.lead_time,
        "best_time_steps": best_time_steps,
        "best_batch_size": best_batch_size,
        "best_hidden_size": best_hidden_size,
        "best_val_rmse": float(best_rmse),
        "best_val_nse": float(best_nse),
        "train_samples": len(X_train),
        "val_samples": len(X_val),
        "timestamp": current_time
    }

    with open(os.path.join(output_dir, "results.json"), "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    log("INFO", f"è¿è¡Œå®Œæˆï¼æœ€ä½³éªŒè¯RMSE: {best_rmse:.3f}, NSE: {best_nse:.3f}")
    log("INFO", f"ç»“æœå·²ä¿å­˜è‡³: {output_dir}")

    # 5. é¢„æµ‹å¯¹æ¯”å›¾ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.plot_prediction:
        log("INFO", "å¼€å§‹ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å›¾...")
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæœ€ç»ˆæ¨¡å‹
        final_model = RunoffLSTM(time_steps=best_time_steps, hidden_size=best_hidden_size, lead_time=args.lead_time)
        final_model.load_state_dict(torch.load(model_path, map_location=device))
        final_model.to(device)
        final_model.eval()
        
        # åˆ›å»ºå®Œæ•´åºåˆ—æ•°æ®ç”¨äºé¢„æµ‹
        X_full, y_full = create_sequences(discharge_series, best_time_steps, args.lead_time)
        
        # è½¬æ¢ä¸ºtensor
        X_tensor = torch.FloatTensor(X_full).to(device)
        y_tensor = torch.FloatTensor(y_full).to(device)
        
        # ç”Ÿæˆé¢„æµ‹
        with torch.no_grad():
            y_pred_tensor = final_model(X_tensor)
            y_pred = y_pred_tensor.cpu().numpy().flatten()
            y_actual = y_tensor.cpu().numpy().flatten()
        
        # è·å–å¯¹åº”çš„æ—¥æœŸï¼ˆéœ€è¦è°ƒæ•´ç´¢å¼•ä»¥åŒ¹é…é¢„æµ‹ç»“æœï¼‰
        pred_dates = dates[best_time_steps + args.lead_time - 1:]
        
        # ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å›¾
        plot_save_path = os.path.join(output_dir, f"prediction_comparison_basin{args.basin_id}_lead{args.lead_time}.png")
        plot_prediction_comparison(
            dates=pred_dates,
            actual=y_actual,
            predicted=y_pred,
            basin_id=args.basin_id,
            lead_time=args.lead_time,
            start_date=args.start,
            end_date=args.end,
            save_path=plot_save_path
        )
        
        log("INFO", "é¢„æµ‹å¯¹æ¯”å›¾ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()