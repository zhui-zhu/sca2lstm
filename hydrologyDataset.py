import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import time
import psutil
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Optional, Tuple
import threading
import functools
import platform

class HydrologyDataset(Dataset):
    def __init__(self, basin_ids: list, config, mode: str = "train", 
                 use_parallel: bool = False, max_load_threads: int = 20, 
                 max_sample_processes: int = 10, enable_monitoring: bool = True):
        self.config = config
        self.mode = mode
        self.basin_ids = basin_ids
        self.lstm2_features = config.LSTM2_FEATURES
        self.lstm1_derived_feats = config.LSTM1_DERIVED_FEATS
        self.target_col = config.TARGET_COL
        self.seq_len = config.SEQ_LEN
        self.pred_len = config.PRED_LEN
        
        # å¹¶è¡ŒåŒ–é…ç½®
        self.use_parallel = use_parallel
        self.max_load_threads = max_load_threads
        self.max_sample_processes = max_sample_processes
        self.enable_monitoring = enable_monitoring
        
        # Windowsç³»ç»Ÿæ£€æµ‹
        self.is_windows = platform.system() == 'Windows'
        if self.is_windows and self.use_parallel:
            print("âš ï¸  Windowsç³»ç»Ÿä¸‹å»ºè®®ç¦ç”¨å¤šè¿›ç¨‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸²è¡Œæ¨¡å¼")
            self.use_parallel = False
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'load_time': 0,
            'sample_time': 0,
            'total_time': 0,
            'memory_peak': 0,
            'errors': []
        }
        
        print(f"ğŸš€ åˆå§‹åŒ–æ°´æ–‡æ•°æ®é›† ({mode}æ¨¡å¼)")
        print(f"ğŸ“Š é…ç½®å‚æ•°:")
        print(f"   - æµåŸŸæ•°é‡: {len(basin_ids)}")
        print(f"   - å¹¶è¡ŒåŒ–æ¨¡å¼: {'âœ… å¯ç”¨' if self.use_parallel else 'âŒ ç¦ç”¨'}")
        if self.use_parallel:
            print(f"   - æ•°æ®åŠ è½½çº¿ç¨‹: {max_load_threads}")
            print(f"   - æ ·æœ¬ç”Ÿæˆè¿›ç¨‹: {max_sample_processes}")
        print(f"   - åºåˆ—é•¿åº¦: {config.SEQ_LEN}")
        print(f"   - é¢„æµ‹é•¿åº¦: {config.PRED_LEN}")
        print()
        
        start_time = time.time()
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®åŠ è½½æ–¹å¼
        if self.use_parallel:
            self.data = self._load_all_basins_data_parallel()
            self.samples = self._generate_samples_parallel()
        else:
            self.data = self._load_all_basins_data()
            self.samples = self._generate_samples()
        
        end_time = time.time()
        self.performance_stats['total_time'] = end_time - start_time
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        if self.enable_monitoring:
            self._print_performance_stats()

    def _load_single_basin_data(self, basin_id: int) -> pd.DataFrame or None:
        basin_dir = os.path.join(self.config.DATA_INPUT_DIR, str(basin_id))
        data_path = os.path.join(basin_dir, f"model_input_{basin_id}.csv")
        
        if not os.path.exists(data_path):
            print(f"âš ï¸  æµåŸŸ{basin_id}çš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}ï¼Œè·³è¿‡")
            return None
        
        df = pd.read_csv(data_path)
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        original_rows = len(df)
        
        # è¿‡æ»¤ç›®æ ‡å€¼NaN
        df = df.dropna(subset=["discharge_vol", self.target_col]).reset_index(drop=True)
        valid_rows = len(df)
        valid_ratio = valid_rows / original_rows if original_rows > 0 else 0
        
        if valid_ratio < self.config.MIN_VALID_LABEL_RATIO:
            print(f"âš ï¸  æµåŸŸ{basin_id}æœ‰æ•ˆæ ‡ç­¾æ¯”ä¾‹{valid_ratio:.2f}ï¼ˆ<{self.config.MIN_VALID_LABEL_RATIO}ï¼‰ï¼Œè·³è¿‡")
            return None
        if valid_rows < self.config.MIN_VALID_ROWS:
            print(f"âš ï¸  æµåŸŸ{basin_id}æœ‰æ•ˆæ ‡ç­¾è¡Œæ•°{valid_rows}ï¼ˆ<{self.config.MIN_VALID_ROWS}ï¼‰ï¼Œè·³è¿‡")
            return None
        
        required_cols = self.lstm2_features + self.lstm1_derived_feats + [self.target_col, "date", "catchment_id"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âš ï¸  æµåŸŸ{basin_id}ç¼ºå°‘å¿…éœ€åˆ—ï¼š{missing_cols}ï¼Œè·³è¿‡")
            return None
        
        df.rename(columns={"catchment_id": "basin_id"}, inplace=True)
        return df

    def _load_all_basins_data(self) -> pd.DataFrame:
        all_data = []
        for basin_id in tqdm(self.basin_ids, desc=f"åŠ è½½{self.mode}æµåŸŸæ•°æ®"):
            try:
                df = self._load_single_basin_data(basin_id)
                if df is not None:
                    all_data.append(df)
            except Exception as e:
                print(f"âŒ åŠ è½½æµåŸŸ{basin_id}å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡è¯¥æµåŸŸ")
                continue
        
        if not all_data:
            raise ValueError(f"âš ï¸  æ— æœ‰æ•ˆæµåŸŸæ•°æ®ï¼ˆæ‰€æœ‰æµåŸŸæ ‡ç­¾å…¨ä¸ºNaNæˆ–æœ‰æ•ˆæ ‡ç­¾ä¸è¶³ï¼‰ï¼")
        
        return pd.concat(all_data, ignore_index=True) 

    def _load_all_basins_data_parallel(self) -> pd.DataFrame:
        """å¹¶è¡ŒåŠ è½½æ‰€æœ‰æµåŸŸæ•°æ®"""
        print(f"ğŸ”„ å¼€å§‹å¹¶è¡Œæ•°æ®åŠ è½½...")
        start_time = time.time()
        
        # è®¡ç®—æœ€ä¼˜çº¿ç¨‹æ•°
        n_threads = min(self.max_load_threads, multiprocessing.cpu_count() * 2, len(self.basin_ids))
        
        all_data = []
        completed_count = 0
        failed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŠ è½½
        with ThreadPoolExecutor(max_workers=n_threads) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_basin = {
                executor.submit(self._load_single_basin_data_with_retry, basin_id): basin_id
                for basin_id in self.basin_ids
            }
            
            # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(self.basin_ids), desc="å¹¶è¡ŒåŠ è½½æµåŸŸæ•°æ®") as pbar:
                for future in as_completed(future_to_basin):
                    basin_id = future_to_basin[future]
                    try:
                        df = future.result()
                        if df is not None:
                            all_data.append(df)
                            completed_count += 1
                        else:
                            failed_count += 1
                    except Exception as e:
                        print(f"âŒ æµåŸŸ{basin_id}åŠ è½½å¤±è´¥: {str(e)}")
                        failed_count += 1
                        self.performance_stats['errors'].append(f"æµåŸŸ{basin_id}: {str(e)}")
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        "æˆåŠŸ": completed_count,
                        "å¤±è´¥": failed_count
                    })
        
        if not all_data:
            raise ValueError(f"âš ï¸ æ— æœ‰æ•ˆæµåŸŸæ•°æ®ï¼æˆåŠŸ: {completed_count}, å¤±è´¥: {failed_count}")
        
        # åˆå¹¶æ•°æ®
        result_df = pd.concat(all_data, ignore_index=True)
        
        end_time = time.time()
        self.performance_stats['load_time'] = end_time - start_time
        
        print(f"âœ… å¹¶è¡Œæ•°æ®åŠ è½½å®Œæˆï¼")
        print(f"   - æˆåŠŸåŠ è½½: {completed_count} ä¸ªæµåŸŸ")
        print(f"   - å¤±è´¥: {failed_count} ä¸ªæµåŸŸ") 
        print(f"   - æ€»æ•°æ®è¡Œæ•°: {len(result_df)}")
        print(f"   - è€—æ—¶: {self.performance_stats['load_time']:.2f} ç§’")
        print()
        
        return result_df
    
    def _load_single_basin_data_with_retry(self, basin_id: int, max_retries: int = 3) -> Optional[pd.DataFrame]:
        """å¸¦é‡è¯•æœºåˆ¶çš„å•æµåŸŸæ•°æ®åŠ è½½"""
        for attempt in range(max_retries):
            try:
                return self._load_single_basin_data(basin_id)
            except Exception as e:
                if attempt < max_retries - 1:
                    # æŒ‡æ•°é€€é¿é‡è¯•
                    time.sleep(0.1 * (attempt + 1))
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥
                    return None
        return None

    def _get_time_encoding(self, date_series: pd.Series) -> np.ndarray:
        month = date_series.dt.month
        day = date_series.dt.day
        month_sin = np.sin(2 * np.pi * month / 12)
        month_cos = np.cos(2 * np.pi * month / 12)
        day_sin = np.sin(2 * np.pi * day / 31)
        day_cos = np.cos(2 * np.pi * day / 31)
        return np.stack([month_sin, month_cos, day_sin, day_cos], axis=1)

    def _get_lstm1_input(self, df_seq: pd.DataFrame) -> np.ndarray:
        lstm1_inputs = []
        # time_encoding = self._get_time_encoding(df_seq["date"])
        # lstm1_inputs.append(time_encoding)
        
        derived_feats = df_seq[self.lstm1_derived_feats].values.astype(np.float32)
        lstm1_inputs.append(derived_feats)
        
        # lstm2_features_data = df_seq[self.lstm2_features]
        # missing_bool = (~np.isnan(lstm2_features_data.values)).astype(np.float32)
        # lstm1_inputs.append(missing_bool)
        
        # æ·»åŠ LSTM2åé¦ˆæ®‹å·®ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰
        seq_len = len(df_seq)
        feedback_residual = np.zeros((seq_len, 1), dtype=np.float32)
        lstm1_inputs.append(feedback_residual)

        result = np.concatenate(lstm1_inputs, axis=1).astype(np.float32)
        return result

    def _generate_samples(self) -> list:
        print(f"å¼€å§‹ç”Ÿæˆ{self.mode}é›†åºåˆ—æ ·æœ¬ï¼ˆåºåˆ—é•¿åº¦={self.seq_len}å¤©ï¼‰...")
        samples = []
        for basin_id, df_basin in self.data.groupby("basin_id"):
            df_basin = df_basin.reset_index(drop=True)
            n_potential_samples = len(df_basin) - self.seq_len - self.pred_len + 1
            if n_potential_samples <= 0:
                print(f"âš ï¸  æµåŸŸ{basin_id}æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæ ·æœ¬ï¼ˆéœ€{self.seq_len+self.pred_len}å¤©ï¼Œå®é™…{len(df_basin)}å¤©ï¼‰")
                continue
            valid_sample_count = 0
            for i in range(n_potential_samples):
                seq_start = i
                seq_end = i + self.seq_len
                df_seq = df_basin.iloc[seq_start:seq_end].copy()
                
                target_start = seq_end
                target_end = seq_end + self.pred_len
                if target_end > len(df_basin):
                    continue
                
                target = df_basin.iloc[target_start:target_end][self.target_col].values
                if np.isnan(target).any():
                    continue
                
                # å¡«å……ç‰¹å¾NaNï¼ˆç”¨åºåˆ—å‡å€¼ï¼‰
                seq_features = df_seq[self.lstm2_features].copy()
                for feat in self.lstm2_features:
                    if seq_features[feat].isna().any():
                        seq_features[feat].fillna(seq_features[feat].mean(), inplace=True)
                seq_features = seq_features.values.astype(np.float32)
                
                lstm1_input = self._get_lstm1_input(df_seq)
                missing_bool = (~np.isnan(df_seq[self.lstm2_features].values)).astype(np.float32)
                
                samples.append({
                    "seq_features": seq_features,
                    "lstm1_input": lstm1_input,
                    "missing_bool": missing_bool,
                    "basin_id": basin_id,
                    "target": target.astype(np.float32)
                })
                valid_sample_count += 1
        
        print(f"{self.mode}é›†ç”Ÿæˆå®Œæˆï¼šå…±{len(samples)}ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼ˆæ ‡ç­¾å‡éNaNï¼‰")
        return samples
    
    def _generate_samples_parallel(self) -> List[Dict]:
        """å¹¶è¡Œç”Ÿæˆæ ·æœ¬"""
        print(f"ğŸ”„ å¼€å§‹å¹¶è¡Œæ ·æœ¬ç”Ÿæˆ...")
        start_time = time.time()
        
        # æŒ‰æµåŸŸåˆ†ç»„æ•°æ®
        basin_groups = list(self.data.groupby("basin_id"))
        all_samples = []
        completed_basins = 0
        failed_basins = 0
        
        # å‡†å¤‡é…ç½®å‚æ•°ï¼ˆè½¬æ¢ä¸ºåŸºæœ¬ç±»å‹ï¼Œé¿å…pickleé—®é¢˜ï¼‰
        config_params = {
            'seq_len': self.config.SEQ_LEN,
            'pred_len': self.config.PRED_LEN,
            'target_col': self.config.TARGET_COL,
            'lstm2_features': list(self.config.LSTM2_FEATURES),
            'lstm1_derived_feats': list(self.config.LSTM1_DERIVED_FEATS)
        }
        
        # è®¡ç®—æœ€ä¼˜è¿›ç¨‹æ•°
        n_processes = min(self.max_sample_processes, multiprocessing.cpu_count(), len(basin_groups))
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=n_processes) as executor:
            # å‡†å¤‡ä»»åŠ¡å‚æ•°
            tasks = [
                (basin_id, df_basin.copy(), config_params, self.mode)
                for basin_id, df_basin in basin_groups
            ]
            
            # æäº¤ä»»åŠ¡
            futures = [
                executor.submit(self._generate_single_basin_samples_parallel, *task)
                for task in tasks
            ]
            
            # æ”¶é›†ç»“æœ
            with tqdm(total=len(basin_groups), desc="å¹¶è¡Œç”Ÿæˆæ ·æœ¬") as pbar:
                for future in as_completed(futures):
                    try:
                        basin_samples = future.result()
                        if basin_samples:
                            all_samples.extend(basin_samples)
                            completed_basins += 1
                        else:
                            failed_basins += 1
                    except Exception as e:
                        failed_basins += 1
                        print(f"âŒ æ ·æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
                        self.performance_stats['errors'].append(f"æ ·æœ¬ç”Ÿæˆ: {str(e)}")
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        "å®ŒæˆæµåŸŸ": completed_basins,
                        "å¤±è´¥æµåŸŸ": failed_basins,
                        "æ ·æœ¬æ•°": len(all_samples)
                    })
        
        end_time = time.time()
        self.performance_stats['sample_time'] = end_time - start_time
        
        print(f"âœ… å¹¶è¡Œæ ·æœ¬ç”Ÿæˆå®Œæˆï¼")
        print(f"   - æˆåŠŸå¤„ç†: {completed_basins} ä¸ªæµåŸŸ")
        print(f"   - å¤±è´¥: {failed_basins} ä¸ªæµåŸŸ")
        print(f"   - ç”Ÿæˆæ ·æœ¬: {len(all_samples)} ä¸ª")
        print(f"   - è€—æ—¶: {self.performance_stats['sample_time']:.2f} ç§’")
        print(f"   - å¹³å‡é€Ÿåº¦: {len(all_samples)/self.performance_stats['sample_time']:.1f} æ ·æœ¬/ç§’")
        print()
        
        return all_samples
    
    @staticmethod
    def _generate_single_basin_samples_parallel(basin_id: int, df_basin: pd.DataFrame, 
                                               config_params: dict, mode: str) -> List[Dict]:
        """å¹¶è¡Œå¤„ç†å•ä¸ªæµåŸŸçš„æ ·æœ¬ç”Ÿæˆï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¤šè¿›ç¨‹ï¼‰"""
        try:
            samples = []
            seq_len = config_params['seq_len']
            pred_len = config_params['pred_len']
            target_col = config_params['target_col']
            lstm2_features = config_params['lstm2_features']
            lstm1_derived_feats = config_params['lstm1_derived_feats']
            
            # é‡ç½®ç´¢å¼•
            df_basin = df_basin.reset_index(drop=True)
            n_potential_samples = len(df_basin) - seq_len - pred_len + 1
            
            if n_potential_samples <= 0:
                return samples
            
            # é¢„è®¡ç®—æ—¶é—´ç¼–ç ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
            month = df_basin["date"].dt.month.values
            day = df_basin["date"].dt.day.values
            month_sin = np.sin(2 * np.pi * month / 12)
            month_cos = np.cos(2 * np.pi * month / 12)
            day_sin = np.sin(2 * np.pi * day / 31)
            day_cos = np.cos(2 * np.pi * day / 31)
            
            for i in range(n_potential_samples):
                seq_start = i
                seq_end = i + seq_len
                
                # æ•°æ®åˆ‡ç‰‡
                df_seq = df_basin.iloc[seq_start:seq_end]
                
                # ç›®æ ‡å€¼æ£€æŸ¥
                target_start = seq_end
                target_end = seq_end + pred_len
                if target_end > len(df_basin):
                    continue
                
                target = df_basin.iloc[target_start:target_end][target_col].values
                if np.isnan(target).any():
                    continue
                
                # ç‰¹å¾å¤„ç†ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰
                seq_features_matrix = df_seq[lstm2_features].values.astype(np.float32)
                
                # NaNå¡«å……ï¼ˆå‘é‡åŒ–ï¼‰
                col_means = np.nanmean(seq_features_matrix, axis=0)
                nan_mask = np.isnan(seq_features_matrix)
                if np.any(nan_mask):
                    # ä½¿ç”¨å‘é‡åŒ–å¡«å……è€Œä¸æ˜¯å¾ªç¯
                    row_indices, col_indices = np.where(nan_mask)
                    seq_features_matrix[nan_mask] = col_means[col_indices]
                
                # LSTM1è¾“å…¥ç”Ÿæˆ
                time_encoding = np.stack([
                    month_sin[seq_start:seq_end],
                    month_cos[seq_start:seq_end],
                    day_sin[seq_start:seq_end],
                    day_cos[seq_start:seq_end]
                ], axis=1)
                
                derived_feats = df_seq[lstm1_derived_feats].values.astype(np.float32)
                missing_bool = (~np.isnan(df_seq[lstm2_features].values)).astype(np.float32)
                
                # æ·»åŠ LSTM2åé¦ˆæ®‹å·®ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰
                feedback_residual = np.zeros((seq_len, 1), dtype=np.float32)
                
                lstm1_input = np.concatenate([
                    time_encoding,
                    derived_feats,
                    missing_bool,
                    feedback_residual
                ], axis=1).astype(np.float32)
                
                # æ·»åŠ åˆ°æ ·æœ¬åˆ—è¡¨
                samples.append({
                    "seq_features": seq_features_matrix,
                    "lstm1_input": lstm1_input,
                    "missing_bool": missing_bool,
                    "basin_id": basin_id,
                    "target": target.astype(np.float32)
                })
            
            return samples
            
        except Exception as e:
            print(f"âŒ æµåŸŸ{basin_id}æ ·æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            return []

    def __len__(self) -> int:
        return len(self.samples)
    
    def _print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_monitoring:
            return
        
        stats = self.performance_stats
        total_time = stats['total_time']
        load_time = stats['load_time']
        sample_time = stats['sample_time']
        
        print("=" * 60)
        print("ğŸ“Š æ•°æ®é›†æ€§èƒ½ç»Ÿè®¡")
        print("=" * 60)
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
        if self.use_parallel:
            print(f"ğŸ“‚ æ•°æ®åŠ è½½: {load_time:.2f} ç§’ ({load_time/total_time*100:.1f}%)")
            print(f"ğŸ¯ æ ·æœ¬ç”Ÿæˆ: {sample_time:.2f} ç§’ ({sample_time/total_time*100:.1f}%)")
            print(f"âš¡ å¹¶è¡ŒåŠ é€Ÿæ¯”: {total_time/(load_time + sample_time):.2f}x")
        
        if stats['errors']:
            print(f"âŒ é”™è¯¯æ•°é‡: {len(stats['errors'])}")
            for error in stats['errors'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                print(f"   - {error}")
        
        print("=" * 60)
        print()
    
    def get_performance_comparison(self, serial_time: float = None) -> Dict:
        """è·å–æ€§èƒ½å¯¹æ¯”ä¿¡æ¯"""
        if not self.use_parallel:
            return {'mode': 'serial', 'total_time': self.performance_stats['total_time']}
        
        parallel_time = self.performance_stats['total_time']
        if serial_time is None:
            # ä¼°ç®—ä¸²è¡Œæ—¶é—´ï¼ˆåŸºäºç»éªŒæ³•åˆ™ï¼‰
            serial_time = parallel_time * 2.5  # ä¿å®ˆä¼°è®¡
        
        speedup = serial_time / parallel_time if parallel_time > 0 else 0
        
        return {
            'mode': 'parallel',
            'serial_time': serial_time,
            'parallel_time': parallel_time,
            'speedup': speedup,
            'dataset_size': len(self.samples),
            'efficiency': min(speedup / 4, 1.0) * 100  # å‡è®¾4æ ¸å¹¶è¡Œ
        }

    def __getitem__(self, idx: int) -> dict:
        sample = self.samples[idx]
        return {
            "seq_features": torch.from_numpy(sample["seq_features"]),
            "lstm1_input": torch.from_numpy(sample["lstm1_input"]),
            "missing_bool": torch.from_numpy(sample["missing_bool"]),
            "basin_id": torch.tensor(sample["basin_id"], dtype=torch.long),
            "target": torch.from_numpy(sample["target"])
        }

# ======================== å¤šçº¿ç¨‹æ•°æ®é¢„å¤„ç† =======================
def preprocess_batch_data(batch_data, config):
    """å¤šçº¿ç¨‹æ•°æ®é¢„å¤„ç†å‡½æ•°"""
    device = config.DEVICE
    # æ•°æ®ç§»åˆ°è®¾å¤‡
    seq_features = batch_data["seq_features"].to(device)
    lstm1_input = batch_data["lstm1_input"].to(device)
    missing_bool = batch_data["missing_bool"].to(device)
    basin_ids = batch_data["basin_id"].to(device)
    target = batch_data["target"].to(device).unsqueeze(-1)  # (batch, 1)
    return {
        'seq_features': seq_features,
        'lstm1_input': lstm1_input,
        'missing_bool': missing_bool,
        'basin_ids': basin_ids,
        'target': target
    }

def parallel_preprocess_batches(batches, config, max_workers=4):
    """å¹¶è¡Œé¢„å¤„ç†å¤šä¸ªæ‰¹æ¬¡"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(preprocess_batch_data, batch, config) for batch in batches]
        results = [future.result() for future in futures]
    return [result for result in results if result is not None]

# ======================== DataLoaderå·¥å‚å‡½æ•° =======================
def create_hydrology_dataloaders(config, use_parallel=False, use_multithreading=True):
    """
    åˆ›å»ºæ°´æ–‡æ•°æ®é›†çš„DataLoader
    
    å‚æ•°:
        config: é…ç½®å¯¹è±¡
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œæ•°æ®é›†
        use_multithreading: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹æ•°æ®åŠ è½½
    
    è¿”å›:
        train_dataset, train_loader, val_dataset, val_loader: è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†åŠDataLoader
    """
    print(f"\n{'='*30} åŠ è½½æ•°æ® {'='*30}")
    
    # é€‰æ‹©æ•°æ®é›†ç±»å‹
    dataset_type = "å¹¶è¡Œ" if use_parallel else "ä¸²è¡Œ"
    print(f"ğŸ“Š ä½¿ç”¨{dataset_type}æ•°æ®é›†")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆç°åœ¨ç»Ÿä¸€ä½¿ç”¨HydrologyDatasetç±»ï¼‰
    train_dataset = HydrologyDataset(config.TRAIN_BASIN_IDS, config, mode="train", use_parallel=use_parallel)
    val_dataset = HydrologyDataset(config.VAL_BASIN_IDS, config, mode="val", use_parallel=use_parallel)
    
    if len(train_dataset) == 0:
        raise ValueError("âš ï¸  è®­ç»ƒé›†æ— æœ‰æ•ˆæ ·æœ¬ï¼ˆæ‰€æœ‰æ ·æœ¬æ ‡ç­¾å‡ä¸ºNaNï¼‰ï¼")
    if len(val_dataset) == 0:
        raise ValueError("âš ï¸  éªŒè¯é›†æ— æœ‰æ•ˆæ ·æœ¬ï¼ˆæ‰€æœ‰æ ·æœ¬æ ‡ç­¾å‡ä¸ºNaNï¼‰ï¼")
    
    # æ•°æ®åŠ è½½å™¨é…ç½®ï¼ˆWindowsç³»ç»Ÿä¸‹é¿å…å¤šè¿›ç¨‹pickleé—®é¢˜ï¼‰
    import platform
    is_windows = platform.system() == 'Windows'
    
    if is_windows:
        # Windowsç³»ç»Ÿï¼šä½¿ç”¨ä¸»è¿›ç¨‹æ•°æ®åŠ è½½ï¼Œé¿å…pickleåºåˆ—åŒ–é—®é¢˜
        num_workers = 0
        print(f"ğŸªŸ Windowsç³»ç»Ÿï¼šä½¿ç”¨ä¸»è¿›ç¨‹æ•°æ®åŠ è½½ï¼ˆnum_workers=0ï¼‰")
    else:
        # Linux/Macç³»ç»Ÿï¼šå¯ä»¥ä½¿ç”¨å¤šè¿›ç¨‹
        num_workers = min(4, multiprocessing.cpu_count()) if use_multithreading else 0
        print(f"ğŸ§ éWindowsç³»ç»Ÿï¼šä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ï¼ˆnum_workers={num_workers}ï¼‰")
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
        persistent_workers=False if is_windows else (num_workers > 0)  # Windowsä¸‹ç¦ç”¨
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
        persistent_workers=False if is_windows else (num_workers > 0)  # Windowsä¸‹ç¦ç”¨
    )
    
    return train_dataset, train_loader, val_dataset, val_loader