"""
æ¢¯åº¦å¼•å¯¼çš„æ­£ä½™å¼¦ä¼˜åŒ–ç®—æ³• (Gradient-Guided Sine Cosine Algorithm)
ç”¨äºSCA2LSTMæ¨¡å‹çš„å‚æ•°ä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Dict, Optional, Tuple


class SCAOptimizer(optim.Optimizer):
    """
    æ¢¯åº¦å¼•å¯¼çš„æ­£ä½™å¼¦ä¼˜åŒ–ç®—æ³•ä¼˜åŒ–å™¨
    
    ç‰¹ç‚¹ï¼š
    - åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯æŒ‡å¯¼æœç´¢æ–¹å‘
    - è‡ªé€‚åº”è°ƒæ•´æœç´¢èŒƒå›´
    - æ”¯æŒåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
    - ä¸PyTorch autogradå®Œå…¨å…¼å®¹
    """
    
    def __init__(self, params, lr=0.01, population_size=20, a_max=2.0, 
                 momentum=0.9, gradient_weight=0.7, adaptive_lr=True):
        """
        åˆå§‹åŒ–æ¢¯åº¦å¼•å¯¼SCAä¼˜åŒ–å™¨
        
        å‚æ•°:
        -----------
        params : iterable
            æ¨¡å‹å‚æ•°è¿­ä»£å™¨
        lr : float, default=0.01
            åŸºç¡€å­¦ä¹ ç‡
        population_size : int, default=20
            ç§ç¾¤å¤§å°ï¼ˆå€™é€‰è§£æ•°é‡ï¼‰
        a_max : float, default=2.0
            æ­£å¼¦/ä½™å¼¦æŒ¯å¹…çš„æœ€å¤§å€¼
        momentum : float, default=0.9
            åŠ¨é‡ç³»æ•°
        gradient_weight : float, default=0.7
            æ¢¯åº¦å¼•å¯¼çš„æƒé‡ï¼ˆ0-1ä¹‹é—´ï¼‰
        adaptive_lr : bool, default=True
            æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡
        """
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= momentum:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if not 0.0 <= gradient_weight <= 1.0:
            raise ValueError(f"Invalid gradient weight: {gradient_weight}")
        
        defaults = dict(lr=lr, population_size=population_size, a_max=a_max,
                       momentum=momentum, gradient_weight=gradient_weight,
                       adaptive_lr=adaptive_lr)
        super(SCAOptimizer, self).__init__(params, defaults)
        
        # å­˜å‚¨æ¢¯åº¦ä¿¡æ¯
        self.gradients = {}
        
        # è¿­ä»£è®¡æ•°
        self.iteration = 0
    
    def store_gradients(self, loss=None):
        """å­˜å‚¨æ¢¯åº¦ä¿¡æ¯"""
        # å¦‚æœå·²ç»è®¡ç®—äº†æ¢¯åº¦ï¼Œç›´æ¥å­˜å‚¨
        param_idx = 0
        for param_group in self.param_groups:
            for param in param_group['params']:
                if param.requires_grad and param.grad is not None:
                    self.gradients[param_idx] = param.grad.clone()
                    param_idx += 1
    
    def _get_gradient_direction(self, param_idx, param):
        """è·å–æ¢¯åº¦æ–¹å‘"""
        if param_idx in self.gradients:
            gradient = self.gradients[param_idx]
            grad_norm = torch.norm(gradient)
            if grad_norm > 1e-8:
                return -gradient / grad_norm  # è´Ÿæ¢¯åº¦æ–¹å‘
        return torch.zeros_like(param.data)
    
    def _sca_update(self, param_data, gradient_direction, lr, gradient_weight, a_max):
        """SCAä½ç½®æ›´æ–°ï¼ˆå¢åŠ ç¨³å®šæ€§ï¼‰"""
        # SCAå‚æ•°ï¼ˆéšè¿­ä»£è¡°å‡ï¼Œå¢åŠ ç¨³å®šæ€§ï¼‰
        a = max(0.2, a_max - (a_max * self.iteration) / 3000)  # å‘¨æœŸä»2000å¢è‡³3000ï¼Œæœ€å°å€¼ä»0.1å¢è‡³0.2
        r1 = a * (2 * np.random.rand() - 1)
        r2 = 2 * np.pi * np.random.rand()
        r3 = 2 * np.random.rand()
        r4 = np.random.rand()
        
        # è®¡ç®—SCAç§»åŠ¨ï¼ˆå¢åŠ è¾¹ç•Œæ£€æŸ¥ï¼‰
        param_np = param_data.cpu().numpy()
        if r4 < 0.5:
            movement = r1 * np.sin(r2) * np.abs(r3 * 0.05 * param_np)  # å¹…åº¦ä»0.1é™è‡³0.05
        else:
            movement = r1 * np.cos(r2) * np.abs(r3 * 0.05 * param_np)  # å¹…åº¦ä»0.1é™è‡³0.05
        
        # é™åˆ¶ç§»åŠ¨èŒƒå›´ï¼Œé¿å…è¿‡å¤§æ›´æ–°
        movement = np.clip(movement, -0.05, 0.05)  # èŒƒå›´ä»0.1é™è‡³0.05
        sca_movement = torch.from_numpy(movement).to(param_data.device)
        
        # æ¢¯åº¦å¼•å¯¼ç§»åŠ¨ï¼ˆå¢åŠ è£å‰ªï¼‰
        grad_norm = torch.norm(gradient_direction)
        if grad_norm > 1.0:  # æ¢¯åº¦è£å‰ª
            gradient_direction = gradient_direction / grad_norm
        
        grad_movement = gradient_weight * lr * gradient_direction
        
        # ç»“åˆä¸¤ç§ç§»åŠ¨ï¼ˆåŠ¨æ€æƒé‡è°ƒæ•´ï¼‰
        # éšç€è®­ç»ƒè¿›è¡Œï¼Œé€æ¸å¢åŠ æ¢¯åº¦æƒé‡ï¼Œå‡å°‘éšæœºæ¢ç´¢
        adaptive_gradient_weight = min(gradient_weight + self.iteration / 10000, 0.8)
        total_movement = (1 - adaptive_gradient_weight) * sca_movement + adaptive_gradient_weight * grad_movement
        
        # æœ€ç»ˆè£å‰ªï¼Œç¡®ä¿æ›´æ–°ä¸ä¼šè¿‡å¤§
        total_movement = torch.clamp(total_movement, -0.05, 0.05)
        
        return total_movement
    
    def step(self, closure=None):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        loss = None
        if closure is not None:
            loss = closure()
        
        # æ›´æ–°æ¯ä¸ªå‚æ•°
        for group in self.param_groups:
            lr = group['lr']
            gradient_weight = group['gradient_weight']
            a_max = group['a_max']
            
            param_idx = 0
            for param in group['params']:
                if param.requires_grad:
                    # è·å–æ¢¯åº¦æ–¹å‘
                    gradient_direction = self._get_gradient_direction(param_idx, param)
                    
                    # SCAæ›´æ–°
                    movement = self._sca_update(param.data, gradient_direction, lr, gradient_weight, a_max)
                    
                    # æ›´æ–°å‚æ•°
                    param.data.add_(movement)
                    
                    param_idx += 1
        
        self.iteration += 1
        return loss
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        super(SCAOptimizer, self).zero_grad()


# æµ‹è¯•å‡½æ•°
def test_sca_optimizer():
    """æµ‹è¯•SCAä¼˜åŒ–å™¨"""
    print("ğŸ§ª æµ‹è¯•æ¢¯åº¦å¼•å¯¼SCAä¼˜åŒ–å™¨...")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 5),
        nn.ReLU(),
        nn.Linear(5, 1)
    )
    
    # ä½¿ç”¨SCAä¼˜åŒ–å™¨
    optimizer = SCAOptimizer(model.parameters(), lr=0.01, population_size=10)
    criterion = nn.MSELoss()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    X = torch.randn(32, 10)
    y = torch.randn(32, 1)
    
    # è®­ç»ƒå‡ æ­¥
    for epoch in range(5):
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        output = model(X)
        loss = criterion(output, y)
        
        # å­˜å‚¨æ¢¯åº¦ä¿¡æ¯
        loss.backward()
        optimizer.store_gradients()
        
        # ä¼˜åŒ–æ­¥éª¤
        optimizer.step()
        
        print(f"Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    print("âœ… SCAä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_sca_optimizer()