#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCA2LSTM æµåŸŸå¾®è°ƒè„šæœ¬
åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šå¯¹ç‰¹å®šæµåŸŸè¿›è¡Œå¾®è°ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import os
import time
import argparse
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from sca2lstm import SCA2LSTM, load_config
from hydrologyDataset import HydrologyDataset
from utils import plot_training_curves, plot_prediction_comparison, plot_loss_distribution, plot_feature_weights_heatmap

def load_pretrained_model(model_path, config, device):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    
    Args:
        model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        config: é…ç½®å¯¹è±¡
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        checkpoint: æ£€æŸ¥ç‚¹ä¿¡æ¯
    """
    print(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = SCA2LSTM(config).to(device)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"ğŸ“Š åŸå§‹è®­ç»ƒä¿¡æ¯:")
    print(f"   - è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'æœªçŸ¥')}")
    best_val_loss = checkpoint.get('best_val_loss', 'æœªçŸ¥')
    if isinstance(best_val_loss, (int, float)):
        print(f"   - éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    else:
        print(f"   - éªŒè¯æŸå¤±: {best_val_loss}")
    
    return model, checkpoint

def freeze_layers(model, freeze_embedding=True, freeze_lstm1=True, freeze_weight_head=False):
    """
    å†»ç»“æ¨¡å‹å±‚
    
    Args:
        model: SCA2LSTMæ¨¡å‹
        freeze_embedding: æ˜¯å¦å†»ç»“åµŒå…¥å±‚
        freeze_lstm1: æ˜¯å¦å†»ç»“LSTM1å±‚
        freeze_weight_head: æ˜¯å¦å†»ç»“æƒé‡å¤´
    """
    print("ğŸ”’ è®¾ç½®å±‚å†»ç»“ç­–ç•¥:")
    
    # å†»ç»“åµŒå…¥å±‚
    if freeze_embedding:
        for param in model.basin_embedding.parameters():
            param.requires_grad = False
        print("   - æµåŸŸåµŒå…¥å±‚: å†»ç»“")
    else:
        print("   - æµåŸŸåµŒå…¥å±‚: å¯è®­ç»ƒ")
    
    # å†»ç»“LSTM1å±‚
    if freeze_lstm1:
        for param in model.lstm1_cell.parameters():
            param.requires_grad = False
        print("   - LSTM1å±‚: å†»ç»“")
    else:
        print("   - LSTM1å±‚: å¯è®­ç»ƒ")
    
    # å†»ç»“æƒé‡å¤´
    if freeze_weight_head:
        for param in model.weight_head.parameters():
            param.requires_grad = False
        print("   - æƒé‡å¤´: å†»ç»“")
    else:
        print("   - æƒé‡å¤´: å¯è®­ç»ƒ")
    
    # LSTM2å±‚å’Œé¢„æµ‹å¤´ä¿æŒå¯è®­ç»ƒï¼ˆå¾®è°ƒé‡ç‚¹ï¼‰
    for param in model.lstm2_cell.parameters():
        param.requires_grad = True
    for param in model.predict_head.parameters():
        param.requires_grad = True
    
    print("   - LSTM2å±‚: å¯è®­ç»ƒ")
    print("   - é¢„æµ‹å¤´: å¯è®­ç»ƒ")

def prepare_basin_specific_data(target_basin_id, config, fine_tune_ratio=0.8):
    """
    å‡†å¤‡ç‰¹å®šæµåŸŸçš„å¾®è°ƒå’ŒéªŒè¯æ•°æ®
    
    Args:
        target_basin_id: ç›®æ ‡æµåŸŸID
        config: é…ç½®å¯¹è±¡
        fine_tune_ratio: å¾®è°ƒæ•°æ®æ¯”ä¾‹
    
    Returns:
        fine_tune_dataset: å¾®è°ƒæ•°æ®é›†
        val_dataset: éªŒè¯æ•°æ®é›†
    """
    print(f"ğŸ“Š å‡†å¤‡æµåŸŸ {target_basin_id} çš„å¾®è°ƒæ•°æ®")
    
    # æ£€æŸ¥ç›®æ ‡æµåŸŸæ˜¯å¦å­˜åœ¨æ•°æ®
    data_path = os.path.join(config.DATA_INPUT_DIR,str(target_basin_id), f"model_input_{target_basin_id}.csv")
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æµåŸŸ {target_basin_id} çš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(data_path)
    total_samples = len(df)
    
    print(f"ğŸ“ˆ æµåŸŸ {target_basin_id} æ•°æ®æ¦‚å†µ:")
    print(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   - æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
    
    # åˆ›å»ºå®Œæ•´çš„æ•°æ®é›†ï¼ˆç”¨äºæ•°æ®åˆ†å‰²ï¼‰
    full_dataset = HydrologyDataset(
        basin_ids=[target_basin_id], 
        config=config, 
        mode="fine_tune",
        use_parallel=False  # å¾®è°ƒæ—¶ç¦ç”¨å¹¶è¡Œ
    )
    
    # åˆ†å‰²æ•°æ®é›†
    total_size = len(full_dataset)
    fine_tune_size = int(total_size * fine_tune_ratio)
    val_size = total_size - fine_tune_size
    
    fine_tune_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, 
        [fine_tune_size, val_size],
        generator=torch.Generator().manual_seed(config.SEED)
    )
    
    print(f"   - å¾®è°ƒæ ·æœ¬æ•°: {len(fine_tune_dataset)}")
    print(f"   - éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    
    return fine_tune_dataset, val_dataset

def fine_tune_one_epoch(model, dataloader, criterion, optimizer, config, epoch):
    """
    å¾®è°ƒå•ä¸ªepoch
    
    Args:
        model: SCA2LSTMæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        config: é…ç½®å¯¹è±¡
        epoch: å½“å‰è½®æ¬¡
    
    Returns:
        avg_loss: å¹³å‡æŸå¤±
    """
    model.train()
    total_loss = 0
    
    progress_bar = tqdm(dataloader, desc=f"å¾®è°ƒ Epoch {epoch+1}")
    
    for batch_idx, batch_data in enumerate(progress_bar):
        # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
        seq_features = batch_data["seq_features"].to(config.DEVICE)
        lstm1_input = batch_data["lstm1_input"].to(config.DEVICE)
        missing_bool = batch_data["missing_bool"].to(config.DEVICE)
        basin_ids = batch_data["basin_id"].to(config.DEVICE)
        target = batch_data["target"].to(config.DEVICE)
        
        # åˆå§‹åŒ–residualä¸º0ï¼ˆæ¼”ç¤ºæ•°æ®ä¸­æ²¡æœ‰residualï¼‰
        residual = torch.zeros((target.size(0), 1), device=config.DEVICE)
        
        # å‰å‘ä¼ æ’­
        pred = model(
            seq_features=seq_features,
            lstm1_input=lstm1_input,
            missing_bool=missing_bool,
            basin_ids=basin_ids,
            residual=residual,
            return_weights=False
        )
        
        # æ£€æŸ¥é¢„æµ‹å€¼çš„æœ‰æ•ˆæ€§
        if torch.isnan(pred).any() or torch.isinf(pred).any():
            print(f"âš ï¸  æ‰¹æ¬¡{batch_idx}åŒ…å«NaN/Infé¢„æµ‹å€¼ï¼Œè·³è¿‡")
            continue
            
        # è®¡ç®—æŸå¤±
        loss = criterion(pred.squeeze(), target)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item() * target.size(0)
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'æŸå¤±': f'{loss.item():.6f}',
            'å¹³å‡æŸå¤±': f'{total_loss / ((batch_idx + 1) * target.size(0)):.6f}'
        })
    
    return total_loss / len(dataloader.dataset)

def validate_fine_tune(model, dataloader, criterion, config, epoch):
    """
    å¾®è°ƒéªŒè¯
    
    Args:
        model: SCA2LSTMæ¨¡å‹
        dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        config: é…ç½®å¯¹è±¡
        epoch: å½“å‰è½®æ¬¡
    
    Returns:
        avg_loss: å¹³å‡éªŒè¯æŸå¤±
    """
    model.eval()
    total_loss = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch_data in tqdm(dataloader, desc=f"éªŒè¯ Epoch {epoch+1}"):
            # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
            seq_features = batch_data["seq_features"].to(config.DEVICE)
            lstm1_input = batch_data["lstm1_input"].to(config.DEVICE)
            missing_bool = batch_data["missing_bool"].to(config.DEVICE)
            basin_ids = batch_data["basin_id"].to(config.DEVICE)
            target = batch_data["target"].to(config.DEVICE)
            
            # åˆå§‹åŒ–residualä¸º0ï¼ˆæ¼”ç¤ºæ•°æ®ä¸­æ²¡æœ‰residualï¼‰
            residual = torch.zeros((target.size(0), 1), device=config.DEVICE)
            
            # å‰å‘ä¼ æ’­
            pred = model(
                seq_features=seq_features,
                lstm1_input=lstm1_input,
                missing_bool=missing_bool,
                basin_ids=basin_ids,
                residual=residual,
                return_weights=False
            )
            
            # æ£€æŸ¥é¢„æµ‹å€¼çš„æœ‰æ•ˆæ€§
            if torch.isnan(pred).any() or torch.isinf(pred).any():
                print(f"âš ï¸  éªŒè¯æ‰¹æ¬¡åŒ…å«NaN/Infé¢„æµ‹å€¼ï¼Œè·³è¿‡")
                continue
                
            # è®¡ç®—æŸå¤±
            loss = criterion(pred.squeeze(), target)
            total_loss += loss.item() * target.size(0)
            
            # æ”¶é›†é¢„æµ‹ç»“æœï¼ˆè¿‡æ»¤NaNå€¼ï¼‰
            pred_numpy = pred.squeeze().cpu().numpy()
            target_numpy = target.cpu().numpy()
            
            # ç¡®ä¿æ•°ç»„æ˜¯ä¸€ç»´çš„
            if pred_numpy.ndim > 1:
                pred_numpy = pred_numpy.flatten()
            if target_numpy.ndim > 1:
                target_numpy = target_numpy.flatten()
            
            # åªæ”¶é›†æœ‰æ•ˆçš„é¢„æµ‹å€¼
            valid_mask = ~(np.isnan(pred_numpy) | np.isinf(pred_numpy) | np.isnan(target_numpy) | np.isinf(target_numpy))
            if valid_mask.any():
                all_preds.extend(pred_numpy[valid_mask])
                all_targets.extend(target_numpy[valid_mask])
    
    avg_loss = total_loss / len(dataloader.dataset)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®
    if len(all_preds) == 0 or len(all_targets) == 0:
        print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹æ•°æ®ç”¨äºè¯„ä¼°")
        return float('inf')
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))
    mae = mean_absolute_error(all_targets, all_preds)
    r2 = r2_score(all_targets, all_preds)
    
    print(f"ğŸ“Š éªŒè¯ç»“æœ:")
    print(f"   - å¹³å‡æŸå¤±: {avg_loss:.6f}")
    print(f"   - RMSE: {rmse:.6f}")
    print(f"   - MAE: {mae:.6f}")
    print(f"   - RÂ²: {r2:.6f}")
    
    return avg_loss

def fine_tune_basin(target_basin_id, config, args):
    """
    å¯¹ç‰¹å®šæµåŸŸè¿›è¡Œå¾®è°ƒ
    
    Args:
        target_basin_id: ç›®æ ‡æµåŸŸID
        config: é…ç½®å¯¹è±¡
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print(f"\n{'='*50}")
    print(f"ğŸ¯ å¼€å§‹å¾®è°ƒæµåŸŸ: {target_basin_id}")
    print(f"{'='*50}")
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    config.DEVICE = device
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model, checkpoint = load_pretrained_model(args.pretrained_model, config, device)
    
    # è®¾ç½®å±‚å†»ç»“ç­–ç•¥
    freeze_layers(
        model, 
        freeze_embedding=not args.unfreeze_embedding,
        freeze_lstm1=not args.unfreeze_lstm1,
        freeze_weight_head=not args.unfreeze_weight_head
    )
    
    # å‡†å¤‡å¾®è°ƒæ•°æ®
    fine_tune_dataset, val_dataset = prepare_basin_specific_data(
        target_basin_id, config, args.fine_tune_ratio
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    fine_tune_loader = DataLoader(
        fine_tune_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=0,  # å¾®è°ƒæ—¶ç¦ç”¨å¤šè¿›ç¨‹
        pin_memory=True if device.type == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )
    
    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒï¼‰
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.learning_rate,
        weight_decay=args.weight_decay,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        patience=args.patience // 2,
        factor=0.5,
        threshold=0.001,
        min_lr=1e-7
    )
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join('model_output', 'fine_tune', f'basin_{target_basin_id}', timestamp)
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    
    # å¾®è°ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹å¾®è°ƒ (æœ€å¤§è½®æ¬¡: {args.epochs})")
    
    for epoch in range(args.epochs):
        print(f"\nğŸ“Œ å¾®è°ƒè½®æ¬¡ {epoch+1}/{args.epochs}")
        
        # å¾®è°ƒ
        train_loss = fine_tune_one_epoch(
            model, fine_tune_loader, criterion, optimizer, config, epoch
        )
        train_losses.append(train_loss)
        
        # éªŒè¯
        val_loss = validate_fine_tune(
            model, val_loader, criterion, config, epoch
        )
        val_losses.append(val_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # æ—©åœé€»è¾‘
        if val_loss < best_val_loss - 1e-6:
            best_val_loss = val_loss
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(output_dir, f'best_model_basin_{target_basin_id}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_loss': best_val_loss,
                'config': {k: v for k, v in config.__dict__.items() if not k.startswith('__') and not callable(v)},
                'target_basin_id': target_basin_id,
                'fine_tune_config': {
                    'freeze_embedding': not args.unfreeze_embedding,
                    'freeze_lstm1': not args.unfreeze_lstm1,
                    'freeze_weight_head': not args.unfreeze_weight_head,
                    'learning_rate': args.learning_rate,
                    'batch_size': args.batch_size,
                    'fine_tune_ratio': args.fine_tune_ratio
                }
            }, best_model_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")
        else:
            patience_counter += 1
            print(f"âš ï¸  æ—©åœè®¡æ•°å™¨: {patience_counter}/{args.patience}")
            
            if patience_counter >= args.patience:
                print(f"âŒ æ—©åœè§¦å‘ï¼Œå¾®è°ƒç»“æŸ")
                break
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        plot_training_curves(train_losses, val_losses, save_dir=output_dir)
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸  è®­ç»ƒæ›²çº¿ç»˜åˆ¶å¤±è´¥: {str(e)}")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\n{'='*30}")
    print(f"ğŸ å¾®è°ƒå®Œæˆæ€»ç»“")
    print(f"{'='*30}")
    print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
    print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {output_dir}")
    
    return best_model_path

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='SCA2LSTM æµåŸŸå¾®è°ƒè„šæœ¬')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--target_basin', type=int, required=True, 
                       help='ç›®æ ‡æµåŸŸID')
    parser.add_argument('--pretrained_model', type=str, required=True,
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    
    # å¾®è°ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=20,
                       help='å¾®è°ƒè½®æ¬¡ (é»˜è®¤: 20)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 16)')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.0001)')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='æƒé‡è¡°å‡ (é»˜è®¤: 1e-5)')
    parser.add_argument('--patience', type=int, default=8,
                       help='æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 8)')
    parser.add_argument('--fine_tune_ratio', type=float, default=0.8,
                       help='å¾®è°ƒæ•°æ®æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    
    # å±‚è§£å†»å‚æ•°
    parser.add_argument('--unfreeze_embedding', action='store_true',
                       help='è§£å†»åµŒå…¥å±‚')
    parser.add_argument('--unfreeze_lstm1', action='store_true',
                       help='è§£å†»LSTM1å±‚')
    parser.add_argument('--unfreeze_weight_head', action='store_true',
                       help='è§£å†»æƒé‡å¤´')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--no_cuda', action='store_true',
                       help='ç¦ç”¨CUDA')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available() and not args.no_cuda:
        torch.cuda.manual_seed(args.seed)
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    try:
        # æ‰§è¡Œå¾®è°ƒ
        best_model_path = fine_tune_basin(args.target_basin, config, args)
        print(f"\nğŸ‰ æµåŸŸ {args.target_basin} å¾®è°ƒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“‚ æœ€ä½³æ¨¡å‹è·¯å¾„: {best_model_path}")
        
    except Exception as e:
        print(f"\nâŒ å¾®è°ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())