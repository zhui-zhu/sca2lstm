#!/usr/bin/env python3
"""
SCA2LSTM é¢„æµ‹è„šæœ¬
ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæµé‡é¢„æµ‹
"""

import torch
import numpy as np
import pandas as pd
import os
import sys
import argparse
from datetime import datetime, timedelta
from tqdm import tqdm
import matplotlib.pyplot as plt
import ast

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from sca2lstm import SCA2LSTM
from hydrologyDataset import HydrologyDataset

def load_config_from_file(config_path="run.config"):
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®å‚æ•°
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å¯¹è±¡
    """
    config_dict = {}
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # é€è¡Œè§£æé…ç½®æ–‡ä»¶
        for line in content.split('\n'):
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
            if not line or line.startswith('#'):
                continue
                
            # å¤„ç†åˆ—è¡¨ç±»å‹çš„é…ç½®
            if line.startswith('LSTM1_DERIVED_FEATS') or line.startswith('LSTM2_FEATURES'):
                key = line.split('=')[0].strip()
                # æå–åˆ—è¡¨å†…å®¹ï¼ˆåœ¨æ–¹æ‹¬å·ä¹‹é—´çš„å†…å®¹ï¼‰
                start_idx = line.find('[')
                end_idx = line.rfind(']')
                if start_idx != -1 and end_idx != -1:
                    list_content = line[start_idx:end_idx+1]
                    try:
                        config_dict[key] = ast.literal_eval(list_content)
                    except:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œæ‰‹åŠ¨è§£æ
                        items = []
                        item_content = line[start_idx+1:end_idx]
                        for item in item_content.split(','):
                            item = item.strip().strip('"\'')
                            if item:
                                items.append(item)
                        config_dict[key] = items
            
            # å¤„ç†ç®€å•é”®å€¼å¯¹
            elif '=' in line and not line.startswith('['):
                parts = line.split('=', 1)
                key = parts[0].strip()
                value = parts[1].strip()
                
                # ç§»é™¤æ³¨é‡Š
                if '#' in value:
                    value = value.split('#')[0].strip()
                
                # è½¬æ¢å€¼ç±»å‹
                if value.startswith('[') and value.endswith(']'):
                    # åˆ—è¡¨ç±»å‹
                    try:
                        config_dict[key] = ast.literal_eval(value)
                    except:
                        config_dict[key] = []
                elif value in ['True', 'False']:
                    # å¸ƒå°”ç±»å‹
                    config_dict[key] = value == 'True'
                elif value.isdigit():
                    # æ•´æ•°ç±»å‹
                    config_dict[key] = int(value)
                elif value.replace('.', '').isdigit():
                    # æµ®ç‚¹æ•°ç±»å‹
                    config_dict[key] = float(value)
                else:
                    # å­—ç¬¦ä¸²ç±»å‹ï¼Œç§»é™¤å¼•å·
                    config_dict[key] = value.strip('"\'')
    
    except FileNotFoundError:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None
    except Exception as e:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None
    
    return config_dict

class Config:
    """é¢„æµ‹é…ç½®ç±» - ä¼˜å…ˆä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°"""
    
    def __init__(self, config_path="run.config"):
        """åˆå§‹åŒ–é…ç½®ï¼Œä¼˜å…ˆä»é…ç½®æ–‡ä»¶åŠ è½½"""
        # é¦–å…ˆè®¾ç½®é»˜è®¤å€¼
        self._set_default_values()
        
        # ç„¶åä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        config_dict = load_config_from_file(config_path)
        if config_dict:
            self._load_from_dict(config_dict)
            print(f"âœ… é…ç½®å·²ä» {config_path} æ–‡ä»¶åŠ è½½")
        else:
            print("âš ï¸  ä½¿ç”¨é»˜è®¤é…ç½®")
    
    def _set_default_values(self):
        """è®¾ç½®é»˜è®¤é…ç½®å€¼"""
        # æ¨¡å‹ç‰¹å¾é…ç½®
        self.LSTM1_DERIVED_FEATS = [
            "ssi", "high_prec_running_days", "low_prec_running_days", "prec_7day_sum", "prec_30day_sum"
        ]
        self.LSTM2_FEATURES = [
            "precipitation", "peti", "temperature", "discharge_vol",
            "area", "dpsbar", "elev_mean", "aridity", "p_seasonality",
            "tawc", "porosity_cosby", "baseflow_index", "dwood_perc", "ewood_perc",
            "grass_perc", "urban_perc", "inwater_perc", "benchmark_catch", "reservoir_cap"
        ]
        
        # æ¨¡å‹å‚æ•°
        self.DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.SEQ_LEN = 3
        self.PRED_LEN = 1
        self.LSTM_HIDDEN_DIM = 64
        self.LSTM_LAYERS = 2
        self.DROPOUT = 0.3
        self.EMBEDDING_DIM = 16
        self.N_FEATURES = len(self.LSTM2_FEATURES)
        self.LSTM1_INPUT_DIM = len(self.LSTM1_DERIVED_FEATS) + 1  # 5ä¸ªè¡ç”Ÿç‰¹å¾ + 1ä¸ªLSTM2åé¦ˆæ®‹å·® = 6
        
        # æ•°æ®é…ç½®
        self.TARGET_COL = "discharge_vol"
        self.DATA_INPUT_DIR = "./model_input_data/"
        self.TRAIN_BASIN_IDS = [32006, 42003, 51001, 75003, 79005]
        self.VAL_BASIN_IDS = [75003]
        self.MIN_VALID_LABEL_RATIO = 0.3  # è®­ç»ƒé›†æµåŸŸå¿…é¡»åŒ…å«30%ä»¥ä¸Šæœ‰æ•ˆæ ‡ç­¾
        self.MIN_VALID_ROWS = 10000  # æ¯ä¸ªæµåŸŸå¿…é¡»åŒ…å«10000è¡Œæœ‰æ•ˆæ•°æ®
    
    def _load_from_dict(self, config_dict):
        """ä»å­—å…¸åŠ è½½é…ç½®"""
        # åˆ—è¡¨ç±»å‹é…ç½®
        if 'LSTM1_DERIVED_FEATS' in config_dict:
            self.LSTM1_DERIVED_FEATS = config_dict['LSTM1_DERIVED_FEATS']
        if 'LSTM2_FEATURES' in config_dict:
            self.LSTM2_FEATURES = config_dict['LSTM2_FEATURES']
        if 'TRAIN_BASIN_IDS' in config_dict:
            self.TRAIN_BASIN_IDS = config_dict['TRAIN_BASIN_IDS']
        if 'VAL_BASIN_IDS' in config_dict:
            self.VAL_BASIN_IDS = config_dict['VAL_BASIN_IDS']
        
        # å­—ç¬¦ä¸²ç±»å‹é…ç½®
        if 'TARGET_COL' in config_dict:
            self.TARGET_COL = config_dict['TARGET_COL']
        if 'DATA_INPUT_DIR' in config_dict:
            self.DATA_INPUT_DIR = config_dict['DATA_INPUT_DIR']
        if 'MODEL_SAVE_PATH' in config_dict:
            self.MODEL_SAVE_PATH = config_dict['MODEL_SAVE_PATH']
        
        # æ•´æ•°ç±»å‹é…ç½®
        if 'SEQ_LEN' in config_dict:
            self.SEQ_LEN = config_dict['SEQ_LEN']
        if 'PRED_LEN' in config_dict:
            self.PRED_LEN = config_dict['PRED_LEN']
        if 'LSTM_HIDDEN_DIM' in config_dict:
            self.LSTM_HIDDEN_DIM = config_dict['LSTM_HIDDEN_DIM']
        if 'LSTM_LAYERS' in config_dict:
            self.LSTM_LAYERS = config_dict['LSTM_LAYERS']
        if 'EMBEDDING_DIM' in config_dict:
            self.EMBEDDING_DIM = config_dict['EMBEDDING_DIM']
        if 'BATCH_SIZE' in config_dict:
            self.BATCH_SIZE = config_dict['BATCH_SIZE']
        if 'N_EPOCHS' in config_dict:
            self.N_EPOCHS = config_dict['N_EPOCHS']
        if 'PATIENCE' in config_dict:
            self.PATIENCE = config_dict['PATIENCE']
        if 'MIN_VALID_ROWS' in config_dict:
            self.MIN_VALID_ROWS = config_dict['MIN_VALID_ROWS']
        
        # æµ®ç‚¹æ•°ç±»å‹é…ç½®
        if 'DROPOUT' in config_dict:
            self.DROPOUT = config_dict['DROPOUT']
        if 'LR' in config_dict:
            self.LR = config_dict['LR']
        if 'MIN_VALID_LABEL_RATIO' in config_dict:
            self.MIN_VALID_LABEL_RATIO = config_dict['MIN_VALID_LABEL_RATIO']
        
        # é‡æ–°è®¡ç®—ä¾èµ–å­—æ®µ
        self.N_FEATURES = len(self.LSTM2_FEATURES)
        self.LSTM1_INPUT_DIM = len(self.LSTM1_DERIVED_FEATS) + 1

def load_model(model_path, basin_id):
    """
    åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        basin_id: ç›®æ ‡æµåŸŸID
    
    Returns:
        åŠ è½½å¥½çš„æ¨¡å‹
    """
    print(f"ğŸ“‚ åŠ è½½å¾®è°ƒæ¨¡å‹: {model_path}")
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # åˆ›å»ºæ¨¡å‹
    model = SCA2LSTM(config)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    checkpoint = torch.load(model_path, map_location=config.DEVICE)
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š å¾®è°ƒè½®æ¬¡: {checkpoint.get('epoch', 'æœªçŸ¥')}")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {checkpoint.get('train_loss', 'æœªçŸ¥')}")
        print(f"ğŸ“Š éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'æœªçŸ¥')}")
    else:
        model.load_state_dict(checkpoint)
        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    
    model.eval()
    model.to(config.DEVICE)
    
    return model, config

def predict_basin_flow(model, config, basin_id, num_samples=50):
    """
    é¢„æµ‹æŒ‡å®šæµåŸŸçš„æµé‡
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        config: é…ç½®å¯¹è±¡
        basin_id: æµåŸŸID
        num_samples: é¢„æµ‹æ ·æœ¬æ•°é‡
    
    Returns:
        é¢„æµ‹ç»“æœDataFrame
    """
    print(f"ğŸ§ª å¼€å§‹é¢„æµ‹æµåŸŸ {basin_id} çš„æµé‡")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = HydrologyDataset([basin_id], config, mode="test", use_parallel=False)
    
    if len(dataset) == 0:
        print(f"âŒ æµåŸŸ {basin_id} æ²¡æœ‰å¯ç”¨æ•°æ®")
        return None
    
    print(f"ğŸ“Š å¯ç”¨æ ·æœ¬æ•°: {len(dataset)}")
    
    # è·å–æ•°æ®
    all_predictions = []
    all_dates = []
    all_actuals = []
    
    # éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œé¢„æµ‹å±•ç¤º
    sample_indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)
    
    # è·å–è¯¥æµåŸŸçš„ç¼©æ”¾å‚æ•°
    discharge_min, discharge_max = get_discharge_scaler_params(str(basin_id), config.DATA_INPUT_DIR)
    
    with torch.no_grad():
        for idx in tqdm(sample_indices, desc="é¢„æµ‹è¿›åº¦"):
            # è·å–æ•°æ®
            sample_data = dataset[idx]
            seq_features = sample_data["seq_features"]
            lstm1_input = sample_data["lstm1_input"]
            missing_bool = sample_data["missing_bool"]
            basin_ids = sample_data["basin_id"]
            target = sample_data["target"]
            
            # è½¬æ¢ä¸ºbatchæ ¼å¼
            seq_features = seq_features.unsqueeze(0).to(config.DEVICE)
            lstm1_input = lstm1_input.unsqueeze(0).to(config.DEVICE)
            missing_bool = missing_bool.unsqueeze(0).to(config.DEVICE)
            basin_ids = torch.tensor([basin_id], dtype=torch.long).to(config.DEVICE)
            residual = torch.zeros(1, 1).to(config.DEVICE)
            
            # é¢„æµ‹
            prediction = model(seq_features, lstm1_input, missing_bool, basin_ids, residual, return_weights=False)
            
            # ä¿å­˜ç»“æœ
            pred_value = prediction.cpu().numpy().flatten()[0]
            target_value = target.cpu().numpy().flatten()[0] if isinstance(target, torch.Tensor) else target
            
            # å¯¹é¢„æµ‹å€¼å’ŒçœŸå®å€¼è¿›è¡Œåå½’ä¸€åŒ–å¤„ç†
            pred_value_denorm = denormalize_discharge(pred_value, discharge_min, discharge_max)
            target_value_denorm = denormalize_discharge(target_value, discharge_min, discharge_max)
            
            # è·å–æ—¥æœŸï¼ˆä»æ•°æ®é›†æ ·æœ¬ä¸­æå–ï¼‰
            sample_data = dataset.samples[idx]
            date = sample_data.get('date', dataset.data.iloc[idx]['date'] if hasattr(dataset, 'data') else None)
            
            all_predictions.append(pred_value_denorm)
            all_actuals.append(target_value_denorm)
            all_dates.append(date)
    
    # åˆ›å»ºç»“æœDataFrame
    results_df = pd.DataFrame({
        'date': all_dates,
        'basin_id': basin_id,
        'predicted_flow': all_predictions,
        'actual_flow': all_actuals,
        'abs_error': np.abs(np.array(all_predictions) - np.array(all_actuals))
    })
    
    return results_df

def get_output_directory():
    """åˆ›å»ºå¹¶è¿”å›æ—¶é—´æˆ³è¾“å‡ºç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"model_output/predict/{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def plot_water_level_comparison(results_df, basin_id, output_dir=None):
    """Plot water level comparison chart"""
    # Sort by date
    results_df = results_df.sort_values('date')
    
    # Use provided output directory or create new one
    if output_dir is None:
        output_dir = get_output_directory()
    
    # Create figure
    fig, axes = plt.subplots(2, 1, figsize=(15, 12))
    fig.suptitle(f'Water Level Prediction vs Actual Comparison - Basin {basin_id}', fontsize=16, fontweight='bold')
    
    # Plot 1: Time series comparison
    ax1 = axes[0]
    ax1.plot(results_df['date'], results_df['actual_flow'], 'b-', linewidth=2, 
            label='Actual Flow', alpha=0.8)
    ax1.plot(results_df['date'], results_df['predicted_flow'], 'r--', linewidth=2, 
            label='Predicted Flow', alpha=0.8)
    ax1.fill_between(results_df['date'], results_df['actual_flow'], 
                    results_df['predicted_flow'], alpha=0.3, color='gray', 
                    label='Error Area')
    
    ax1.set_xlabel('Date', fontsize=12)
    ax1.set_ylabel('Discharge (mÂ³/s)', fontsize=12)
    ax1.set_title('Time Series Comparison', fontsize=14)
    ax1.legend(fontsize=12)
    ax1.grid(True, alpha=0.3)
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    # Plot 2: Scatter plot (Predicted vs Actual)
    ax2 = axes[1]
    ax2.scatter(results_df['actual_flow'], results_df['predicted_flow'], 
               alpha=0.6, s=30, c='blue')
    
    # Add perfect prediction line
    min_val = min(results_df['actual_flow'].min(), results_df['predicted_flow'].min())
    max_val = max(results_df['actual_flow'].max(), results_df['predicted_flow'].max())
    ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, 
            label='Perfect Prediction Line')
    
    ax2.set_xlabel('Actual Flow (mÂ³/s)', fontsize=12)
    ax2.set_ylabel('Predicted Flow (mÂ³/s)', fontsize=12)
    ax2.set_title('Predicted vs Actual Scatter Plot', fontsize=14)
    ax2.legend(fontsize=12)
    ax2.grid(True, alpha=0.3)
    
    # Calculate statistics
    mae = results_df['abs_error'].mean()
    rmse = np.sqrt(np.mean((results_df['actual_flow'] - results_df['predicted_flow']) ** 2))
    correlation = results_df['actual_flow'].corr(results_df['predicted_flow'])
    
    # Add statistics info on scatter plot
    stats_text = f'MAE: {mae:.4f}\nRMSE: {rmse:.4f}\nCorrelation: {correlation:.4f}'
    ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes, 
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
            verticalalignment='top', fontsize=10)
    
    plt.tight_layout()
    
    # Save figure
    output_file = f"{output_dir}/water_level_comparison_basin_{basin_id}.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Water level comparison chart saved to: {output_file}")
    
    # Close figure to release memory
    plt.close(fig)

def fill_missing_data(data_df, start_date, end_date):
    """Fill missing data in the specified date range"""
    # Create complete date range
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    
    # Reindex to include all dates
    data_df = data_df.set_index('date').reindex(date_range)
    data_df.index.name = 'date'
    
    # Count missing values
    missing_count = data_df.isnull().sum().sum()
    total_count = len(data_df)
    
    if missing_count > 0:
        print(f"âš ï¸  Found {missing_count} missing values out of {total_count} total data points")
        
        # For each column with missing data
        for col in data_df.columns:
            if data_df[col].isnull().sum() > 0:
                missing_dates = data_df[data_df[col].isnull()].index
                print(f"ğŸ“… Column '{col}' missing data on dates: {missing_dates[:5].tolist()}...")
                
                # Use different strategies based on missing data amount
                if len(missing_dates) <= 2:
                    # For few missing values, use interpolation with neighboring days
                    data_df[col] = data_df[col].interpolate(method='linear', limit_direction='both')
                    print(f"âœ… Filled {len(missing_dates)} missing values using linear interpolation")
                else:
                    # For many missing values, use 7-day rolling mean
                    data_df[col] = data_df[col].fillna(data_df[col].rolling(window=7, min_periods=1, center=True).mean())
                    print(f"âœ… Filled {len(missing_dates)} missing values using 7-day rolling mean")
    
    return data_df.reset_index()

def predict_continuous_flow(model, config, basin_id, start_date, end_date):
    """
    Continuous prediction for specified date range
    
    Args:
        model: Trained model
        config: Configuration object
        basin_id: Basin ID
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
    
    Returns:
        Prediction results DataFrame
    """
    print(f"ğŸ§ª Starting continuous prediction for basin {basin_id} from {start_date} to {end_date}")
    
    # Convert dates to datetime objects
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    
    # Create dataset for the basin
    dataset = HydrologyDataset([basin_id], config, mode="test", use_parallel=False)
    
    if len(dataset) == 0:
        print(f"âŒ No available data for basin {basin_id}")
        return None
    
    # Get all data for the basin
    basin_data = dataset.data[dataset.data['basin_id'] == basin_id].copy()
    basin_data['date'] = pd.to_datetime(basin_data['date'])
    
    # Filter data within the specified date range
    mask = (basin_data['date'] >= start_date) & (basin_data['date'] <= end_date)
    target_data = basin_data[mask].copy()
    
    if len(target_data) == 0:
        print(f"âŒ No data available in the specified date range: {start_date} to {end_date}")
        return None
    
    # Fill missing data
    target_data = fill_missing_data(target_data, start_date, end_date)
    
    print(f"ğŸ“Š Processing {len(target_data)} days of data from {start_date.date()} to {end_date.date()}")
    
    # è·å–è¯¥æµåŸŸçš„ç¼©æ”¾å‚æ•°
    discharge_min, discharge_max = get_discharge_scaler_params(str(basin_id), config.DATA_INPUT_DIR)
    
    # Prepare results storage
    all_predictions = []
    all_actuals = []
    all_dates = []
    
    # Create a working copy of data for continuous prediction
    working_data = target_data.copy()
    
    with torch.no_grad():
        for i in tqdm(range(len(target_data)), desc="Continuous prediction progress"):
            current_date = target_data.iloc[i]['date']
            
            # For each day, we need 3 days of historical data
            if i < 3:
                # For the first few days, use actual historical data
                historical_end_idx = i
                historical_start_idx = max(0, i - 3)
            else:
                # For subsequent days, use a mix of actual and predicted data
                historical_end_idx = i
                historical_start_idx = i - 3
            
            # Prepare sequence data
            historical_data = working_data.iloc[historical_start_idx:historical_end_idx + 1].copy()
            
            if len(historical_data) < 3 and i > 0:
                # If we don't have enough historical data, skip or use available data
                print(f"âš ï¸  Insufficient historical data for {current_date.date()}, using available data")
                continue
            
            # Create a temporary dataset for this prediction
            temp_data = historical_data.tail(3)  # Use last 3 days
            
            if len(temp_data) < 3:
                continue
            
            # Convert to model input format
            # This is a simplified version - you may need to adapt based on your exact data structure
            try:
                # Extract features (this part needs to be adapted to your exact data structure)
                seq_features = []  # You'll need to implement this based on your data format
                lstm1_input = []   # You'll need to implement this based on your data format
                
                # For now, let's use a simpler approach by finding matching samples in the dataset
                matching_samples = []
                for j in range(len(dataset.samples)):
                    sample_date = dataset.samples[j].get('date', dataset.data.iloc[j]['date'])
                    if pd.to_datetime(sample_date) == current_date:
                        matching_samples.append(j)
                        break
                
                if matching_samples:
                    # Use the matching sample from dataset
                    sample_data = dataset[matching_samples[0]]
                    seq_features = sample_data["seq_features"].unsqueeze(0).to(config.DEVICE)
                    lstm1_input = sample_data["lstm1_input"].unsqueeze(0).to(config.DEVICE)
                    missing_bool = sample_data["missing_bool"].unsqueeze(0).to(config.DEVICE)
                    basin_ids = torch.tensor([basin_id], dtype=torch.long).to(config.DEVICE)
                    residual = torch.zeros(1, 1).to(config.DEVICE)
                    
                    # Get actual value
                    actual_value = target_data.iloc[i]['discharge_vol']
                    
                    # Make prediction
                    prediction = model(seq_features, lstm1_input, missing_bool, basin_ids, residual, return_weights=False)
                    pred_value = prediction.cpu().numpy().flatten()[0]
                    
                    # å¯¹é¢„æµ‹å€¼å’ŒçœŸå®å€¼è¿›è¡Œåå½’ä¸€åŒ–å¤„ç†
                    pred_value_denorm = denormalize_discharge(pred_value, discharge_min, discharge_max)
                    actual_value_denorm = denormalize_discharge(actual_value, discharge_min, discharge_max)
                    
                    # Store results
                    all_predictions.append(pred_value_denorm)
                    all_actuals.append(actual_value_denorm)
                    all_dates.append(current_date)
                    
                    # Update working data with prediction for next iterations
                    working_data.loc[working_data['date'] == current_date, 'discharge_vol'] = pred_value
                    
                else:
                    print(f"âš ï¸  No matching sample found for {current_date.date()}")
                    continue
                    
            except Exception as e:
                print(f"âš ï¸  Error processing {current_date.date()}: {str(e)}")
                continue
    
    # Create results DataFrame
    if all_predictions:
        results_df = pd.DataFrame({
            'date': all_dates,
            'basin_id': basin_id,
            'predicted_flow': all_predictions,
            'actual_flow': all_actuals,
            'abs_error': np.abs(np.array(all_predictions) - np.array(all_actuals))
        })
        
        print(f"âœ… Continuous prediction completed: {len(results_df)} days predicted")
        return results_df
    else:
        print("âŒ No predictions were made")
        return None

def plot_error_analysis(results_df, basin_id, output_dir=None):
    """Plot comprehensive error analysis chart"""
    # Sort by date
    results_df = results_df.sort_values('date')
    
    # Use provided output directory or create new one
    if output_dir is None:
        output_dir = get_output_directory()
    
    # Create error analysis figure
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'Error Analysis - Basin {basin_id}', fontsize=16, fontweight='bold')
    
    # Plot 1: Error time series
    ax1 = axes[0, 0]
    ax1.plot(results_df['date'], results_df['abs_error'], 'r-', linewidth=1, alpha=0.7)
    ax1.axhline(y=results_df['abs_error'].mean(), color='blue', linestyle='--', 
                label=f'Mean Error: {results_df["abs_error"].mean():.4f}')
    ax1.set_xlabel('Date', fontsize=12)
    ax1.set_ylabel('Absolute Error', fontsize=12)
    ax1.set_title('Error Time Series', fontsize=14)
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    # Plot 2: Error distribution histogram
    ax2 = axes[0, 1]
    ax2.hist(results_df['abs_error'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    ax2.axvline(x=results_df['abs_error'].mean(), color='red', linestyle='--', 
                label=f'Mean: {results_df["abs_error"].mean():.4f}')
    ax2.axvline(x=results_df['abs_error'].median(), color='green', linestyle='--', 
                label=f'Median: {results_df["abs_error"].median():.4f}')
    ax2.set_xlabel('Absolute Error', fontsize=12)
    ax2.set_ylabel('Frequency', fontsize=12)
    ax2.set_title('Error Distribution Histogram', fontsize=14)
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Relative error
    results_df['relative_error'] = np.abs((results_df['actual_flow'] - results_df['predicted_flow']) / results_df['actual_flow']) * 100
    ax3 = axes[1, 0]
    ax3.plot(results_df['date'], results_df['relative_error'], 'orange', linewidth=1, alpha=0.7)
    ax3.axhline(y=results_df['relative_error'].mean(), color='red', linestyle='--', 
                label=f'Mean Relative Error: {results_df["relative_error"].mean():.2f}%')
    ax3.set_xlabel('Date', fontsize=12)
    ax3.set_ylabel('Relative Error (%)', fontsize=12)
    ax3.set_title('Relative Error Time Series', fontsize=14)
    ax3.legend(fontsize=10)
    ax3.grid(True, alpha=0.3)
    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
    
    # Plot 4: Error by flow ranges
    ax4 = axes[1, 1]
    # Divide flow into several ranges
    flow_bins = pd.qcut(results_df['actual_flow'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    error_by_flow = results_df.groupby(flow_bins, observed=False)['abs_error'].mean()
    
    bars = ax4.bar(error_by_flow.index, error_by_flow.values, 
                   color=['red', 'orange', 'yellow', 'lightgreen', 'green'], alpha=0.7)
    ax4.set_xlabel('Flow Range', fontsize=12)
    ax4.set_ylabel('Mean Absolute Error', fontsize=12)
    ax4.set_title('Error by Flow Ranges', fontsize=14)
    ax4.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar, value in zip(bars, error_by_flow.values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{value:.3f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    
    # Save figure
    output_file = f"{output_dir}/error_analysis_basin_{basin_id}.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Error analysis chart saved to: {output_file}")
    
    # Close figure to release memory
    plt.close(fig)

def main():
    """Main function with command line arguments"""
    print("ğŸš€ SCA2LSTM Flow Prediction System")
    print("=" * 50)
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='SCA2LSTM Continuous Flow Prediction')
    parser.add_argument('--start', type=str, help='Start date (YYYY-MM-DD)', default='2000-01-01')
    parser.add_argument('--end', type=str, help='End date (YYYY-MM-DD)', default='2000-12-31')
    parser.add_argument('--basin', type=int, help='Basin ID', default=32006)
    parser.add_argument('--model', type=str, help='Model path', default=None)
    
    args = parser.parse_args()
    
    # Create unified timestamped output directory for this prediction run
    output_dir = get_output_directory()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # Set default model path if not provided
    if args.model is None:
        model_path = "model_output/fine_tune/basin_32006/20251123_163859/best_model_basin_32006.pth"
    else:
        model_path = args.model
    
    if not os.path.exists(model_path):
        print(f"âŒ Model file does not exist: {model_path}")
        print("Please confirm the fine-tuned model path is correct")
        return
    
    # Load model
    try:
        model, config = load_model(model_path, args.basin)
    except Exception as e:
        print(f"âŒ Model loading failed: {str(e)}")
        return
    
    # Determine prediction mode
    if args.start and args.end:
        # Continuous prediction mode
        print(f"\nğŸ§ª Starting continuous prediction for basin {args.basin}...")
        print(f"ğŸ“… Date range: {args.start} to {args.end}")
        
        results = predict_continuous_flow(model, config, args.basin, args.start, args.end)
        
        if results is not None and len(results) > 0:
            print(f"\nğŸ“Š Continuous Prediction Summary:")
            print(f"Prediction period: {args.start} to {args.end}")
            print(f"Total days predicted: {len(results)}")
            print(f"Average predicted flow: {results['predicted_flow'].mean():.4f}")
            print(f"Average actual flow: {results['actual_flow'].mean():.4f}")
            print(f"Mean absolute error: {results['abs_error'].mean():.4f}")
            print(f"Maximum predicted flow: {results['predicted_flow'].max():.4f}")
            print(f"Minimum predicted flow: {results['predicted_flow'].min():.4f}")
            print(f"Prediction standard deviation: {results['predicted_flow'].std():.4f}")
            
            # Save results
            output_file = f"{output_dir}/prediction_results_basin_{args.basin}_{args.start}_to_{args.end}.csv"
            results.to_csv(output_file, index=False)
            print(f"\nâœ… Prediction results saved to: {output_file}")
            
            # Generate water level comparison chart
            print("\nğŸ“Š Generating water level comparison chart...")
            plot_water_level_comparison(results, args.basin, output_dir)
            
            # Generate error analysis chart
            print("\nğŸ“Š Generating error analysis chart...")
            plot_error_analysis(results, args.basin, output_dir)
            
            # Show first 10 prediction results
            print(f"\nğŸ“ˆ First 10 prediction results:")
            print(results.head(10).to_string())
            
        else:
            print("âŒ Continuous prediction failed")
    
    else:
        # Original random sampling mode
        print(f"\nğŸ§ª Starting random sampling prediction for basin {args.basin}...")
        results = predict_basin_flow(model, config, args.basin, num_samples=100)
        
        if results is not None and len(results) > 0:
            print(f"\nğŸ“Š Prediction Summary:")
            print(f"Sample count: {len(results)}")
            print(f"Average predicted flow: {results['predicted_flow'].mean():.4f}")
            print(f"Average actual flow: {results['actual_flow'].mean():.4f}")
            print(f"Mean absolute error: {results['abs_error'].mean():.4f}")
            print(f"Maximum predicted flow: {results['predicted_flow'].max():.4f}")
            print(f"Minimum predicted flow: {results['predicted_flow'].min():.4f}")
            print(f"Prediction standard deviation: {results['predicted_flow'].std():.4f}")
            
            # Save results
            output_file = f"{output_dir}/prediction_results_basin_{args.basin}.csv"
            results.to_csv(output_file, index=False)
            print(f"\nâœ… Prediction results saved to: {output_file}")
            
            # Generate water level comparison chart
            print("\nğŸ“Š Generating water level comparison chart...")
            plot_water_level_comparison(results, args.basin, output_dir)
            
            # Generate error analysis chart
            print("\nğŸ“Š Generating error analysis chart...")
            plot_error_analysis(results, args.basin, output_dir)
            
            # Show first 10 prediction results
            print(f"\nğŸ“ˆ First 10 prediction results:")
            print(results.head(10).to_string())
            
        else:
            print("âŒ Prediction failed")

if __name__ == "__main__":
    main()