import numpy as np
import pandas as pd
from typing import Union, Tuple, List
import json
import matplotlib.pyplot as plt
import torch
import warnings
import os
warnings.filterwarnings('ignore')


def get_discharge_scaler_params(basin_id: str, scaler_dir: str = "model_input_data"):
    """
    è·å–æŒ‡å®šæµåŸŸçš„discharge_volç¼©æ”¾å‚æ•°ï¼ˆMin-Maxå½’ä¸€åŒ–çš„minå’Œmaxå€¼ï¼‰
    
    å‚æ•°:
    ---------
    basin_id : str
        æµåŸŸID
    scaler_dir : str, default="model_input_data"
        ç¼©æ”¾å‚æ•°æ–‡ä»¶æ‰€åœ¨ç›®å½•
        
    è¿”å›:
    ---------
    discharge_min : float
        discharge_volçš„æœ€å°å€¼
    discharge_max : float
        discharge_volçš„æœ€å¤§å€¼
        
    å¼‚å¸¸:
    ---------
    FileNotFoundError: å¦‚æœç¼©æ”¾å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨
    KeyError: å¦‚æœæ–‡ä»¶ä¸­ç¼ºå°‘discharge_volçš„ç¼©æ”¾å‚æ•°
    """
    scaler_file = f"{scaler_dir}/{basin_id}/ts_scaler_{basin_id}.json"
    
    try:
        with open(scaler_file, 'r', encoding='utf-8') as f:
            scaler_data = json.load(f)
        
        # è·å–discharge_volçš„ç¼©æ”¾å‚æ•°
        if "discharge_vol" not in scaler_data:
            raise KeyError(f"æµåŸŸ{basin_id}çš„ç¼©æ”¾å‚æ•°æ–‡ä»¶ä¸­ç¼ºå°‘'discharge_vol'å­—æ®µ")
        
        discharge_params = scaler_data["discharge_vol"]
        discharge_min = discharge_params["min"]
        discharge_max = discharge_params["max"]
        
        return discharge_min, discharge_max
        
    except FileNotFoundError:
        raise FileNotFoundError(f"æµåŸŸ{basin_id}çš„ç¼©æ”¾å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨ï¼š{scaler_file}")
    except json.JSONDecodeError as e:
        raise ValueError(f"æµåŸŸ{basin_id}çš„ç¼©æ”¾å‚æ•°æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼š{e}")


def denormalize_discharge(normalized_values: Union[float, np.ndarray, torch.Tensor], 
                         discharge_min: float, discharge_max: float):
    """
    åå½’ä¸€åŒ–discharge_volå€¼ï¼ˆå°†å½’ä¸€åŒ–å€¼è½¬æ¢ä¸ºçœŸå®å€¼ï¼‰
    
    å‚æ•°:
    ---------
    normalized_values : float, np.ndarray, or torch.Tensor
        å½’ä¸€åŒ–åçš„discharge_volå€¼ï¼ˆèŒƒå›´é€šå¸¸åœ¨0-1ä¹‹é—´ï¼‰
    discharge_min : float
        åŸå§‹æ•°æ®çš„æœ€å°å€¼
    discharge_max : float
        åŸå§‹æ•°æ®çš„æœ€å¤§å€¼
        
    è¿”å›:
    ---------
    denormalized_values : ä¸è¾“å…¥ç±»å‹ç›¸åŒ
        åå½’ä¸€åŒ–åçš„çœŸå®discharge_volå€¼
        
    å…¬å¼:
    ---------
    çœŸå®å€¼ = å½’ä¸€åŒ–å€¼ Ã— (max - min) + min
    """
    if isinstance(normalized_values, torch.Tensor):
        return normalized_values * (discharge_max - discharge_min) + discharge_min
    elif isinstance(normalized_values, np.ndarray):
        return normalized_values * (discharge_max - discharge_min) + discharge_min
    else:  # float or int
        return normalized_values * (discharge_max - discharge_min) + discharge_min


def identify_flood_events(
    discharge_series: Union[np.ndarray, pd.Series, List],
    threshold_method: str = "percentile",
    threshold_value: float = 90.0,
    min_duration: int = 3,
    min_interval: int = 5,
    smoothing_window: int = 3
) -> np.ndarray:
    """
    æ´ªæ°´è¯†åˆ«å‡½æ•° - åŸºäºæµé‡æ—¶åºæ•°æ®è¯†åˆ«æ´ªæ°´äº‹ä»¶
    
    å‚æ•°:
    -----------
    discharge_series : array-like
        æµé‡æ—¶åºæ•°æ® (å•ä½: mÂ³/s æˆ– mm/day)
    threshold_method : str, default="percentile"
        é˜ˆå€¼ç¡®å®šæ–¹æ³•: "percentile" (ç™¾åˆ†ä½æ•°), "mean_plus_std" (å‡å€¼+æ ‡å‡†å·®), "fixed" (å›ºå®šå€¼)
    threshold_value : float, default=95.0
        é˜ˆå€¼å‚æ•°: ç™¾åˆ†ä½æ•°(å¦‚95è¡¨ç¤º95%åˆ†ä½æ•°) æˆ– å›ºå®šé˜ˆå€¼ æˆ– æ ‡å‡†å·®å€æ•°
    min_duration : int, default=3
        æœ€å°æ´ªæ°´æŒç»­æ—¶é—´ (å¤©)
    min_interval : int, default=5
        ç›¸é‚»æ´ªæ°´äº‹ä»¶çš„æœ€å°é—´éš” (å¤©)
    smoothing_window : int, default=3
        å¹³æ»‘çª—å£å¤§å° (å¤©), ç”¨äºæ¶ˆé™¤å™ªå£°
    
    è¿”å›:
    -----------
    flood_mask : np.ndarray
        å¸ƒå°”æ•°ç»„, Trueè¡¨ç¤ºæ´ªæ°´æœŸ, Falseè¡¨ç¤ºéæ´ªæ°´æœŸ
    
    ç®—æ³•è¯´æ˜:
    -----------
    1. æ•°æ®å¹³æ»‘å¤„ç† (ç§»åŠ¨å¹³å‡)
    2. æ ¹æ®æŒ‡å®šæ–¹æ³•è®¡ç®—æ´ªæ°´é˜ˆå€¼
    3. è¯†åˆ«è¶…è¿‡é˜ˆå€¼çš„è¿ç»­æ—¶æ®µ
    4. åº”ç”¨æœ€å°æŒç»­æ—¶é—´è¿‡æ»¤
    5. åˆå¹¶ç›¸é‚»çš„æ´ªæ°´äº‹ä»¶ (é—´éš”å°äºmin_interval)
    """
    
    # è¾“å…¥éªŒè¯å’Œè½¬æ¢
    if isinstance(discharge_series, (list, pd.Series)):
        discharge_series = np.array(discharge_series, dtype=float)
    elif isinstance(discharge_series, np.ndarray):
        discharge_series = discharge_series.astype(float)
    else:
        raise TypeError("discharge_series å¿…é¡»æ˜¯ np.ndarray, pd.Series æˆ– list")
    
    if len(discharge_series) == 0:
        return np.array([], dtype=bool)
    
    if np.all(np.isnan(discharge_series)):
        return np.full(len(discharge_series), False, dtype=bool)
    
    # 1. æ•°æ®å¹³æ»‘å¤„ç†
    if smoothing_window > 1 and len(discharge_series) >= smoothing_window:
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡è¿›è¡Œå¹³æ»‘
        kernel = np.ones(smoothing_window) / smoothing_window
        smoothed_discharge = np.convolve(discharge_series, kernel, mode='same')
    else:
        smoothed_discharge = discharge_series.copy()
    
    # 2. è®¡ç®—æ´ªæ°´é˜ˆå€¼
    valid_data = smoothed_discharge[~np.isnan(smoothed_discharge)]
    
    if len(valid_data) == 0:
        return np.full(len(discharge_series), False, dtype=bool)
    
    if threshold_method == "percentile":
        # ç™¾åˆ†ä½æ•°æ–¹æ³• (é»˜è®¤95%åˆ†ä½æ•°)
        flood_threshold = np.percentile(valid_data, threshold_value)
    elif threshold_method == "mean_plus_std":
        # å‡å€¼ + nå€æ ‡å‡†å·®æ–¹æ³•
        mean_flow = np.mean(valid_data)
        std_flow = np.std(valid_data)
        flood_threshold = mean_flow + threshold_value * std_flow
    elif threshold_method == "fixed":
        # å›ºå®šé˜ˆå€¼æ–¹æ³•
        flood_threshold = threshold_value
    else:
        raise ValueError("threshold_method å¿…é¡»æ˜¯ 'percentile', 'mean_plus_std' æˆ– 'fixed'")
    
    # ç¡®ä¿é˜ˆå€¼ä¸ä½äºæ•°æ®çš„æœ€å°å€¼
    flood_threshold = max(flood_threshold, np.min(valid_data))
    
    # 3. è¯†åˆ«è¶…è¿‡é˜ˆå€¼çš„æ—¶æ®µ
    above_threshold = smoothed_discharge >= flood_threshold
    
    # 4. å¯»æ‰¾è¿ç»­çš„é«˜æµé‡æ—¶æ®µ
    flood_mask = np.full(len(discharge_series), False, dtype=bool)
    
    # æ‰¾åˆ°æ‰€æœ‰è¿ç»­çš„é«˜æµé‡æ®µ
    high_flow_periods = []
    start_idx = None
    
    for i, is_high in enumerate(above_threshold):
        if is_high and start_idx is None:
            start_idx = i
        elif not is_high and start_idx is not None:
            high_flow_periods.append((start_idx, i - 1))
            start_idx = None
    
    # å¤„ç†æœ€åä¸€ä¸ªæ®µ
    if start_idx is not None:
        high_flow_periods.append((start_idx, len(above_threshold) - 1))
    
    # 5. åº”ç”¨æœ€å°æŒç»­æ—¶é—´è¿‡æ»¤
    valid_periods = []
    for start, end in high_flow_periods:
        duration = end - start + 1
        if duration >= min_duration:
            valid_periods.append((start, end))
    
    # 6. åˆå¹¶ç›¸é‚»çš„æ´ªæ°´äº‹ä»¶
    if len(valid_periods) > 0:
        merged_periods = [valid_periods[0]]
        
        for current_start, current_end in valid_periods[1:]:
            last_start, last_end = merged_periods[-1]
            
            # å¦‚æœå½“å‰æ®µä¸ä¸Šä¸€æ®µé—´éš”å°äºmin_intervalï¼Œåˆ™åˆå¹¶
            if current_start - last_end <= min_interval:
                merged_periods[-1] = (last_start, current_end)
            else:
                merged_periods.append((current_start, current_end))
        
        # æ ‡è®°æœ€ç»ˆçš„æ´ªæ°´æ—¶æ®µ
        for start, end in merged_periods:
            flood_mask[start:end + 1] = True
    
    return flood_mask


def get_flood_statistics(
    discharge_series: Union[np.ndarray, pd.Series, List],
    flood_mask: np.ndarray
) -> dict:
    """
    è®¡ç®—æ´ªæ°´äº‹ä»¶çš„ç»Ÿè®¡ç‰¹å¾
    
    å‚æ•°:
    -----------
    discharge_series : array-like
        åŸå§‹æµé‡æ—¶åºæ•°æ®
    flood_mask : np.ndarray
        æ´ªæ°´è¯†åˆ«ç»“æœ (æ¥è‡ª identify_flood_events)
    
    è¿”å›:
    -----------
    stats : dict
        æ´ªæ°´ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
        - n_floods: æ´ªæ°´äº‹ä»¶æ•°é‡
        - total_flood_days: æ€»æ´ªæ°´å¤©æ•°
        - avg_flood_duration: å¹³å‡æ´ªæ°´æŒç»­æ—¶é—´
        - max_flood_duration: æœ€é•¿æ´ªæ°´æŒç»­æ—¶é—´
        - avg_flood_intensity: å¹³å‡æ´ªæ°´å¼ºåº¦
        - max_flood_intensity: æœ€å¤§æ´ªæ°´å¼ºåº¦
        - flood_frequency: æ´ªæ°´é¢‘ç‡ (%)
    """
    
    if isinstance(discharge_series, (list, pd.Series)):
        discharge_series = np.array(discharge_series, dtype=float)
    
    if len(discharge_series) != len(flood_mask):
        raise ValueError("discharge_series å’Œ flood_mask é•¿åº¦å¿…é¡»ä¸€è‡´")
    
    # åŸºæœ¬ç»Ÿè®¡
    total_days = len(flood_mask)
    flood_days = np.sum(flood_mask)
    flood_frequency = (flood_days / total_days) * 100 if total_days > 0 else 0
    
    # æ´ªæ°´äº‹ä»¶ç»Ÿè®¡
    flood_events = []
    start_idx = None
    
    for i, is_flood in enumerate(flood_mask):
        if is_flood and start_idx is None:
            start_idx = i
        elif not is_flood and start_idx is not None:
            flood_events.append((start_idx, i - 1))
            start_idx = None
    
    # å¤„ç†æœ€åä¸€ä¸ªäº‹ä»¶
    if start_idx is not None:
        flood_events.append((start_idx, len(flood_mask) - 1))
    
    n_floods = len(flood_events)
    
    if n_floods > 0:
        durations = [end - start + 1 for start, end in flood_events]
        avg_duration = np.mean(durations)
        max_duration = np.max(durations)
        
        # è®¡ç®—æ´ªæ°´å¼ºåº¦ (ä½¿ç”¨æ´ªæ°´æœŸé—´çš„å¹³å‡æµé‡)
        intensities = []
        for start, end in flood_events:
            flood_flow = discharge_series[start:end + 1]
            if len(flood_flow) > 0:
                intensities.append(np.mean(flood_flow))
        
        avg_intensity = np.mean(intensities) if intensities else 0
        max_intensity = np.max(intensities) if intensities else 0
    else:
        avg_duration = max_duration = avg_intensity = max_intensity = 0
    
    return {
        "n_floods": n_floods,
        "total_flood_days": flood_days,
        "flood_frequency": flood_frequency,
        "avg_flood_duration": avg_duration,
        "max_flood_duration": max_duration,
        "avg_flood_intensity": avg_intensity,
        "max_flood_intensity": max_intensity
    }


def evaluate_flood_prediction(
    true_discharge: Union[np.ndarray, pd.Series, List],
    pred_discharge: Union[np.ndarray, pd.Series, List],
    threshold_method: str = "percentile",
    threshold_value: float = 95.0
) -> dict:
    """
    è¯„ä¼°æ´ªæ°´é¢„æµ‹æ•ˆæœ - å¯¹æ¯”çœŸå®æµé‡å’Œé¢„æµ‹æµé‡çš„æ´ªæ°´è¯†åˆ«ç»“æœ
    
    å‚æ•°:
    -----------
    true_discharge : array-like
        çœŸå®æµé‡æ•°æ®
    pred_discharge : array-like
        é¢„æµ‹æµé‡æ•°æ®
    threshold_method : str
        æ´ªæ°´è¯†åˆ«é˜ˆå€¼æ–¹æ³•
    threshold_value : float
        æ´ªæ°´è¯†åˆ«é˜ˆå€¼å‚æ•°
    
    è¿”å›:
    -----------
    evaluation : dict
        æ´ªæ°´é¢„æµ‹è¯„ä¼°ç»“æœï¼ŒåŒ…æ‹¬:
        - true_floods: çœŸå®æ´ªæ°´äº‹ä»¶æ•°é‡
        - pred_floods: é¢„æµ‹æ´ªæ°´äº‹ä»¶æ•°é‡
        - true_flood_days: çœŸå®æ´ªæ°´å¤©æ•°
        - pred_flood_days: é¢„æµ‹æ´ªæ°´å¤©æ•°
        - flood_detection_rate: æ´ªæ°´æ£€æµ‹ç‡ (%)
        - false_alarm_rate: è¯¯æŠ¥ç‡ (%)
        - flood_day_accuracy: æ´ªæ°´æ—¥å‡†ç¡®ç‡ (%)
    """
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(true_discharge, (list, pd.Series)):
        true_discharge = np.array(true_discharge, dtype=float)
    if isinstance(pred_discharge, (list, pd.Series)):
        pred_discharge = np.array(pred_discharge, dtype=float)
    
    if len(true_discharge) != len(pred_discharge):
        raise ValueError("çœŸå®æµé‡å’Œé¢„æµ‹æµé‡é•¿åº¦å¿…é¡»ä¸€è‡´")
    
    # è¯†åˆ«æ´ªæ°´äº‹ä»¶
    true_flood_mask = identify_flood_events(
        true_discharge, 
        threshold_method=threshold_method,
        threshold_value=threshold_value
    )
    
    pred_flood_mask = identify_flood_events(
        pred_discharge,
        threshold_method=threshold_method, 
        threshold_value=threshold_value
    )
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    total_days = len(true_discharge)
    
    # æ´ªæ°´æ—¥ç»Ÿè®¡
    true_flood_days = np.sum(true_flood_mask)
    pred_flood_days = np.sum(pred_flood_mask)
    
    # æ··æ·†çŸ©é˜µè®¡ç®—
    true_positives = np.sum(true_flood_mask & pred_flood_mask)  # æ­£ç¡®è¯†åˆ«çš„æ´ªæ°´æ—¥
    false_positives = np.sum(~true_flood_mask & pred_flood_mask)  # è¯¯æŠ¥
    false_negatives = np.sum(true_flood_mask & ~pred_flood_mask)  # æ¼æŠ¥
    true_negatives = np.sum(~true_flood_mask & ~pred_flood_mask)  # æ­£ç¡®è¯†åˆ«çš„éæ´ªæ°´æ—¥
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    flood_detection_rate = (true_positives / true_flood_days * 100) if true_flood_days > 0 else 0
    false_alarm_rate = (false_positives / (false_positives + true_negatives) * 100) if (false_positives + true_negatives) > 0 else 0
    flood_day_accuracy = ((true_positives + true_negatives) / total_days * 100) if total_days > 0 else 0
    
    # è·å–æ´ªæ°´äº‹ä»¶ç»Ÿè®¡
    true_stats = get_flood_statistics(true_discharge, true_flood_mask)
    pred_stats = get_flood_statistics(pred_discharge, pred_flood_mask)
    
    return {
        "true_floods": true_stats["n_floods"],
        "pred_floods": pred_stats["n_floods"],
        "true_flood_days": true_flood_days,
        "pred_flood_days": pred_flood_days,
        "flood_detection_rate": flood_detection_rate,
        "false_alarm_rate": false_alarm_rate,
        "flood_day_accuracy": flood_day_accuracy,
        "true_flood_statistics": true_stats,
        "pred_flood_statistics": pred_stats,
        "confusion_matrix": {
            "true_positives": true_positives,
            "false_positives": false_positives,
            "false_negatives": false_negatives,
            "true_negatives": true_negatives
        }
    }


def test_flood_identification():
    """
    æµ‹è¯•æ´ªæ°´è¯†åˆ«å‡½æ•°
    """
    print("=== æ´ªæ°´è¯†åˆ«å‡½æ•°æµ‹è¯• ===")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæµé‡æ•°æ® (åŒ…å«æ˜æ˜¾çš„æ´ªæ°´äº‹ä»¶)
    np.random.seed(42)
    n_days = 365
    base_flow = 10.0  # åŸºç¡€æµé‡
    
    # ç”Ÿæˆå­£èŠ‚æ€§æµé‡æ¨¡å¼
    seasonal_pattern = 5.0 * np.sin(2 * np.pi * np.arange(n_days) / 365)
    
    # æ·»åŠ å‡ ä¸ªæ˜æ˜¾çš„æ´ªæ°´äº‹ä»¶
    flood_events = [
        (50, 60),   # ç¬¬50-60å¤©
        (120, 130), # ç¬¬120-130å¤©
        (200, 210), # ç¬¬200-210å¤©
        (280, 290), # ç¬¬280-290å¤©
    ]
    
    discharge_data = base_flow + seasonal_pattern + np.random.normal(0, 1, n_days)
    
    # åœ¨æ´ªæ°´äº‹ä»¶æœŸé—´å¢åŠ æµé‡
    for start, end in flood_events:
        flood_magnitude = np.random.uniform(15, 25)  # æ´ªæ°´å¼ºåº¦
        discharge_data[start:end+1] += flood_magnitude
    
    # ç¡®ä¿æ²¡æœ‰è´Ÿå€¼
    discharge_data = np.maximum(discharge_data, 0.1)
    
    print(f"ç”Ÿæˆ {n_days} å¤©çš„æ¨¡æ‹Ÿæµé‡æ•°æ®")
    print(f"æµé‡èŒƒå›´: {np.min(discharge_data):.2f} - {np.max(discharge_data):.2f} mÂ³/s")
    
    # æµ‹è¯•ä¸åŒçš„æ´ªæ°´è¯†åˆ«æ–¹æ³•
    methods = [
        ("percentile", 90.0),
        ("percentile", 95.0),
        ("mean_plus_std", 2.0),
    ]
    
    for method, value in methods:
        print(f"\n--- æ–¹æ³•: {method} (é˜ˆå€¼: {value}) ---")
        
        # è¯†åˆ«æ´ªæ°´
        flood_mask = identify_flood_events(
            discharge_data,
            threshold_method=method,
            threshold_value=value,
            min_duration=3,
            min_interval=5
        )
        
        # è®¡ç®—ç»Ÿè®¡
        stats = get_flood_statistics(discharge_data, flood_mask)
        
        print(f"æ´ªæ°´äº‹ä»¶æ•°é‡: {stats['n_floods']}")
        print(f"æ€»æ´ªæ°´å¤©æ•°: {stats['total_flood_days']}")
        print(f"æ´ªæ°´é¢‘ç‡: {stats['flood_frequency']:.1f}%")
        print(f"å¹³å‡æŒç»­æ—¶é—´: {stats['avg_flood_duration']:.1f} å¤©")
        print(f"å¹³å‡æ´ªæ°´å¼ºåº¦: {stats['avg_flood_intensity']:.2f} mÂ³/s")
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")


def visualize_lstm2_weights(model, sample_input, save_dir=None):
    """
    LSTM2 Weight Visualization - Display feature weights over time
    
    Parameters:
    -----------
    model : SCA2LSTM model
        Trained SCA2LSTM model
    sample_input : dict
        Dictionary containing model inputs
    save_dir : str, optional
        Directory to save images, if None only display
    
    Returns:
    -----------
    figs : list of matplotlib.figure.Figure
        List of generated figure objects
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    model.eval()
    with torch.no_grad():
        # å‡†å¤‡è¾“å…¥æ•°æ®
        device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')
        seq_features = sample_input["seq_features"].unsqueeze(0).to(device)
        lstm1_input = sample_input["lstm1_input"].unsqueeze(0).to(device)
        missing_bool = sample_input["missing_bool"].unsqueeze(0).to(device)
        basin_ids = sample_input["basin_id"].unsqueeze(0).to(device)
        
        # ä½¿ç”¨æ–°çš„åŒå‘åé¦ˆæ¶æ„è·å–LSTM1è¾“å‡º
        # åˆå§‹åŒ–éšçŠ¶æ€
        batch_size = lstm1_input.size(0)
        h1 = torch.zeros(batch_size, model.config.LSTM_HIDDEN_DIM).to(device)
        c1 = torch.zeros(batch_size, model.config.LSTM_HIDDEN_DIM).to(device)
        h2 = torch.zeros(batch_size, model.config.LSTM_HIDDEN_DIM).to(device)
        c2 = torch.zeros(batch_size, model.config.LSTM_HIDDEN_DIM).to(device)
        
        # é€šè¿‡æ—¶é—´æ­¥ä¼ é€’è·å–æœ€ç»ˆçš„éšçŠ¶æ€
        seq_len = lstm1_input.size(1)
        for t in range(seq_len):
            lstm1_input_t = lstm1_input[:, t, :]
            # LSTM1å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨é›¶åˆå§‹åŒ–çš„h2ï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒæƒé‡ç”Ÿæˆï¼‰
            lstm1_input_with_feedback = torch.cat([lstm1_input_t, h1, h2], dim=-1)
            h1, c1 = model.lstm1_cell(lstm1_input_with_feedback, (h1, c1))
        
        # ä½¿ç”¨æœ€ç»ˆçš„éšçŠ¶æ€ç”Ÿæˆç‰¹å¾æƒé‡
        lstm1_out = h1.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        feature_weights = model.weight_head(lstm1_out)
        
        # Apply missing mask and normalization
        feature_weights = feature_weights * missing_bool
        feature_weights = feature_weights / (feature_weights.sum(dim=-1, keepdim=True) + 1e-8)
        
        # Convert to numpy
        weights_np = feature_weights.squeeze(0).cpu().numpy()  # (seq_len, n_features)
        
        # Get feature names
        feature_names = model.config.LSTM2_FEATURES
        
        # Set large font sizes
        plt.rcParams.update({'font.size': 14})
        figs = []
        
        # Figure 1: Heatmap of weight changes
        fig1, ax1 = plt.subplots(figsize=(16, 10))
        sns.heatmap(weights_np.T, 
                   xticklabels=range(1, weights_np.shape[0] + 1),
                   yticklabels=feature_names,
                   cmap='YlOrRd', 
                   cbar_kws={'label': 'Feature Weights', 'shrink': 0.8},
                   ax=ax1)
        ax1.set_title('LSTM2 Feature Weights Heatmap Over Time', fontsize=18, fontweight='bold', pad=20)
        ax1.set_xlabel('Time Step (Days)', fontsize=14)
        ax1.set_ylabel('Features', fontsize=14)
        ax1.tick_params(axis='both', which='major', labelsize=12)
        figs.append(fig1)
        
        if save_dir:
            fig1.savefig(f"{save_dir}/lstm2_weights_heatmap.png", dpi=300, bbox_inches='tight')
        
        # Figure 2: Average weights bar chart
        fig2, ax2 = plt.subplots(figsize=(14, 10))
        avg_weights = np.mean(weights_np, axis=0)
        bars = ax2.bar(range(len(feature_names)), avg_weights, color='steelblue', alpha=0.7, linewidth=1.5)
        ax2.set_title('LSTM2 Average Feature Weights Distribution', fontsize=18, fontweight='bold', pad=20)
        ax2.set_xlabel('Features', fontsize=14)
        ax2.set_ylabel('Average Weights', fontsize=14)
        ax2.set_xticks(range(len(feature_names)))
        ax2.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=12)
        ax2.tick_params(axis='y', which='major', labelsize=12)
        ax2.grid(axis='y', alpha=0.3)
        
        # Display values on bars
        for bar, weight in zip(bars, avg_weights):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{weight:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        figs.append(fig2)
        if save_dir:
            fig2.savefig(f"{save_dir}/lstm2_average_weights.png", dpi=300, bbox_inches='tight')
        
        # Figure 3: Weight time series
        fig3, ax3 = plt.subplots(figsize=(16, 10))
        time_steps = range(weights_np.shape[0])
        colors = plt.cm.Set3(np.linspace(0, 1, len(feature_names)))
        for i, (feature_name, color) in enumerate(zip(feature_names, colors)):
            ax3.plot(time_steps, weights_np[:, i], label=feature_name, alpha=0.8, linewidth=3, color=color)
        
        ax3.set_title('LSTM2 Feature Weights Time Series', fontsize=18, fontweight='bold', pad=20)
        ax3.set_xlabel('Time Step (Days)', fontsize=14)
        ax3.set_ylabel('Feature Weights', fontsize=14)
        ax3.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=12, framealpha=0.8)
        ax3.grid(True, alpha=0.3)
        ax3.tick_params(axis='both', which='major', labelsize=12)
        figs.append(fig3)
        
        if save_dir:
            fig3.savefig(f"{save_dir}/lstm2_weights_timeseries.png", dpi=300, bbox_inches='tight')
        
        # Figure 4: Weight statistics
        fig4, ax4 = plt.subplots(figsize=(12, 10))
        ax4.axis('off')
        stats_text = f"""Weight Statistics Summary:

Mean Weight: {np.mean(avg_weights):.4f}
Weight Std Dev: {np.std(avg_weights):.4f}
Max Weight: {np.max(avg_weights):.4f} ({feature_names[np.argmax(avg_weights)]})
Min Weight: {np.min(avg_weights):.4f} ({feature_names[np.argmin(avg_weights)]})

Weight Distribution:
> 0.1: {np.sum(avg_weights > 0.1)} features
0.05-0.1: {np.sum((avg_weights > 0.05) & (avg_weights <= 0.1))} features
< 0.05: {np.sum(avg_weights <= 0.05)} features

Temporal Stability:
Mean Weight Variance: {np.mean(np.var(weights_np, axis=0)):.4f}
Weight Range: {np.max(weights_np) - np.min(weights_np):.4f}"""
        
        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, 
                verticalalignment='top', fontsize=14, fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=1.0', facecolor='lightblue', alpha=0.8, edgecolor='navy', linewidth=2))
        
        figs.append(fig4)
        if save_dir:
            fig4.savefig(f"{save_dir}/lstm2_weight_statistics.png", dpi=300, bbox_inches='tight')
        
        return figs


def visualize_feature_weights(model, sample_input, feature_names=None, save_dir=None):
    """
    Feature Weight Visualization - Display attention weights for each feature
    
    Parameters:
    -----------
    model : SCA2LSTM model
        Trained SCA2LSTM model
    sample_input : dict
        Dictionary containing model inputs
    feature_names : list, optional
        List of feature names, if None use model config
    save_dir : str, optional
        Directory to save images, if None only display
    
    Returns:
    -----------
    figs : list of matplotlib.figure.Figure
        List of generated figure objects
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    model.eval()
    with torch.no_grad():
        # å‡†å¤‡è¾“å…¥æ•°æ®
        device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')
        seq_features = sample_input["seq_features"].unsqueeze(0).to(device)
        lstm1_input = sample_input["lstm1_input"].unsqueeze(0).to(device)
        missing_bool = sample_input["missing_bool"].unsqueeze(0).to(device)
        basin_ids = sample_input["basin_id"].unsqueeze(0).to(device)
        
        # è·å–ç‰¹å¾æƒé‡ - ä½¿ç”¨æ¨¡å‹å†…éƒ¨é€»è¾‘æå–æƒé‡è€Œä¸æ˜¯é¢„æµ‹å€¼
        batch_size = seq_features.shape[0]
        seq_len = seq_features.shape[1]
        
        # åˆå§‹åŒ–éšçŠ¶æ€
        h1 = torch.zeros(batch_size, model.lstm_hidden_dim, device=device)
        c1 = torch.zeros(batch_size, model.lstm_hidden_dim, device=device)
        h2 = torch.zeros(batch_size, model.lstm_hidden_dim, device=device)
        c2 = torch.zeros(batch_size, model.lstm_hidden_dim, device=device)
        
        # æµåŸŸåµŒå…¥
        basin_id_to_idx = {bid: idx for idx, bid in enumerate(set(model.basin_ids))}
        basin_indices = torch.tensor([basin_id_to_idx[bid.item()] for bid in basin_ids], dtype=torch.long).to(device)
        basin_embed = model.basin_embedding(basin_indices)
        
        # æ”¶é›†æ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾æƒé‡
        feature_weights_list = []
        
        for t in range(seq_len):
            # LSTM1ï¼šç”Ÿæˆç‰¹å¾æƒé‡
            lstm1_input_t = lstm1_input[:, t, :]
            lstm1_input_with_feedback = torch.cat([lstm1_input_t, h1, h2], dim=-1)
            h1, c1 = model.lstm1_cell(lstm1_input_with_feedback, (h1, c1))
            
            # ç”Ÿæˆç‰¹å¾æƒé‡
            feature_weights = model.weight_head(h1)
            feature_weights = feature_weights * missing_bool[:, t, :]
            # é˜²æ­¢é™¤é›¶ï¼šå¦‚æœæ‰€æœ‰ç‰¹å¾éƒ½ç¼ºå¤±ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            weight_sums = feature_weights.sum(dim=-1, keepdim=True)
            uniform_weights = torch.ones_like(feature_weights) / feature_weights.shape[-1]
            feature_weights = torch.where(
                weight_sums < 1e-8,
                uniform_weights * missing_bool[:, t, :],
                feature_weights / (weight_sums + 1e-8)
            )
            feature_weights_list.append(feature_weights)
            
            # LSTM2ï¼šæ›´æ–°h2çŠ¶æ€ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„åé¦ˆï¼‰
            seq_features_t = seq_features[:, t, :]
            weighted_features = seq_features_t * feature_weights
            basin_embed_t = basin_embed
            lstm2_input_with_feedback = torch.cat([weighted_features, basin_embed_t, h1], dim=-1)
            h2, c2 = model.lstm2_cell(lstm2_input_with_feedback, (h2, c2))
        
        # å †å æ‰€æœ‰æ—¶é—´æ­¥çš„æƒé‡
        feature_weights = torch.stack(feature_weights_list, dim=1)  # (batch, seq_len, n_features)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–inf
        if torch.isnan(feature_weights).any() or torch.isinf(feature_weights).any():
            print("âš ï¸  è­¦å‘Šï¼šç‰¹å¾æƒé‡åŒ…å«NaNæˆ–infï¼Œä½¿ç”¨å¤‡ç”¨æƒé‡")
            # ä½¿ç”¨å‡åŒ€æƒé‡ä½œä¸ºå¤‡ç”¨
            feature_weights = torch.ones_like(feature_weights) / feature_weights.shape[-1]
        
        # Convert to numpy
        weights_np = feature_weights.squeeze(0).cpu().numpy()  # (seq_len, n_features)
        
        # æ£€æŸ¥numpyæ•°ç»„æ˜¯å¦æœ‰æ•ˆ
        if np.isnan(weights_np).any() or np.isinf(weights_np).any():
            print("âš ï¸  è­¦å‘Šï¼šè½¬æ¢åçš„æƒé‡åŒ…å«NaNæˆ–infï¼Œä½¿ç”¨å‡åŒ€æƒé‡")
            weights_np = np.ones_like(weights_np) / weights_np.shape[-1]
        
        # Get feature names
        if feature_names is None:
            feature_names = [f'Feature_{i}' for i in range(weights_np.shape[1])]
        
        # Set large font sizes
        plt.rcParams.update({'font.size': 14})
        figs = []
        
        # Figure 1: Feature weights heatmap
        fig1, ax1 = plt.subplots(figsize=(16, 10))
        sns.heatmap(weights_np.T, 
                   xticklabels=range(1, weights_np.shape[0] + 1),
                   yticklabels=feature_names,
                   cmap='RdYlBu_r', 
                   cbar_kws={'label': 'Feature Weight', 'shrink': 0.8},
                   ax=ax1,
                   annot=False,
                   fmt='.3f',
                   linewidths=0.5)
        
        ax1.set_title('Feature Attention Weights Heatmap', fontsize=20, fontweight='bold', pad=20)
        ax1.set_xlabel('Time Step (Days)', fontsize=16)
        ax1.set_ylabel('Features', fontsize=16)
        ax1.tick_params(axis='both', which='major', labelsize=14)
        figs.append(fig1)
        
        if save_dir:
            fig1.savefig(f"{save_dir}/feature_weights_heatmap.png", dpi=300, bbox_inches='tight')
        
        # Figure 2: Average feature weights
        fig2, ax2 = plt.subplots(figsize=(14, 10))
        avg_weights = np.mean(weights_np, axis=0)
        
        # Create color map based on weight values
        colors = plt.cm.RdYlBu_r(avg_weights / np.max(avg_weights))
        bars = ax2.bar(range(len(feature_names)), avg_weights, color=colors, alpha=0.8, linewidth=1.5)
        
        ax2.set_title('Average Feature Attention Weights', fontsize=20, fontweight='bold', pad=20)
        ax2.set_xlabel('Features', fontsize=16)
        ax2.set_ylabel('Average Weight', fontsize=16)
        ax2.set_xticks(range(len(feature_names)))
        ax2.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=14)
        ax2.tick_params(axis='y', which='major', labelsize=14)
        ax2.grid(axis='y', alpha=0.3)
        
        # Display values on bars
        for bar, weight in zip(bars, avg_weights):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{weight:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        figs.append(fig2)
        if save_dir:
            fig2.savefig(f"{save_dir}/feature_weights_average.png", dpi=300, bbox_inches='tight')
        
        # Figure 3: Feature weight time series
        fig3, ax3 = plt.subplots(figsize=(16, 10))
        time_steps = range(weights_np.shape[0])
        
        # Use distinct colors for different features
        colors = plt.cm.Set3(np.linspace(0, 1, len(feature_names)))
        for i, (feature_name, color) in enumerate(zip(feature_names, colors)):
            ax3.plot(time_steps, weights_np[:, i], label=feature_name, 
                    alpha=0.8, linewidth=3, color=color, marker='', markersize=0, antialiased=True)
        
        ax3.set_title('Feature Attention Weights Over Time', fontsize=20, fontweight='bold', pad=20)
        ax3.set_xlabel('Time Step (Days)', fontsize=16)
        ax3.set_ylabel('Feature Weight', fontsize=16)
        ax3.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=12, framealpha=0.8)
        ax3.grid(True, alpha=0.3)
        ax3.tick_params(axis='both', which='major', labelsize=14)
        
        figs.append(fig3)
        if save_dir:
            fig3.savefig(f"{save_dir}/feature_weights_timeseries.png", dpi=300, bbox_inches='tight')
        
        # Figure 4: Feature weight distribution
        fig4, ax4 = plt.subplots(figsize=(12, 10))
        
        # Create violin plot for weight distribution
        data_for_violin = [weights_np[:, i] for i in range(len(feature_names))]
        parts = ax4.violinplot(data_for_violin, positions=range(len(feature_names)), 
                              showmeans=True, showmedians=True)
        
        # Customize violin plot colors
        for i, pc in enumerate(parts['bodies']):
            pc.set_facecolor(plt.cm.RdYlBu_r(i / len(feature_names)))
            pc.set_alpha(0.7)
        
        ax4.set_title('Feature Weight Distribution', fontsize=20, fontweight='bold', pad=20)
        ax4.set_xlabel('Features', fontsize=16)
        ax4.set_ylabel('Weight Value', fontsize=16)
        ax4.set_xticks(range(len(feature_names)))
        ax4.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=14)
        ax4.tick_params(axis='y', which='major', labelsize=14)
        ax4.grid(axis='y', alpha=0.3)
        
        figs.append(fig4)
        if save_dir:
            fig4.savefig(f"{save_dir}/feature_weights_distribution.png", dpi=300, bbox_inches='tight')
        
        return figs


def plot_water_level_comparison(true_discharge, pred_discharge, dates=None, 
                               flood_mask=None, save_dir=None, title="Water Level Comparison"):
    """
    Water Level Comparison - Visualize the difference between observed and predicted values
    
    Parameters:
    -----------
    true_discharge : array-like
        Observed discharge data
    pred_discharge : array-like
        Predicted discharge data
    dates : array-like, optional
        Date sequence for x-axis
    flood_mask : array-like, optional
        Flood event mask for highlighting flood periods
    save_dir : str, optional
        Directory to save images
    title : str
        Figure title
    
    Returns:
    -----------
    figs : list of matplotlib.figure.Figure
        List of generated figure objects
    """
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    from datetime import datetime, timedelta
    
    # Data preparation
    true_discharge = np.array(true_discharge, dtype=float)
    pred_discharge = np.array(pred_discharge, dtype=float)
    
    if len(true_discharge) != len(pred_discharge):
        raise ValueError("Observed and predicted values must have the same length")
    
    # Generate default dates (if not provided)
    if dates is None:
        start_date = datetime(2020, 1, 1)
        dates = [start_date + timedelta(days=i) for i in range(len(true_discharge))]
    else:
        dates = pd.to_datetime(dates)
    
    # Set large font sizes
    plt.rcParams.update({'font.size': 14})
    figs = []
    
    # Figure 1: Main comparison plot
    fig1, ax1 = plt.subplots(figsize=(16, 10))
    # Smooth curve plotting with enhanced visual appeal
    ax1.plot(dates, true_discharge, color='#1f77b4', linewidth=3.5, label='Observed', 
             alpha=0.9, linestyle='-', marker='', markersize=0, antialiased=True)
    ax1.plot(dates, pred_discharge, color='#d62728', linewidth=3.5, label='Predicted', 
             alpha=0.9, linestyle='-', marker='', markersize=0, antialiased=True)
    
    # Highlight flood periods
    if flood_mask is not None:
        flood_dates = np.array(dates)[flood_mask]
        flood_true = true_discharge[flood_mask]
        if len(flood_dates) > 0:
            ax1.scatter(flood_dates, flood_true, c='orange', s=100, 
                       label='Flood Period Observations', alpha=0.8, zorder=5, edgecolors='black', linewidth=1)
    
    ax1.set_title(title, fontsize=20, fontweight='bold', pad=20)
    ax1.set_ylabel('Discharge (mÂ³/s)', fontsize=16)
    ax1.legend(loc='upper right', fontsize=14, framealpha=0.8)
    ax1.grid(True, alpha=0.3)
    ax1.tick_params(axis='both', which='major', labelsize=14)
    
    # Set x-axis date format
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=2))
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    figs.append(fig1)
    if save_dir:
        fig1.savefig(f"{save_dir}/water_level_comparison_main.png", dpi=300, bbox_inches='tight')
    
    # Figure 2: Residual analysis
    fig2, ax2 = plt.subplots(figsize=(16, 10))
    residuals = true_discharge - pred_discharge
    # Smooth residual curve with enhanced styling
    ax2.plot(dates, residuals, color='#2ca02c', linewidth=3, alpha=0.85, 
             linestyle='-', antialiased=True)
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.9, linewidth=2.5)
    ax2.axhline(y=np.mean(residuals), color='#d62728', linestyle='--', linewidth=3,
                label=f'Mean Residual: {np.mean(residuals):.2f}')
    ax2.fill_between(dates, residuals, 0, alpha=0.3, color='green')
    
    ax2.set_title('Residual Analysis (Observed - Predicted)', fontsize=20, fontweight='bold', pad=20)
    ax2.set_ylabel('Residuals (mÂ³/s)', fontsize=16)
    ax2.legend(fontsize=14, framealpha=0.8)
    ax2.grid(True, alpha=0.3)
    ax2.tick_params(axis='both', which='major', labelsize=14)
    
    # Set x-axis date format
    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=2))
    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
    
    figs.append(fig2)
    if save_dir:
        fig2.savefig(f"{save_dir}/water_level_residuals.png", dpi=300, bbox_inches='tight')
    
    # Figure 3: Scatter plot and correlation analysis
    fig3, ax3 = plt.subplots(figsize=(14, 12))
    ax3.scatter(true_discharge, pred_discharge, alpha=0.7, s=60, color='purple', edgecolors='black', linewidth=0.5)
    
    # Add 1:1 line
    min_val = min(np.min(true_discharge), np.min(pred_discharge))
    max_val = max(np.max(true_discharge), np.max(pred_discharge))
    ax3.plot([min_val, max_val], [min_val, max_val], color='black', linewidth=3.5, 
             linestyle='--', label='1:1 Line', alpha=0.9, antialiased=True)
    
    # Calculate statistics
    from scipy import stats
    correlation, p_value = stats.pearsonr(true_discharge, pred_discharge)
    rmse = np.sqrt(np.mean(residuals**2))
    mae = np.mean(np.abs(residuals))
    nse = 1 - np.sum(residuals**2) / np.sum((true_discharge - np.mean(true_discharge))**2)
    
    # Add statistical information text
    stats_text = f'Correlation Coefficient: {correlation:.3f}\nRMSE: {rmse:.2f} mÂ³/s\nMAE: {mae:.2f} mÂ³/s\nNSE: {nse:.3f}'
    ax3.text(0.05, 0.95, stats_text, transform=ax3.transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.8', facecolor='wheat', alpha=0.9, edgecolor='black', linewidth=2),
             fontsize=16, fontweight='bold')
    
    ax3.set_xlabel('Observed Discharge (mÂ³/s)', fontsize=16)
    ax3.set_ylabel('Predicted Discharge (mÂ³/s)', fontsize=16)
    ax3.set_title('Observed vs Predicted Scatter Plot', fontsize=20, fontweight='bold', pad=20)
    ax3.legend(fontsize=14, framealpha=0.8)
    ax3.grid(True, alpha=0.3)
    ax3.tick_params(axis='both', which='major', labelsize=14)
    
    figs.append(fig3)
    if save_dir:
        fig3.savefig(f"{save_dir}/water_level_scatter.png", dpi=300, bbox_inches='tight')
    
    print(f"Water level comparison plots saved to: {save_dir}")
    return figs


def create_evaluation_report(true_discharge, pred_discharge, dates=None, basin_id=None):
    """
    ç”Ÿæˆå®Œæ•´çš„æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
    
    å‚æ•°:
    -----------
    true_discharge : array-like
        è§‚æµ‹æµé‡æ•°æ®
    pred_discharge : array-like
        é¢„æµ‹æµé‡æ•°æ®
    dates : array-like, optional
        æ—¥æœŸåºåˆ—
    basin_id : str/int, optional
        æµåŸŸID
    
    è¿”å›:
    -----------
    report : dict
        åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–ç»“æœçš„å­—å…¸
    """
    from scipy import stats
    
    # åŸºç¡€ç»Ÿè®¡
    true_discharge = np.array(true_discharge, dtype=float)
    pred_discharge = np.array(pred_discharge, dtype=float)
    residuals = true_discharge - pred_discharge
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰å˜åŒ–ï¼ˆæ ‡å‡†å·®æ˜¯å¦ä¸º0ï¼‰
    pred_std = np.std(pred_discharge)
    if pred_std == 0:
        # å¦‚æœé¢„æµ‹å€¼æ’å®šï¼Œç›¸å…³ç³»æ•°ä¸º0ï¼Œpå€¼ä¸º1
        correlation = 0.0
        p_value = 1.0
    else:
        correlation, p_value = stats.pearsonr(true_discharge, pred_discharge)
    
    rmse = np.sqrt(np.mean(residuals**2))
    mae = np.mean(np.abs(residuals))
    bias = np.mean(residuals)
    
    # Nash-Sutcliffeæ•ˆç‡ç³»æ•°
    nse = 1 - np.sum(residuals**2) / np.sum((true_discharge - np.mean(true_discharge))**2)
    
    # ç›¸å¯¹è¯¯å·®
    relative_rmse = rmse / np.mean(true_discharge) * 100
    relative_mae = mae / np.mean(true_discharge) * 100
    
    # æ´ªæ°´æœŸè¯„ä¼°ï¼ˆå¦‚æœæœ‰æ´ªæ°´è¯†åˆ«ç»“æœï¼‰
    flood_evaluation = None
    try:
        flood_mask = identify_flood_events(true_discharge)
        flood_evaluation = evaluate_flood_prediction(true_discharge, pred_discharge)
        
        # è½¬æ¢æ´ªæ°´è¯„ä¼°ä¸­çš„numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        if flood_evaluation:
            def convert_flood_types(obj):
                """é€’å½’è½¬æ¢æ´ªæ°´è¯„ä¼°ä¸­çš„numpyç±»å‹"""
                if isinstance(obj, dict):
                    return {k: convert_flood_types(v) for k, v in obj.items()}
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                else:
                    return obj
            
            flood_evaluation = convert_flood_types(flood_evaluation)
            
    except:
        pass
    
    report = {
        'basin_id': basin_id,
        'sample_size': len(true_discharge),
        'correlation': {'value': float(correlation), 'p_value': float(p_value)},
        'rmse': float(rmse),
        'mae': float(mae),
        'bias': float(bias),
        'nse': float(nse),
        'relative_rmse': float(relative_rmse),
        'relative_mae': float(relative_mae),
        'true_stats': {
            'mean': float(np.mean(true_discharge)),
            'std': float(np.std(true_discharge)),
            'min': float(np.min(true_discharge)),
            'max': float(np.max(true_discharge))
        },
        'pred_stats': {
            'mean': float(np.mean(pred_discharge)),
            'std': float(np.std(pred_discharge)),
            'min': float(np.min(pred_discharge)),
            'max': float(np.max(pred_discharge))
        },
        'flood_evaluation': flood_evaluation
    }
    
    return report


def test_visualization_functions():
    """
    æµ‹è¯•å¯è§†åŒ–å‡½æ•°
    """
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¯è§†åŒ–å‡½æ•°...")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_days = 365
    
    # æ¨¡æ‹Ÿæµé‡æ•°æ®ï¼ˆå«å­£èŠ‚æ€§å˜åŒ–å’Œæ´ªæ°´äº‹ä»¶ï¼‰
    t = np.linspace(0, 4*np.pi, n_days)
    base_flow = 50 + 30 * np.sin(t)  # å­£èŠ‚æ€§åŸºæµ
    noise = np.random.normal(0, 5, n_days)
    
    # æ·»åŠ æ´ªæ°´äº‹ä»¶
    flood_events = [
        (50, 70, 150),   # å¼€å§‹, ç»“æŸ, å³°å€¼
        (150, 180, 200),
        (250, 280, 180)
    ]
    
    true_discharge = base_flow + noise
    for start, end, peak in flood_events:
        flood_shape = np.exp(-((np.arange(end-start) - (end-start)//2)**2) / (2*5**2))
        true_discharge[start:end] += peak * flood_shape
    
    # ç”Ÿæˆé¢„æµ‹æ•°æ®ï¼ˆæ·»åŠ ä¸€äº›è¯¯å·®ï¼‰
    pred_discharge = true_discharge + np.random.normal(0, 8, n_days)
    
    # ç”Ÿæˆæ—¥æœŸåºåˆ—
    dates = pd.date_range('2020-01-01', periods=n_days, freq='D')
    
    # æµ‹è¯•æ°´ä½å¯¹æ¯”å›¾
    print("ğŸ“Š æµ‹è¯•æ°´ä½å¯¹æ¯”å›¾...")
    try:
        figs = plot_water_level_comparison(
            true_discharge=true_discharge,
            pred_discharge=pred_discharge,
            dates=dates,
            save_dir='.'
        )
        for fig in figs:
            plt.close(fig)
        print("âœ… æ°´ä½å¯¹æ¯”å›¾æµ‹è¯•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ°´ä½å¯¹æ¯”å›¾æµ‹è¯•å¤±è´¥: {str(e)}")
    
    # æµ‹è¯•è¯„ä¼°æŠ¥å‘Š
    print("ğŸ“‹ æµ‹è¯•è¯„ä¼°æŠ¥å‘Š...")
    try:
        report = create_evaluation_report(
            true_discharge=true_discharge,
            pred_discharge=pred_discharge,
            dates=dates,
            basin_id="TEST_BASIN"
        )
        
        # ä¿å­˜æŠ¥å‘Šï¼ˆå¤„ç†numpyæ•°æ®ç±»å‹ï¼‰
        def convert_numpy_types(obj):
            """è½¬æ¢numpyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        with open('test_evaluation_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=convert_numpy_types)
        
        print("âœ… è¯„ä¼°æŠ¥å‘Šæµ‹è¯•æˆåŠŸ")
        print(f"   - æ ·æœ¬æ•°é‡: {report['sample_size']}")
        print(f"   - ç›¸å…³ç³»æ•°: {report['correlation']['value']:.3f}")
        print(f"   - RMSE: {report['rmse']:.2f}")
        print(f"   - NSE: {report['nse']:.3f}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°æŠ¥å‘Šæµ‹è¯•å¤±è´¥: {str(e)}")
    
    print("ğŸ‰ å¯è§†åŒ–å‡½æ•°æµ‹è¯•å®Œæˆï¼")


def plot_training_curves(train_losses, val_losses, save_dir=None, show_plot=False):
    """
    ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿
    
    å‚æ•°:
    -----------
    train_losses : list
        è®­ç»ƒæŸå¤±å†å²
    val_losses : list
        éªŒè¯æŸå¤±å†å²
    save_dir : str, optional
        ä¿å­˜å›¾åƒçš„ç›®å½•
    show_plot : bool, default=False
        æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    try:
        plt.figure(figsize=(12, 5))
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        epochs = range(1, len(train_losses) + 1)
        plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
        plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
        
        # æ ‡è®°æœ€ä½³éªŒè¯æŸå¤±
        if val_losses:
            best_epoch = np.argmin(val_losses)
            best_loss = val_losses[best_epoch]
            plt.scatter(best_epoch + 1, best_loss, c='red', s=100, marker='*', 
                       label=f'Best Validation Loss: {best_loss:.4f}')
        
        plt.title('Training and Validation Loss Curves', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, 'training_curves.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜è‡³: {save_path}")
        
        if show_plot:
            plt.show()
        else:
            plt.close()
            
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶æŸå¤±æ›²çº¿å¤±è´¥: {str(e)}")


def plot_prediction_comparison(pred_values, target_values, basin_ids=None, epoch=None, save_dir=None, sample_size=50):
    """
    ç»˜åˆ¶é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¯¹æ¯”å›¾
    
    å‚æ•°:
    -----------
    pred_values : array-like
        é¢„æµ‹å€¼åˆ—è¡¨
    target_values : array-like
        çœŸå®å€¼åˆ—è¡¨
    basin_ids : list, optional
        æµåŸŸIDåˆ—è¡¨
    epoch : int, optional
        å½“å‰è½®æ¬¡
    save_dir : str, optional
        ä¿å­˜å›¾åƒçš„ç›®å½•
    sample_size : int, default=50
        æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
    """
    try:
        if len(pred_values) == 0 or len(target_values) == 0:
            print("âš ï¸  æ²¡æœ‰é¢„æµ‹æ•°æ®å¯ä¾›å¯è§†åŒ–")
            return
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å–å‰sample_sizeä¸ªæ ·æœ¬
        pred_array = np.array(pred_values)[:sample_size]
        target_array = np.array(target_values)[:sample_size]
        
        plt.figure(figsize=(12, 5))
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        sample_indices = range(len(pred_array))
        plt.plot(sample_indices, target_array, 'b-', label='True Values', linewidth=2, marker='o', markersize=4)
        plt.plot(sample_indices, pred_array, 'r--', label='Predictions', linewidth=2, marker='s', markersize=4)
        
        # æ·»åŠ æ ‡é¢˜ä¿¡æ¯
        title = 'Prediction vs True Values Comparison'
        if epoch is not None:
            title += f' (Epoch {epoch+1})'
        if basin_ids and len(set(basin_ids)) <= 3:  # åªæ˜¾ç¤ºå°‘é‡æµåŸŸID
            unique_basins = list(set(basin_ids))
            title += f'\nBasins: {", ".join(map(str, unique_basins))}'
        
        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('Sample Index', fontsize=12)
        plt.ylabel('Discharge Value', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºè¯¯å·®æŒ‡æ ‡
        if len(pred_array) == len(target_array):
            mse = np.mean((pred_array - target_array) ** 2)
            mae = np.mean(np.abs(pred_array - target_array))
            plt.text(0.02, 0.98, f'MSE: {mse:.4f}\nMAE: {mae:.4f}', 
                    transform=plt.gca().transAxes, fontsize=10,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            epoch_str = f'_epoch_{epoch+1}' if epoch is not None else ''
            save_path = os.path.join(save_dir, f'prediction_comparison{epoch_str}.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.close()
        
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶é¢„æµ‹å¯¹æ¯”å›¾å¤±è´¥: {str(e)}")


def plot_loss_distribution(losses, epoch=None, save_dir=None, bins=30):
    """
    ç»˜åˆ¶æŸå¤±åˆ†å¸ƒç›´æ–¹å›¾
    
    å‚æ•°:
    -----------
    losses : array-like
        æŸå¤±å€¼åˆ—è¡¨
    epoch : int, optional
        å½“å‰è½®æ¬¡
    save_dir : str, optional
        ä¿å­˜å›¾åƒçš„ç›®å½•
    bins : int, default=30
        ç›´æ–¹å›¾çš„åˆ†ç®±æ•°é‡
    """
    try:
        if len(losses) == 0:
            print("âš ï¸  æ²¡æœ‰æŸå¤±æ•°æ®å¯ä¾›å¯è§†åŒ–")
            return
        
        plt.figure(figsize=(10, 6))
        
        # ç»˜åˆ¶ç›´æ–¹å›¾
        plt.hist(losses, bins=bins, alpha=0.7, color='skyblue', edgecolor='black')
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_loss = np.mean(losses)
        std_loss = np.std(losses)
        
        plt.axvline(mean_loss, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_loss:.4f}')
        plt.axvline(mean_loss + std_loss, color='orange', linestyle='--', linewidth=2, label=f'Mean+Std: {mean_loss + std_loss:.4f}')
        plt.axvline(mean_loss - std_loss, color='orange', linestyle='--', linewidth=2, label=f'Mean-Std: {mean_loss - std_loss:.4f}')
        
        title = 'Loss Distribution'
        if epoch is not None:
            title += f' (Epoch {epoch+1})'
        
        plt.title(title, fontsize=14, fontweight='bold')
        plt.xlabel('Loss Value', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
        plt.text(0.02, 0.98, f'Samples: {len(losses)}\nMean: {mean_loss:.4f}\nStd: {std_loss:.4f}', 
                transform=plt.gca().transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            epoch_str = f'_epoch_{epoch+1}' if epoch is not None else ''
            save_path = os.path.join(save_dir, f'loss_distribution{epoch_str}.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æŸå¤±åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")
        
        plt.close()
        
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶æŸå¤±åˆ†å¸ƒå›¾å¤±è´¥: {str(e)}")


def plot_feature_weights_heatmap(feature_weights_history, feature_names=None, save_dir=None, show_plot=False):
    """
    ç»˜åˆ¶ç‰¹å¾æƒé‡çƒ­åŠ›å›¾ï¼Œå±•ç¤ºä¸åŒç‰¹å¾æƒé‡éšè®­ç»ƒè½®æ•°çš„å˜åŒ–
    
    å‚æ•°:
    -----------
    feature_weights_history : dict
        ç‰¹å¾æƒé‡å†å²æ•°æ®ï¼Œæ ¼å¼ï¼š{epoch: {basin_id: weights_array}}
        å…¶ä¸­weights_arrayå½¢çŠ¶ä¸º (seq_len, n_features) æˆ– (n_features,)
    feature_names : list, optional
        ç‰¹å¾åç§°åˆ—è¡¨ï¼Œç”¨äºYè½´æ ‡ç­¾
    save_dir : str, optional
        ä¿å­˜å›¾åƒçš„ç›®å½•
    show_plot : bool, default=False
        æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
    """
    try:
        if not feature_weights_history:
            print("âš ï¸  æ²¡æœ‰ç‰¹å¾æƒé‡æ•°æ®å¯ä¾›å¯è§†åŒ–")
            return
        
        # è·å–æ‰€æœ‰epochå’ŒæµåŸŸID
        all_epochs = sorted(feature_weights_history.keys())
        all_basins = set()
        for epoch_data in feature_weights_history.values():
            all_basins.update(epoch_data.keys())
        all_basins = sorted(list(all_basins))
        
        if not all_basins:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æµåŸŸæ•°æ®")
            return
        
        # ä¸ºæ¯ä¸ªæµåŸŸåˆ›å»ºå•ç‹¬çš„çƒ­åŠ›å›¾
        for basin_id in all_basins:
            # æ”¶é›†è¯¥æµåŸŸçš„æƒé‡æ•°æ®
            basin_weights = []
            valid_epochs = []
            
            for epoch in all_epochs:
                if basin_id in feature_weights_history[epoch]:
                    weights = feature_weights_history[epoch][basin_id]
                    if weights is not None and len(weights) > 0:
                        # å¤„ç†ä¸åŒå½¢çŠ¶çš„æƒé‡æ•°æ®
                        if weights.ndim == 2:  # (seq_len, n_features)
                            # å¯¹æ—¶é—´ç»´åº¦å–å¹³å‡ï¼Œå¾—åˆ° (n_features,)
                            avg_weights = np.mean(weights, axis=0)
                            basin_weights.append(avg_weights)
                        elif weights.ndim == 1:  # (n_features,)
                            basin_weights.append(weights)
                        else:
                            print(f"âš ï¸  æƒé‡æ•°æ®ç»´åº¦ä¸æ”¯æŒ: {weights.ndim}")
                            continue
                        valid_epochs.append(epoch)
            
            if not basin_weights:
                print(f"âš ï¸  æµåŸŸ {basin_id} æ²¡æœ‰æœ‰æ•ˆçš„æƒé‡æ•°æ®")
                continue
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            weights_matrix = np.array(basin_weights)  # shape: (n_epochs, n_features)
            
            # åˆ›å»ºå¤§å­—ä½“ã€å¤§å›¾åƒçš„çƒ­åŠ›å›¾
            plt.figure(figsize=(16, 10))
            
            # æ ¹æ®epochæ•°é‡è°ƒæ•´Xè½´å¯†åº¦
            n_epochs = len(valid_epochs)
            if n_epochs <= 20:
                x_tick_interval = 1  # æ¯è½®éƒ½æ˜¾ç¤º
            elif n_epochs <= 50:
                x_tick_interval = 5  # æ¯5è½®æ˜¾ç¤ºä¸€æ¬¡
            else:
                x_tick_interval = 10  # æ¯10è½®æ˜¾ç¤ºä¸€æ¬¡
            
            # åˆ›å»ºçƒ­åŠ›å›¾
            im = plt.imshow(weights_matrix.T, 
                           aspect='auto', 
                           cmap='YlOrRd',  # é»„-æ©™-çº¢æ¸å˜
                           interpolation='nearest')
            
            # è®¾ç½®åæ ‡è½´
            plt.title(f'Feature Weights Heatmap - Basin {basin_id}', 
                     fontsize=20, fontweight='bold', pad=20)
            plt.xlabel('Training Epoch', fontsize=16, fontweight='bold')
            plt.ylabel('Features', fontsize=16, fontweight='bold')
            
            # è®¾ç½®Yè½´æ ‡ç­¾
            if feature_names and len(feature_names) == weights_matrix.shape[1]:
                plt.yticks(range(len(feature_names)), feature_names, fontsize=12)
            else:
                plt.yticks(range(weights_matrix.shape[1]), 
                        [f'Feature {i}' for i in range(weights_matrix.shape[1])], 
                        fontsize=12)
            
            # è®¾ç½®Xè½´æ ‡ç­¾
            x_ticks = range(0, n_epochs, x_tick_interval)
            x_labels = [valid_epochs[i] + 1 for i in x_ticks]  # +1è½¬æ¢ä¸ºäººç±»å¯è¯»çš„epochç¼–å·
            plt.xticks(x_ticks, x_labels, fontsize=12)
            
            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(im, shrink=0.8)
            cbar.set_label('Weight Value', fontsize=14, fontweight='bold')
            cbar.ax.tick_params(labelsize=12)
            
            # åœ¨çƒ­åŠ›å›¾ä¸Šæ˜¾ç¤ºå…·ä½“æ•°å€¼
            for i in range(0, n_epochs, max(1, n_epochs // 10)):  # é™åˆ¶æ˜¾ç¤ºçš„æ•°å€¼æ•°é‡
                for j in range(weights_matrix.shape[1]):
                    text = plt.text(i, j, f'{weights_matrix[i, j]:.3f}',
                                   ha="center", va="center", 
                                   color="white" if weights_matrix[i, j] < 0.3 else "black",
                                   fontsize=10, fontweight='bold')
            
            # æ·»åŠ ç½‘æ ¼çº¿
            plt.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾åƒ
            if save_dir:
                os.makedirs(save_dir, exist_ok=True)
                save_path = os.path.join(save_dir, f'feature_weights_basin_{basin_id}.png')
                plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
                print(f"ğŸ”¥ ç‰¹å¾æƒé‡çƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {save_path}")
            
            if show_plot:
                plt.show()
            else:
                plt.close()
        
        print(f"âœ… å·²å®Œæˆ {len(all_basins)} ä¸ªæµåŸŸçš„ç‰¹å¾æƒé‡çƒ­åŠ›å›¾ç»˜åˆ¶")
        
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶ç‰¹å¾æƒé‡çƒ­åŠ›å›¾å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_flood_identification()
    print("\n" + "="*50)
    test_visualization_functions()