import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import json
from datetime import datetime
import importlib.util
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import multiprocessing as mp
from hydrologyDataset import HydrologyDataset, preprocess_batch_data, parallel_preprocess_batches
from utils import get_discharge_scaler_params, denormalize_discharge, plot_training_curves, plot_prediction_comparison, plot_loss_distribution


def load_config():
    """åŠ è½½é…ç½®"""
    class Config:
        """SCA2LSTMé…ç½®ç±»"""
        def __init__(self, config_dict=None):
            if config_dict is None:
                config_dict = self._load_config_from_file()
            
            # è®¾ç½®æ‰€æœ‰é…ç½®å±æ€§
            self.LSTM1_DERIVED_FEATS = config_dict.get('LSTM1_DERIVED_FEATS', [])
            self.LSTM2_FEATURES = config_dict.get('LSTM2_FEATURES', [])
            self.TARGET_COL = config_dict.get('TARGET_COL', 'discharge_vol')
            self.SEQ_LEN = config_dict.get('SEQ_LEN', 7)
            self.PRED_LEN = config_dict.get('PRED_LEN', 1)
            self.BATCH_SIZE = config_dict.get('BATCH_SIZE', 32)
            self.N_EPOCHS = config_dict.get('N_EPOCHS', 60)
            self.PATIENCE = config_dict.get('PATIENCE', 10)
            self.LR = config_dict.get('LR', 1e-5)
            self.LSTM_HIDDEN_DIM = config_dict.get('LSTM_HIDDEN_DIM', 64)
            self.LSTM_LAYERS = config_dict.get('LSTM_LAYERS', 2)
            self.DROPOUT = config_dict.get('DROPOUT', 0.3)
            self.EMBEDDING_DIM = config_dict.get('EMBEDDING_DIM', 16)
            self.N_FEATURES = len(self.LSTM2_FEATURES)
            self.LSTM1_INPUT_DIM = config_dict.get('LSTM1_INPUT_DIM', 29)
            self.SEED = config_dict.get('SEED', 42)
            self.MODEL_SAVE_PATH = config_dict.get('MODEL_SAVE_PATH', './model_output/sca2lstm.pth')
            self.DATA_INPUT_DIR = config_dict.get('DATA_INPUT_DIR', './model_input_data/')
            self.TRAIN_BASIN_IDS = config_dict.get('TRAIN_BASIN_IDS', [])
            self.VAL_BASIN_IDS = config_dict.get('VAL_BASIN_IDS', [])
            self.MIN_VALID_LABEL_RATIO = config_dict.get('MIN_VALID_LABEL_RATIO', 0.3)
            self.MIN_VALID_ROWS = config_dict.get('MIN_VALID_ROWS', 10000)
            self.DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        def _load_config_from_file(self):
            """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®å‚æ•°"""
            # è·å–å½“å‰æ–‡ä»¶ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(current_dir, 'run.config')
            
            if not os.path.exists(config_path):
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            
            # åˆ›å»ºé…ç½®å‘½åç©ºé—´
            config_namespace = {}
            
            try:
                # è¯»å–å¹¶æ‰§è¡Œé…ç½®æ–‡ä»¶
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_code = f.read()
                
                # åœ¨å®‰å…¨å‘½åç©ºé—´ä¸­æ‰§è¡Œé…ç½®ä»£ç 
                exec(config_code, config_namespace)
                
            except Exception as e:
                raise ImportError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            return config_namespace
        
        def __getstate__(self):
            """æ”¯æŒpickleåºåˆ—åŒ–"""
            return self.__dict__.copy()
        
        def __setstate__(self, state):
            """æ”¯æŒpickleååºåˆ—åŒ–"""
            self.__dict__.update(state)
    return Config()

# ======================== SCA2LSTMæ¨¡å‹ï¼ˆä¿®å¤ç»´åº¦+ä¼˜åŒ–åˆå§‹åŒ–ï¼‰=======================
class SCA2LSTM(nn.Module):
    def __init__(self, config):
        super(SCA2LSTM, self).__init__()
        self.config = config
        self.n_features = config.N_FEATURES
        self.lstm1_input_dim = config.LSTM1_INPUT_DIM
        self.lstm_hidden_dim = config.LSTM_HIDDEN_DIM 
        self.lstm_layers = config.LSTM_LAYERS
        self.dropout = config.DROPOUT
        self.embedding_dim = config.EMBEDDING_DIM
        self.basin_ids = config.TRAIN_BASIN_IDS + config.VAL_BASIN_IDS
        self.basin_num = len(set(self.basin_ids))
        
        # æµåŸŸåµŒå…¥å±‚ï¼ˆæ›´å°çš„åˆå§‹åŒ–æ–¹å·®ï¼‰
        self.basin_embedding = nn.Embedding(
            num_embeddings=self.basin_num,
            embedding_dim=self.embedding_dim,
            padding_idx=-1
        )
        nn.init.normal_(self.basin_embedding.weight, mean=0.0, std=0.001)  # é™ä½æ–¹å·®
        
        # LSTM1ï¼ˆä¼˜åŒ–åˆå§‹åŒ–ï¼‰
        self.lstm1_cell = nn.LSTMCell(
            input_size=self.lstm1_input_dim,
            hidden_size=self.lstm_hidden_dim
        )
        
        # æƒé‡è¾“å‡ºå¤´ï¼ˆæ›´ç¨³å®šçš„æ¿€æ´»ï¼‰
        self.weight_head = nn.Sequential(
            nn.Linear(self.lstm_hidden_dim, 32),
            nn.LeakyReLU(0.01),  # é¿å…ReLUæ­»äº¡é—®é¢˜
            nn.Dropout(self.dropout),
            nn.Linear(32, self.n_features),
            nn.Softmax(dim=-1)
        )
        
        # LSTM2ï¼ˆä¼˜åŒ–åˆå§‹åŒ–ï¼‰
        self.lstm2_input_dim = self.n_features + self.embedding_dim
        self.lstm2_cell = nn.LSTMCell(
            input_size=self.lstm2_input_dim,
            hidden_size=self.lstm_hidden_dim
        )
        
        # é¢„æµ‹å¤´ï¼ˆæ›´ç¨³å®šçš„æ¿€æ´»ï¼‰
        self.predict_head = nn.Sequential(
            nn.Linear(self.lstm_hidden_dim, 16),
            nn.LeakyReLU(0.01),
            nn.Dropout(self.dropout),
            nn.Linear(16, 1)
        )
        
        # æ®‹å·®å½’ä¸€åŒ–å±‚ï¼ˆæ›´å°çš„åˆå§‹åŒ–ï¼‰
        self.residual_norm = nn.LayerNorm(1)
        self._init_weights()

    def _init_weights(self):
        # LSTMæƒé‡åˆå§‹åŒ–ï¼ˆé™ä½æ–¹å·®ï¼‰
        for name, param in self.lstm1_cell.named_parameters():
            if "weight_ih" in name:
                nn.init.xavier_uniform_(param.data, gain=0.1)  # é™ä½å¢ç›Š
            elif "weight_hh" in name:
                nn.init.orthogonal_(param.data, gain=0.1)
            elif "bias" in name:
                param.data.fill_(0.01)  # é™ä½biasåˆå§‹åŒ–å€¼
        
        for name, param in self.lstm2_cell.named_parameters():
            if "weight_ih" in name:
                nn.init.xavier_uniform_(param.data, gain=0.1)
            elif "weight_hh" in name:
                nn.init.orthogonal_(param.data, gain=0.1)
            elif "bias" in name:
                param.data.fill_(0.01)
        
        # å…¨è¿æ¥å±‚åˆå§‹åŒ–
        for module in self.weight_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.1)
                if module.bias is not None:
                    module.bias.data.fill_(0.01)
        
        for module in self.predict_head.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.1)
                if module.bias is not None:
                    module.bias.data.fill_(0.01)
        
        # æ®‹å·®å½’ä¸€åŒ–å±‚åˆå§‹åŒ–
        for name, param in self.residual_norm.named_parameters():
            if "weight" in name:
                nn.init.ones_(param.data) * 0.1  # é™ä½æƒé‡
            elif "bias" in name:
                param.data.fill_(0.0)

    def forward(self, seq_features, lstm1_input, missing_bool, basin_ids, residual, return_weights=False):
        batch_size = seq_features.shape[0]
        seq_len = seq_features.shape[1]
        device = self.config.DEVICE
        
        # ç¡®ä¿æ®‹å·®æ˜¯2ç»´ï¼ˆbatch, 1ï¼‰
        if residual.dim() == 1:
            residual = residual.unsqueeze(-1)  # (batch,) â†’ (batch, 1)
        elif residual.dim() != 2:
            residual = residual.view(-1, 1)  # åŠ¨æ€reshapeä¸º2ç»´
        
        # æµåŸŸåµŒå…¥
        basin_id_to_idx = {bid: idx for idx, bid in enumerate(set(self.basin_ids))}
        basin_indices = torch.tensor([basin_id_to_idx[bid.item()] for bid in basin_ids], dtype=torch.long).to(device)
        basin_embed = self.basin_embedding(basin_indices)  # (batch, 16)
        
        # æ®‹å·®å¤„ç†ï¼ˆå¢åŠ è£å‰ªï¼Œé¿å…å¼‚å¸¸å€¼ï¼‰
        residual_norm = self.residual_norm(residual)  # (batch, 16)
        residual_norm = torch.clamp(residual_norm, min=0.0, max=2.0)  # é™åˆ¶æ®‹å·®èŒƒå›´
        residual_broadcast = residual_norm.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, 7, 16)
        
        lstm2_outputs = []
        
        # å¦‚æœreturn_weights=Trueï¼Œæ”¶é›†æ‰€æœ‰æ—¶é—´æ­¥çš„æƒé‡
        all_feature_weights = torch.zeros(batch_size, seq_len, self.n_features).to(device)
        
        for t in range(seq_len):
            # LSTM1è¾“å…¥å¤„ç†ï¼ˆè£å‰ªæç«¯å€¼ï¼‰
            lstm1_input_t = lstm1_input[:, t, :]  # (batch, 29)
            lstm1_input_t = torch.clamp(lstm1_input_t, min=-3.0, max=3.0)  # ä¸¥æ ¼è£å‰ª
            
            # LSTM1å‰å‘ï¼ˆåˆå§‹éšæ€æ›´å¹³ç¼“ï¼‰
            h1 = torch.zeros(batch_size, self.lstm_hidden_dim, device=device) * 0.01
            c1 = torch.zeros(batch_size, self.lstm_hidden_dim, device=device) * 0.01
            h1, c1 = self.lstm1_cell(lstm1_input_t, (h1, c1))
            h1 = torch.clamp(h1, min=-5.0, max=5.0)
            
            # ç”Ÿæˆæƒé‡ï¼ˆæ›´ç¨³å®šçš„å½’ä¸€åŒ–ï¼‰
            feature_weights = self.weight_head(h1)  # (batch, 19)
            feature_weights = feature_weights * missing_bool[:, t, :]
            weight_sums = feature_weights.sum(dim=-1, keepdim=True) + 1e-8
            feature_weights = feature_weights / weight_sums
            feature_weights = torch.clamp(feature_weights, min=1e-6, max=0.5)  # æ”¾å®½æƒé‡èŒƒå›´
            
            # ä¿å­˜æƒé‡ï¼ˆå¦‚æœéœ€è¦è¿”å›ï¼‰
            if return_weights:
                all_feature_weights[:, t, :] = feature_weights
            
            # LSTM2è¾“å…¥å¤„ç†ï¼ˆè£å‰ªæç«¯å€¼ï¼‰
            seq_features_t = seq_features[:, t, :]  # (batch, 19)
            seq_features_t = torch.clamp(seq_features_t, min=-3.0, max=3.0)
            weighted_features = seq_features_t * feature_weights
            weighted_features = torch.clamp(weighted_features, min=-3.0, max=3.0)
            
            lstm2_input_final = torch.cat([weighted_features, basin_embed], dim=-1)
            lstm2_input_final = torch.clamp(lstm2_input_final, min=-3.0, max=3.0)
            
            # LSTM2å‰å‘ï¼ˆåˆå§‹éšæ€æ›´å¹³ç¼“ï¼‰
            h2 = torch.zeros(batch_size, self.lstm_hidden_dim, device=device) * 0.01
            c2 = torch.zeros(batch_size, self.lstm_hidden_dim, device=device) * 0.01
            h2, c2 = self.lstm2_cell(lstm2_input_final, (h2, c2))
            h2 = torch.clamp(h2, min=-5.0, max=5.0)
            
            lstm2_outputs.append(h2)
        
        # é¢„æµ‹è¾“å‡ºï¼ˆè£å‰ªéè´Ÿï¼‰
        lstm2_last = lstm2_outputs[-1]
        pred = self.predict_head(lstm2_last)
        pred = torch.clamp(pred, min=0.0, max=100.0)  # é™åˆ¶é¢„æµ‹å€¼èŒƒå›´
        
        if return_weights:
            return pred, all_feature_weights
        else:
            return pred

# ======================== è®­ç»ƒ/éªŒè¯å·¥å…·å‡½æ•°ï¼ˆä¿®å¤æ¢¯åº¦æ£€æŸ¥+ä¼˜åŒ–æµç¨‹ï¼‰=======================
def train_one_epoch(model, dataloader, criterion, optimizer, config):
    model.train()
    total_loss = 0.0
    skipped_batches = 0
    prev_residual = torch.zeros(config.BATCH_SIZE, 1).to(config.DEVICE)  # ç¡®ä¿åˆå§‹æ®‹å·®æ˜¯2ç»´
    
    for batch_idx, batch in enumerate(tqdm(dataloader, desc="è®­ç»ƒepoch")):
        # é¢„å¤„ç†æ‰¹æ¬¡æ•°æ®
        processed = preprocess_batch_data(batch, config)
        if processed is None:
            skipped_batches += 1
            continue
        
        # è·å–å¤„ç†åçš„æ•°æ®
        seq_features = processed['seq_features']
        lstm1_input = processed['lstm1_input']
        missing_bool = processed['missing_bool']
        basin_ids = processed['basin_ids']
        target = processed['target']
        
        # è°ƒæ•´æ®‹å·®batchå¤§å°
        current_batch_size = seq_features.shape[0]
        if current_batch_size != prev_residual.shape[0]:
            prev_residual = torch.zeros(current_batch_size, 1).to(config.DEVICE)
        
        # å‰å‘ä¼ æ’­
        try:
            pred = model(
                seq_features=seq_features,
                lstm1_input=lstm1_input,
                missing_bool=missing_bool,
                basin_ids=basin_ids,
                residual=prev_residual,
                return_weights=False  # è®­ç»ƒæ—¶ä¸éœ€è¦è¿”å›æƒé‡
            )
        except Exception as e:
            print(f"âš ï¸  å‰å‘ä¼ æ’­å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            skipped_batches += 1
            continue
        
        # æ£€æŸ¥é¢„æµ‹å€¼
        if torch.isnan(pred).any() or torch.isinf(pred).any():
            skipped_batches += 1
            continue
        
        # åå½’ä¸€åŒ–é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ï¼Œè®¡ç®—çœŸå®æŸå¤±
        try:
            # è·å–å½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰æµåŸŸçš„ç¼©æ”¾å‚æ•°
            pred_denorm_list = []
            target_denorm_list = []
            
            for i in range(current_batch_size):
                basin_id = basin_ids[i].item()
                # è¯»å–è¯¥æµåŸŸçš„ç¼©æ”¾å‚æ•°
                discharge_min, discharge_max = get_discharge_scaler_params(str(basin_id), config.DATA_INPUT_DIR)
                
                # åå½’ä¸€åŒ–é¢„æµ‹å€¼å’Œç›®æ ‡å€¼
                pred_denorm = denormalize_discharge(pred[i].squeeze(), discharge_min, discharge_max)
                target_denorm = denormalize_discharge(target[i].squeeze(), discharge_min, discharge_max)
                
                pred_denorm_list.append(pred_denorm)
                target_denorm_list.append(target_denorm)
            
            # å°†åå½’ä¸€åŒ–åçš„å€¼ç»„åˆæˆå¼ é‡
            pred_denorm_tensor = torch.stack(pred_denorm_list)
            target_denorm_tensor = torch.stack(target_denorm_list)
            
            # è®¡ç®—çœŸå®æŸå¤±ï¼ˆä½¿ç”¨åå½’ä¸€åŒ–åçš„å€¼ï¼‰
            loss = criterion(pred_denorm_tensor, target_denorm_tensor)
            
        except Exception as e:
            # å¦‚æœåå½’ä¸€åŒ–å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨å½’ä¸€åŒ–å€¼è®¡ç®—æŸå¤±
            print(f"âš ï¸  åå½’ä¸€åŒ–å¤±è´¥ï¼Œä½¿ç”¨å½’ä¸€åŒ–å€¼è®¡ç®—æŸå¤±ï¼š{str(e)}")
            loss = criterion(pred, target)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦æ£€æŸ¥ï¼ˆæ”¾å®½é˜ˆå€¼ï¼Œåªè¿‡æ»¤NaN/Infï¼‰
        nan_grad_found = False
        for p in model.parameters():
            if p.grad is not None:
                if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                    nan_grad_found = True
                    break
        
        if nan_grad_found:
            optimizer.zero_grad()
            skipped_batches += 1
            continue
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        
        # ä¼˜åŒ–å™¨æ›´æ–°
        optimizer.step()
        
        total_loss += loss.item() * current_batch_size
        
        # æ›´æ–°æ®‹å·®ï¼ˆä½¿ç”¨ç›¸å¯¹è¯¯å·®ï¼Œæ›´ç¨³å®šï¼‰
        current_residual = torch.abs(pred - target) / (target + 1e-8)  # ç›¸å¯¹è¯¯å·®
        # ç¡®ä¿æ®‹å·®æ˜¯2ç»´ï¼ˆbatch_size, 1ï¼‰
        if current_residual.dim() == 1:
            prev_residual = current_residual.unsqueeze(-1).detach()  # (batch_size,) â†’ (batch_size, 1)
        else:
            prev_residual = current_residual.view(-1, 1).detach()  # ä¿æŒ(batch_size, 1)å½¢çŠ¶
    
    # æ‰“å°è·³è¿‡æ‰¹æ¬¡ç»Ÿè®¡
    if skipped_batches > 0:
        skip_ratio = skipped_batches / len(dataloader) * 100
        print(f"âš ï¸  æœ¬epochè·³è¿‡äº†{skipped_batches}ä¸ªæ‰¹æ¬¡ï¼ˆ{skip_ratio:.1f}%ï¼‰")
    
    avg_loss = total_loss / len(dataloader.dataset) if len(dataloader.dataset) > 0 else float('inf')
    return avg_loss

def validate_one_epoch(model, dataloader, criterion, config, epoch=None):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_targets = []
    all_basin_ids = []  # æ–°å¢ï¼šæ”¶é›†æµåŸŸID
    
    # æ–°å¢ï¼šæ”¶é›†ç‰¹å¾æƒé‡æ•°æ®
    feature_weights_history = {}  # {basin_id: []}
    
    prev_residual = torch.zeros(config.BATCH_SIZE, 1).to(config.DEVICE)  # ç¡®ä¿åˆå§‹æ®‹å·®æ˜¯2ç»´
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="éªŒè¯epoch")):
            # é¢„å¤„ç†æ‰¹æ¬¡æ•°æ®
            processed = preprocess_batch_data(batch, config)
            if processed is None:
                continue
            
            # è·å–å¤„ç†åçš„æ•°æ®
            seq_features = processed['seq_features']
            lstm1_input = processed['lstm1_input']
            missing_bool = processed['missing_bool']
            basin_ids = processed['basin_ids']
            target = processed['target']
            
            # è°ƒæ•´æ®‹å·®batchå¤§å°
            current_batch_size = seq_features.shape[0]
            if current_batch_size != prev_residual.shape[0]:
                prev_residual = torch.zeros(current_batch_size, 1).to(config.DEVICE)
            
            # å‰å‘ä¼ æ’­å¹¶æ”¶é›†æƒé‡ - ä½¿ç”¨return_weights=Trueè·å–æƒé‡
            pred, batch_feature_weights = model(
                seq_features=seq_features,
                lstm1_input=lstm1_input,
                missing_bool=missing_bool,
                basin_ids=basin_ids,
                residual=prev_residual,
                return_weights=True
            )
            
            # è®¡ç®—å¹³å‡æƒé‡ï¼ˆè·¨æ—¶é—´æ­¥ï¼‰
            avg_feature_weights = batch_feature_weights.mean(dim=1)  # (batch, n_features)
            
            # ä¿å­˜æƒé‡æ•°æ®ï¼ˆæŒ‰æµåŸŸåˆ†ç»„ï¼‰
            for i in range(current_batch_size):
                basin_id = basin_ids[i].item()
                if basin_id not in feature_weights_history:
                    feature_weights_history[basin_id] = []
                feature_weights_history[basin_id].append(avg_feature_weights[i].cpu().numpy())
            
            if torch.isnan(pred).any() or torch.isinf(pred).any():
                continue
            
            # åå½’ä¸€åŒ–é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ï¼Œè®¡ç®—çœŸå®æŸå¤±
            try:
                # è·å–å½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰æµåŸŸçš„ç¼©æ”¾å‚æ•°
                pred_denorm_list = []
                target_denorm_list = []
                
                for i in range(current_batch_size):
                    basin_id = basin_ids[i].item()
                    # è¯»å–è¯¥æµåŸŸçš„ç¼©æ”¾å‚æ•°
                    discharge_min, discharge_max = get_discharge_scaler_params(str(basin_id), config.DATA_INPUT_DIR)
                    
                    # åå½’ä¸€åŒ–é¢„æµ‹å€¼å’Œç›®æ ‡å€¼
                    pred_denorm = denormalize_discharge(pred[i].squeeze(), discharge_min, discharge_max)
                    target_denorm = denormalize_discharge(target[i].squeeze(), discharge_min, discharge_max)
                    
                    pred_denorm_list.append(pred_denorm)
                    target_denorm_list.append(target_denorm)
                
                # å°†åå½’ä¸€åŒ–åçš„å€¼ç»„åˆæˆå¼ é‡
                pred_denorm_tensor = torch.stack(pred_denorm_list)
                target_denorm_tensor = torch.stack(target_denorm_list)
                
                # è®¡ç®—çœŸå®æŸå¤±ï¼ˆä½¿ç”¨åå½’ä¸€åŒ–åçš„å€¼ï¼‰
                loss = criterion(pred_denorm_tensor, target_denorm_tensor)
                
            except Exception as e:
                # å¦‚æœåå½’ä¸€åŒ–å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨å½’ä¸€åŒ–å€¼è®¡ç®—æŸå¤±
                print(f"âš ï¸  éªŒè¯é˜¶æ®µåå½’ä¸€åŒ–å¤±è´¥ï¼Œä½¿ç”¨å½’ä¸€åŒ–å€¼è®¡ç®—æŸå¤±ï¼š{str(e)}")
                loss = criterion(pred, target)
            
            total_loss += loss.item() * current_batch_size
            
            # æ›´æ–°æ®‹å·®
            current_residual = torch.abs(pred - target) / (target + 1e-8)
            # ç¡®ä¿æ®‹å·®æ˜¯2ç»´ï¼ˆbatch_size, 1ï¼‰
            if current_residual.dim() == 1:
                prev_residual = current_residual.unsqueeze(-1).detach()  # (batch_size,) â†’ (batch_size, 1)
            else:
                prev_residual = current_residual.view(-1, 1).detach()  # ä¿æŒ(batch_size, 1)å½¢çŠ¶
            
            # æ”¶é›†ç»“æœ
            all_preds.extend(pred.squeeze().cpu().numpy())
            all_targets.extend(target.squeeze().cpu().numpy())
            all_basin_ids.extend(basin_ids.cpu().numpy())  # æ–°å¢ï¼šæ”¶é›†æµåŸŸID
    
    avg_loss = total_loss / len(dataloader.dataset) if len(dataloader.dataset) > 0 else float('inf')
    
    # å¤„ç†æƒé‡æ•°æ® - è®¡ç®—æ¯ä¸ªæµåŸŸçš„å¹³å‡æƒé‡
    final_weights_data = {}
    for basin_id, weights_list in feature_weights_history.items():
        if weights_list:
            # è®¡ç®—è¯¥æµåŸŸåœ¨æ‰€æœ‰batchä¸­çš„å¹³å‡æƒé‡
            basin_weights = np.array(weights_list)
            final_weights_data[basin_id] = basin_weights.mean(axis=0)  # (n_features,)
    
    # å°†æƒé‡æ•°æ®å­˜å‚¨åˆ°configä¸­ï¼Œç”¨äºåç»­å¯è§†åŒ–
    if not hasattr(config, 'feature_weights_history'):
        config.feature_weights_history = {}
    if epoch is not None:
        config.feature_weights_history[epoch] = final_weights_data
    
    # ä½¿ç”¨utils.pyä¸­çš„å¯è§†åŒ–å‡½æ•°
    if epoch is not None and len(all_preds) > 0:
        try:
            # ä½¿ç”¨ä¸è®­ç»ƒè¿‡ç¨‹ç›¸åŒçš„æ—¶é—´æˆ³ç›®å½•
            timestamp = getattr(config, 'VIZ_TIMESTAMP', None)
            if timestamp is None:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            viz_dir = os.path.join('model_output', 'visualizations', timestamp)
            os.makedirs(viz_dir, exist_ok=True)
            
            # ç»˜åˆ¶é¢„æµ‹å¯¹æ¯”å›¾
            plot_prediction_comparison(
                pred_values=all_preds,
                target_values=all_targets,
                basin_ids=all_basin_ids if 'all_basin_ids' in locals() else None,
                epoch=epoch,
                save_dir=viz_dir,
                sample_size=50
            )
            
            # æ¯10ä¸ªepochç»˜åˆ¶ä¸€æ¬¡æŸå¤±åˆ†å¸ƒ
            if (epoch + 1) % 10 == 0:
                individual_losses = []
                # é‡æ–°è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ç”¨äºåˆ†å¸ƒå›¾
                for i in range(min(len(all_preds), 100)):  # é™åˆ¶æ ·æœ¬æ•°é‡
                    pred_val = torch.tensor(all_preds[i])
                    target_val = torch.tensor(all_targets[i])
                    individual_loss = nn.MSELoss()(pred_val, target_val).item()
                    individual_losses.append(individual_loss)
                
                if individual_losses:
                    plot_loss_distribution(individual_losses, epoch=epoch, save_dir=viz_dir)
            
            # æ¯5ä¸ªepochç»˜åˆ¶ä¸€æ¬¡ç‰¹å¾æƒé‡çƒ­åŠ›å›¾
            if (epoch + 1) % 5 == 0 and final_weights_data:
                from utils import plot_feature_weights_heatmap
                plot_feature_weights_heatmap(
                    feature_weights_history=config.feature_weights_history,
                    feature_names=config.LSTM2_FEATURES if hasattr(config, 'LSTM2_FEATURES') else None,
                    save_dir=viz_dir
                )
            
        except Exception as e:
            print(f"âš ï¸  å¯è§†åŒ–å¤±è´¥ï¼š{str(e)}")
    
    return avg_loss

# ======================== ä¸»è®­ç»ƒæµç¨‹ï¼ˆä¼˜åŒ–é…ç½®ï¼‰=======================
def train_sca2lstm(config, use_parallel=True, use_multithreading=True):
    # ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºDataLoaderï¼ˆç®€åŒ–ä»£ç ç»“æ„ï¼‰
    from hydrologyDataset import create_hydrology_dataloaders
    train_dataset, train_loader, val_dataset, val_loader = create_hydrology_dataloaders(config, use_parallel, use_multithreading)
    
    # æ¨¡å‹åˆå§‹åŒ–
    model = SCA2LSTM(config).to(config.DEVICE)
    criterion = nn.MSELoss() 
    # ä¼˜åŒ–å™¨ï¼ˆæ›´ç¨³å®šçš„å‚æ•°ï¼‰
    optimizer = optim.AdamW(model.parameters(), 
                           lr=config.LR, 
                           weight_decay=1e-5,  # é™ä½æƒé‡è¡°å‡
                           betas=(0.9, 0.999),
                           eps=1e-8)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 
                                                    patience=15,  # å»¶é•¿è€å¿ƒå€¼
                                                    factor=0.5,
                                                    threshold=0.001,
                                                    min_lr=1e-7)
    
    best_val_loss = float("inf")
    patience_counter = 0
    best_model_path = config.MODEL_SAVE_PATH
    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
    
    # åˆå§‹åŒ–æŸå¤±å†å²è®°å½•
    train_losses_history = []
    val_losses_history = []
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¯è§†åŒ–ç›®å½•
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    viz_dir = os.path.join('model_output', 'visualizations', timestamp)
    os.makedirs(viz_dir, exist_ok=True)
    print(f"ğŸ“ å¯è§†åŒ–ç›®å½•å·²åˆ›å»º: {viz_dir}")
    
    # å°†æ—¶é—´æˆ³å­˜å‚¨åˆ°configä¸­ï¼Œä¾›éªŒè¯å‡½æ•°ä½¿ç”¨
    config.VIZ_TIMESTAMP = timestamp
    
    print(f"\n{'='*30} å¼€å§‹è®­ç»ƒSCA2LSTM {'='*30}")
    print(f"æ¨¡å‹é…ç½®ï¼š{ {k: v for k, v in config.__dict__.items() if not k.startswith('__') and not callable(v)} }")
    print(f"è®¾å¤‡ï¼š{config.DEVICE}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°ï¼š{len(train_dataset)}ï¼Œè®­ç»ƒæ‰¹æ¬¡ï¼š{len(train_loader)}")
    print(f"éªŒè¯æ ·æœ¬æ•°ï¼š{len(val_dataset)}ï¼ŒéªŒè¯æ‰¹æ¬¡ï¼š{len(val_loader)}")
    print(f"å­¦ä¹ ç‡ï¼š{config.LR}ï¼ŒDropoutï¼š{config.DROPOUT}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.N_EPOCHS):
        print(f"\nğŸ“Œ Epoch {epoch+1}/{config.N_EPOCHS}")
        
        # è®­ç»ƒ
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, config)
        # éªŒè¯
        val_loss = validate_one_epoch(
            model, val_loader, criterion, config, 
            epoch=epoch
        )
        
        # è®°å½•æŸå¤±å†å²
        train_losses_history.append(train_loss)
        val_losses_history.append(val_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # æ‰“å°æ—¥å¿—
        print(f"è®­ç»ƒæŸå¤±ï¼š{train_loss:.6f}ï¼ŒéªŒè¯æŸå¤±ï¼š{val_loss:.6f}")
        print(f"å½“å‰å­¦ä¹ ç‡ï¼š{optimizer.param_groups[0]['lr']:.8f}")
        
        # æ¯5ä¸ªepochç»˜åˆ¶ä¸€æ¬¡æŸå¤±æ›²çº¿
        if (epoch + 1) % 5 == 0 or epoch == config.N_EPOCHS - 1:
            try:
                plot_training_curves(train_losses_history, val_losses_history, save_dir=viz_dir)
                print(f"ğŸ“Š å·²æ›´æ–°æŸå¤±æ›²çº¿å›¾")
            except Exception as e:
                print(f"âš ï¸  æŸå¤±æ›²çº¿ç»˜åˆ¶å¤±è´¥: {str(e)}")
        
        # æ—©åœé€»è¾‘
        if val_loss < best_val_loss - 1e-6:  # å¢åŠ å¾®å°é˜ˆå€¼ï¼Œé¿å…æµ®ç‚¹è¯¯å·®
            best_val_loss = val_loss
            patience_counter = 0
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "best_val_loss": best_val_loss,
                "config": {k: v for k, v in config.__dict__.items() if not k.startswith('__') and not callable(v)}
            }, best_model_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.6f}ï¼‰")
        else:
            patience_counter += 1
            print(f"âš ï¸  æ—©åœè®¡æ•°å™¨ï¼š{patience_counter}/{config.PATIENCE}")
            if patience_counter >= config.PATIENCE:
                print(f"âŒ æ—©åœè§¦å‘ï¼Œè®­ç»ƒç»“æŸ")
                break
    
    # æƒé‡åˆ†æ
    print(f"\n{'='*30} æ¨¡å‹æƒé‡åˆ†æ {'='*30}")
    try:
        checkpoint = torch.load(best_model_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print("ğŸ“Š LSTM1æƒé‡ç»Ÿè®¡:")
        for name, param in model.named_parameters():
            if 'lstm1_cell' in name:
                print(f"   {name}: å‡å€¼={param.mean().item():.6f}, æ ‡å‡†å·®={param.std().item():.6f}")
        
        print("\nğŸ“Š LSTM2æƒé‡ç»Ÿè®¡:")
        for name, param in model.named_parameters():
            if 'lstm2_cell' in name:
                print(f"   {name}: å‡å€¼={param.mean().item():.6f}, æ ‡å‡†å·®={param.std().item():.6f}")
    
    except Exception as e:
        print(f"âš ï¸  æƒé‡åˆ†æå¤±è´¥ï¼š{str(e)}")
    
    print(f"\n{'='*30} è®­ç»ƒå®Œæˆ {'='*30}")
    print(f"æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_val_loss:.6f}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{best_model_path}")
    
    return best_model_path

# ======================== è¿è¡Œè®­ç»ƒ ========================
if __name__ == "__main__":
    import argparse
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='SCA2LSTM è®­ç»ƒè„šæœ¬')
    parser.add_argument('--parallel', action='store_true', help='ä½¿ç”¨å¹¶è¡Œæ•°æ®é›†ï¼ˆå¤šçº¿ç¨‹/å¤šè¿›ç¨‹ï¼‰')
    parser.add_argument('--serial', action='store_true', help='ä½¿ç”¨ä¸²è¡Œæ•°æ®é›†ï¼ˆé»˜è®¤ï¼‰')
    parser.add_argument('--epochs', type=int, default=60, help='è®­ç»ƒè½®æ•°')
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨å¹¶è¡Œ
    use_parallel = args.parallel and not args.serial
    if args.parallel and args.serial:
        print("âš ï¸  åŒæ—¶æŒ‡å®šäº†--parallelå’Œ--serialï¼Œé»˜è®¤ä½¿ç”¨å¹¶è¡Œæ¨¡å¼")
        use_parallel = True
    elif not args.parallel and not args.serial:
        # é»˜è®¤ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆWindowsç³»ç»Ÿä¸‹æ›´å®‰å…¨ï¼‰
        import platform
        is_windows = platform.system() == 'Windows'
        use_parallel = not is_windows  # Windowsä¸‹é»˜è®¤ä¸²è¡Œï¼Œå…¶ä»–ç³»ç»Ÿé»˜è®¤å¹¶è¡Œ
        print(f"ğŸ¯ é»˜è®¤æ¨¡å¼ï¼š{'å¹¶è¡Œ' if use_parallel else 'ä¸²è¡Œ'}æ•°æ®é›†")
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # å›ºå®šéšæœºç§å­
    np.random.seed(config.SEED)
    torch.manual_seed(config.SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.SEED)
    
    try:
        # æ›´æ–°é…ç½®ä¸­çš„epochså‚æ•°
        config.N_EPOCHS = args.epochs
        best_model_path = train_sca2lstm(
            config, 
            use_parallel=use_parallel,
            use_multithreading=True
        )
        print(f"\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path}")
        print(f"ğŸ“ˆ ä½¿ç”¨{'å¹¶è¡Œ' if use_parallel else 'ä¸²è¡Œ'}æ•°æ®é›†å®Œæˆè®­ç»ƒ")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()