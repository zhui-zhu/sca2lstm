import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import json

# ======================== æ ¸å¿ƒé…ç½® ========================
TS_DATA_DIR = "./datasets/CAMELS_GB/CAMELS_GB_timeseries/"
STATIC_DATA_DIR = "./datasets/CAMELS_GB/CAMELS_GB_static/"
OUTPUT_DIR = "./model_input_data/"
CATCHMENT_IDS = [10002, 10003, 22001, 22006, 32006, 39025, 42003, 45003, 51001, 54017, 75003, 79005]

# é™æ€æ•°æ®æ–‡ä»¶åç§°
STATIC_FILES = {
    "topo": "CAMELS_GB_topographic_attributes.csv",
    "clim": "CAMELS_GB_climatic_attributes.csv",
    "land": "CAMELS_GB_landcover_attributes.csv",
    "soil": "CAMELS_GB_soil_attributes.csv",
    "hydro": "CAMELS_GB_hydrologic_attributes.csv",
    "human": "CAMELS_GB_humaninfluence_attributes.csv"
}

# é™æ€æŒ‡æ ‡ç‰©ç†å‚è€ƒèŒƒå›´
STATIC_REF_RANGE = {
    "area": (0, 10000),           # kmÂ²
    "dpsbar": (0, 500),           # m/km
    "elev_mean": (0, 700),        # m
    "aridity": (0, 1),            # å¹²æ—±æŒ‡æ•°
    "p_seasonality": (-1, 1),     # é™æ°´å­£èŠ‚æ€§
    "tawc": (0, 250),             # mm
    "porosity_cosby": (0, 100),   # å­”éš™åº¦ 
    "baseflow_index": (0, 1),     # åŸºæµæŒ‡æ•°
    "dwood_perc": (0, 100),       # è½å¶æ—å æ¯” ï¼ˆ%ï¼‰
    "ewood_perc": (0, 100),       # å¸¸ç»¿æ—å æ¯” ï¼ˆ%ï¼‰
    "grass_perc": (0, 100),       # è‰åœ°å æ¯” ï¼ˆ%ï¼‰
    "urban_perc": (0, 100),       # åŸé•‡å æ¯” ï¼ˆ%ï¼‰
    "inwater_perc": (0, 100),     # æ°´åŸŸå æ¯” ï¼ˆ%ï¼‰
    "benchmark_catch": (0, 1),    # åŸºå‡†æµåŸŸï¼ˆ0/1ï¼‰
    "reservoir_cap": (0, 1e8)     # æ°´åº“åº“å®¹ï¼ˆmÂ³ï¼‰
}

# æ—¶åºæŒ‡æ ‡ç‰©ç†ç¡¬çº¦æŸ(ç»Ÿè®¡æ•°æ®é€‚ç”¨ CAMELS_GB æ•°æ®é›†ï¼Œè‹±å›½åœ°åŒº)
TS_PHYSICAL_CONSTRAINTS = {
    "precipitation": (0, 200),   # æ—¥é™æ°´0-200mmï¼ˆè‹±å›½æ²¿æµ·æç«¯å€¼ï¼‰
    "peti": (0, 15),             # æ—¥è’¸æ•£å‘0-15mm   
    "temperature": (-10, 40),    # æ—¥æ¸©-10~40â„ƒ
    "discharge_vol": (0, 1000),   # æµé‡0-1000mÂ³/s
    # æ–°å¢è¡ç”ŸæŒ‡æ ‡ç‰©ç†çº¦æŸï¼ˆåŸºäºCAMELS_GBæ•°æ®é›†ç»Ÿè®¡ï¼‰
    "high_prec_running_days": (0, 15),  # è¿ç»­é«˜é™æ°´æœ€å¤š15å¤©
    "low_prec_running_days": (0, 60),   # è¿ç»­ä½é™æ°´æœ€å¤š60å¤©
    "prec_7day_sum": (0, 500),          # 7å¤©ç´¯è®¡é™æ°´0-500mm
    "prec_30day_sum": (0, 1000)         # 30å¤©ç´¯è®¡é™æ°´0-1000mm
}

# ======================== æ ¸å¿ƒå·¥å…·å‡½æ•° =======================
def get_ssi(flow_series, window=30):
    """ç”¨åŸå§‹æµé‡ç”Ÿæˆ1ç»´æ—±æ¶åœºæ™¯ï¼ˆ0=æ—±æœŸï¼Œ0.5=æ­£å¸¸ï¼Œ1=æ¶æœŸï¼‰â€”â€”æ–¹æ¡ˆ1æ ¸å¿ƒ"""
    # 30å¤©æ»šåŠ¨çª—å£ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆåŸºäºåŸå§‹æµé‡ï¼Œæœ‰ç‰©ç†æ„ä¹‰ï¼‰
    mean_flow = flow_series.rolling(window=window, min_periods=7).mean()  # è‡³å°‘7ä¸ªæœ‰æ•ˆæ•°æ®æ‰è®¡ç®—
    std_flow = flow_series.rolling(window=window, min_periods=7).std()
    # è®¡ç®—åç¦»åº¦ï¼ˆé¿å…é™¤ä»¥0ï¼‰
    ssi = (flow_series - mean_flow) / (std_flow + 1e-8)
    # è´´æ ‡ç­¾ï¼ˆå›ºå®šé˜ˆå€¼ï¼ŒåŸºäºç‰©ç†æ„ä¹‰çš„åç¦»åº¦ï¼‰
    return np.where(ssi < -1.5, 0, np.where(ssi > 1.5, 1, 0.5)).reshape(-1, 1)

def fill_timeseries_nan(series: pd.Series, is_extreme_context: pd.Series = None) -> pd.Series:
    """æ—¶åºæŒ‡æ ‡ç¼ºå¤±å€¼å¡«å……"""
    series_filled = series.copy()
    nan_mask = series_filled.isna()
    if nan_mask.sum() == 0:
        return series_filled

    # æç«¯äº‹ä»¶å‰åç¼ºå¤±ï¼šç”¨æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå‡å€¼Ã—0.8å¡«å……
    if is_extreme_context is not None:
        extreme_nan_idx = nan_mask & is_extreme_context
        if extreme_nan_idx.sum() > 0:
            extreme_valid_val = series_filled[is_extreme_context & (~nan_mask)].mean() # æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå‡å€¼
            if not pd.isna(extreme_valid_val):
                series_filled.loc[extreme_nan_idx] = extreme_valid_val * 0.8 # æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå‡å€¼Ã—0.8
                nan_mask = series_filled.isna()
                if nan_mask.sum() == 0:
                    return series_filled

    # å•ä¸ªç¼ºå¤±å€¼ï¼šå‰åä¸€å¤©å‡å€¼
    single_nan_idx = []
    for idx in series_filled[nan_mask].index:
        prev_valid = (idx > 0) and (~pd.isna(series_filled.iloc[idx-1]))
        next_valid = (idx < len(series_filled)-1) and (~pd.isna(series_filled.iloc[idx+1]))
        if prev_valid and next_valid:
            single_nan_idx.append(idx)
    
    if single_nan_idx:
        series_filled.loc[single_nan_idx] = (
            series_filled.iloc[[i-1 for i in single_nan_idx]].values +
            series_filled.iloc[[i+1 for i in single_nan_idx]].values
        ) / 2
        nan_mask = series_filled.isna()
        if nan_mask.sum() == 0:
            return series_filled

    # å‰©ä½™è¿ç»­ç¼ºå¤±ï¼š7å¤©æ»šåŠ¨å‡å€¼
    rolling_mean = series_filled.rolling(window=7, center=True, min_periods=3).mean()
    series_filled.loc[nan_mask] = rolling_mean.loc[nan_mask]

    # æç«¯æƒ…å†µï¼šå…¨å±€å‡å€¼å…œåº•
    final_nan_mask = series_filled.isna()
    if final_nan_mask.sum() > 0:
        series_filled.loc[final_nan_mask] = series_filled.dropna().mean()

    return series_filled

def handle_timeseries_outliers(series: pd.Series, indicator: str, catchment_id: int) -> pd.Series:
    """æ—¶åºæŒ‡æ ‡å¼‚å¸¸å€¼å¤„ç†ï¼ˆä¿ç•™çœŸå®æç«¯å€¼ï¼‰"""
    series_clean = series.copy()
    valid_data = series_clean.dropna()
    if len(valid_data) < 10: # æ•°æ®ä¸è¶³10å¤©ï¼Œä¸å¤„ç†
        return series_clean

    # ç‰©ç†ç¡¬çº¦æŸï¼ˆæ–°å¢è¡ç”ŸæŒ‡æ ‡çš„çº¦æŸï¼‰
    min_phys, max_phys = TS_PHYSICAL_CONSTRAINTS.get(indicator, (series.min(), series.max()))
    series_clean = np.clip(series_clean, min_phys, max_phys)

    # æµåŸŸè‡ªé€‚åº”é˜ˆå€¼ï¼ˆå†å²æœ€å¤§Ã—1.2ï¼Œè¡ç”ŸæŒ‡æ ‡åŒæ ·é€‚ç”¨ï¼‰
    historical_max = valid_data.max()
    adaptive_thresh = historical_max * 1.2
    series_clean = np.where(
        (series_clean > adaptive_thresh) & (series_clean <= max_phys),
        adaptive_thresh,
        series_clean
    )

    print(f"æµåŸŸ{catchment_id}-{indicator}ï¼šå†å²æœ€å¤§={historical_max:.2f}ï¼Œè‡ªé€‚åº”é˜ˆå€¼={adaptive_thresh:.2f}")
    return series_clean

def calculate_dynamic_features(ts_data: pd.DataFrame, catchment_id: int) -> pd.DataFrame:
    """ä¼˜åŒ–åçš„æ—¶åºè¡ç”ŸæŒ‡æ ‡è®¡ç®—ï¼ˆé€‚é…CAMELS_GBæ•°æ®é›†ï¼‰"""
    ts_data = ts_data.copy()
    # é¿å…é‡å¤è®¡ç®—è¡ç”Ÿåˆ—ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰
    dynamic_cols = ["high_prec_running_days", "low_prec_running_days", "prec_7day_sum", "prec_30day_sum"]
    existing_dynamic_cols = [col for col in dynamic_cols if col in ts_data.columns]
    if existing_dynamic_cols:
        print(f"âš ï¸  å·²å­˜åœ¨è¡ç”ŸæŒ‡æ ‡ï¼š{existing_dynamic_cols}ï¼Œå°†è¦†ç›–è®¡ç®—")

    # åŸºäºå½“å‰æµåŸŸåŸå§‹é™æ°´çš„åˆ†ä½æ•°è®¾å®šé˜ˆå€¼ï¼ˆæ›´ç§‘å­¦ï¼Œé€‚é…ä¸åŒæµåŸŸï¼‰
    valid_prec = ts_data["precipitation"].dropna()
    if len(valid_prec) < 30:  # é™æ°´æ•°æ®ä¸è¶³30å¤©ï¼Œç”¨å›ºå®šé˜ˆå€¼å…œåº•
        high_prec_thresh = 10  # è‹±å›½æµåŸŸé«˜é™æ°´é˜ˆå€¼é»˜è®¤10mm
        low_prec_thresh = 1    # ä½é™æ°´é˜ˆå€¼é»˜è®¤1mm
        print(f"âš ï¸  æµåŸŸ{catchment_id}é™æ°´æ•°æ®ä¸è¶³30å¤©ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼ï¼ˆé«˜=10mmï¼Œä½=1mmï¼‰")
    else:
        high_prec_thresh = valid_prec.quantile(0.9)  # 90åˆ†ä½æ•°ä¸ºé«˜é™æ°´é˜ˆå€¼
        low_prec_thresh = valid_prec.quantile(0.1)   # 10åˆ†ä½æ•°ä¸ºä½é™æ°´é˜ˆå€¼
        print(f"âœ… æµåŸŸ{catchment_id}é™æ°´é˜ˆå€¼ï¼šé«˜={high_prec_thresh:.2f}mmï¼Œä½={low_prec_thresh:.2f}mm")

    # 1. é«˜é™æ°´æŒç»­å¤©æ•°ï¼ˆè¿ç»­â‰¥é«˜é™æ°´é˜ˆå€¼çš„å¤©æ•°ï¼‰
    ts_data["is_high_prec"] = (ts_data["precipitation"] >= high_prec_thresh).astype(int)
    # è¿ç»­åºåˆ—åˆ†ç»„ï¼šå½“å½“å‰çŠ¶æ€ä¸å‰ä¸€å¤©ä¸åŒæ—¶ï¼Œç”Ÿæˆæ–°åˆ†ç»„
    high_prec_groups = (ts_data["is_high_prec"] != ts_data["is_high_prec"].shift(1)).cumsum()
    # åˆ†ç»„å†…ç´¯è®¡è®¡æ•°ï¼Œéé«˜é™æ°´æ—¥ç½®0
    ts_data["high_prec_running_days"] = ts_data.groupby(high_prec_groups)["is_high_prec"].cumcount() + 1
    ts_data["high_prec_running_days"] = ts_data["high_prec_running_days"] * ts_data["is_high_prec"]

    # 2. ä½é™æ°´æŒç»­å¤©æ•°ï¼ˆè¿ç»­â‰¤ä½é™æ°´é˜ˆå€¼çš„å¤©æ•°ï¼‰
    ts_data["is_low_prec"] = (ts_data["precipitation"] <= low_prec_thresh).astype(int)
    low_prec_groups = (ts_data["is_low_prec"] != ts_data["is_low_prec"].shift(1)).cumsum()
    ts_data["low_prec_running_days"] = ts_data.groupby(low_prec_groups)["is_low_prec"].cumcount() + 1
    ts_data["low_prec_running_days"] = ts_data["low_prec_running_days"] * ts_data["is_low_prec"]

    # 3. 7å¤©ç´¯è®¡é™æ°´ï¼ˆæ»‘åŠ¨çª—å£ï¼Œæœ€å°1å¤©æœ‰æ•ˆæ•°æ®ï¼‰
    ts_data["prec_7day_sum"] = ts_data["precipitation"].rolling(window=7, min_periods=1).sum()

    # 4. 30å¤©ç´¯è®¡é™æ°´ï¼ˆæ»‘åŠ¨çª—å£ï¼Œæœ€å°7å¤©æœ‰æ•ˆæ•°æ®ï¼Œé¿å…å‰æœŸå¤±çœŸï¼‰
    ts_data["prec_30day_sum"] = ts_data["precipitation"].rolling(window=30, min_periods=7).sum()

    # åˆ é™¤ä¸­é—´åˆ—
    ts_data = ts_data.drop(columns=["is_high_prec", "is_low_prec"], errors="ignore")
    print(f"âœ… è¡ç”ŸæŒ‡æ ‡è®¡ç®—å®Œæˆï¼š{dynamic_cols}")
    return ts_data

def normalize_static_feature(value: float, feature_name: str) -> float:
    """é™æ€æŒ‡æ ‡å½’ä¸€åŒ–"""
    if pd.isna(value):
        return np.nan
    min_ref, max_ref = STATIC_REF_RANGE[feature_name]
    if max_ref - min_ref < 1e-8:
        return 0.5
    value_clipped = np.clip(value, min_ref, max_ref)
    value_norm = (value_clipped - min_ref) / (max_ref - min_ref)
    return round(value_norm, 6)

def load_static_data_complete(catchment_id: int) -> pd.DataFrame:
    """å®Œæ•´åŠ è½½15ä¸ªé™æ€æŒ‡æ ‡"""
    static_data = pd.DataFrame({"gauge_id": [catchment_id]})

    # 1. åœ°å½¢æŒ‡æ ‡ï¼ˆarea/dpsbar/elev_meanï¼‰
    topo_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["topo"])
    if os.path.exists(topo_path):
        topo_df = pd.read_csv(topo_path)
        topo_df["gauge_id"] = pd.to_numeric(topo_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in topo_df["gauge_id"].values:
            row = topo_df[topo_df["gauge_id"] == catchment_id].iloc[0]
            static_data["area"] = row.get("area", np.nan)
            static_data["dpsbar"] = row.get("dpsbar", np.nan)
            static_data["elev_mean"] = row.get("elev_mean", np.nan)
        else:
            static_data["area"] = static_data["dpsbar"] = static_data["elev_mean"] = np.nan
    else:
        static_data["area"] = static_data["dpsbar"] = static_data["elev_mean"] = np.nan

    # 2. æ°”å€™æŒ‡æ ‡ï¼ˆaridity/p_seasonalityï¼‰
    clim_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["clim"])
    if os.path.exists(clim_path):
        clim_df = pd.read_csv(clim_path)
        clim_df["gauge_id"] = pd.to_numeric(clim_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in clim_df["gauge_id"].values:
            row = clim_df[clim_df["gauge_id"] == catchment_id].iloc[0]
            static_data["aridity"] = row.get("aridity", np.nan)
            static_data["p_seasonality"] = row.get("p_seasonality", np.nan)
        else:
            static_data["aridity"] = static_data["p_seasonality"] = np.nan
    else:
        static_data["aridity"] = static_data["p_seasonality"] = np.nan

    # 3. åœŸå£¤æŒ‡æ ‡ï¼ˆtawc/porosity_cosbyï¼‰
    soil_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["soil"])
    if os.path.exists(soil_path):
        soil_df = pd.read_csv(soil_path)
        soil_df["gauge_id"] = pd.to_numeric(soil_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in soil_df["gauge_id"].values:
            row = soil_df[soil_df["gauge_id"] == catchment_id].iloc[0]
            static_data["tawc"] = row.get("tawc", np.nan)
            static_data["porosity_cosby"] = row.get("porosity_cosby", np.nan)
        else:
            static_data["tawc"] = static_data["porosity_cosby"] = np.nan
    else:
        static_data["tawc"] = static_data["porosity_cosby"] = np.nan

    # 4. æ°´æ–‡æŒ‡æ ‡ï¼ˆbaseflow_indexï¼‰
    hydro_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["hydro"])
    if os.path.exists(hydro_path):
        hydro_df = pd.read_csv(hydro_path)
        hydro_df["gauge_id"] = pd.to_numeric(hydro_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in hydro_df["gauge_id"].values:
            row = hydro_df[hydro_df["gauge_id"] == catchment_id].iloc[0]
            static_data["baseflow_index"] = row.get("baseflow_index", np.nan)
        else:
            static_data["baseflow_index"] = np.nan
    else:
        static_data["baseflow_index"] = np.nan

    # 5. åœŸåœ°è¦†ç›–æŒ‡æ ‡ï¼ˆå„å æ¯”ï¼‰
    land_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["land"])
    if os.path.exists(land_path):
        land_df = pd.read_csv(land_path)
        land_df["gauge_id"] = pd.to_numeric(land_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in land_df["gauge_id"].values:
            row = land_df[land_df["gauge_id"] == catchment_id].iloc[0]
            static_data["dwood_perc"] = row.get("dwood_perc", np.nan)
            static_data["ewood_perc"] = row.get("ewood_perc", np.nan)
            static_data["grass_perc"] = row.get("grass_perc", np.nan)
            static_data["urban_perc"] = row.get("urban_perc", np.nan)
            static_data["inwater_perc"] = row.get("inwater_perc", np.nan)
        else:
            land_cols = ["dwood_perc", "ewood_perc", "grass_perc", "urban_perc", "inwater_perc"]
            static_data[land_cols] = np.nan
    else:
        land_cols = ["dwood_perc", "ewood_perc", "grass_perc", "urban_perc", "inwater_perc"]
        static_data[land_cols] = np.nan

    # 6. äººç±»å½±å“æŒ‡æ ‡ï¼ˆbenchmark_catch/reservoir_capï¼‰
    human_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["human"])
    if os.path.exists(human_path):
        human_df = pd.read_csv(human_path)
        human_df["gauge_id"] = pd.to_numeric(human_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in human_df["gauge_id"].values:
            row = human_df[human_df["gauge_id"] == catchment_id].iloc[0]
            bench_val = row.get("benchmark_catch", np.nan)
            static_data["benchmark_catch"] = 1 if bench_val == "Y" else 0 if bench_val == "N" else np.nan
            static_data["reservoir_cap"] = row.get("reservoir_cap", np.nan)
        else:
            static_data["benchmark_catch"] = static_data["reservoir_cap"] = np.nan
    else:
        static_data["benchmark_catch"] = static_data["reservoir_cap"] = np.nan

    # ç¡®ä¿æ— é‡å¤åˆ—å
    static_data = static_data.loc[:, ~static_data.columns.duplicated()]
    return static_data

def fill_discharge_nan(ts_data: pd.DataFrame) -> pd.DataFrame:
    """æµé‡ç¼ºå¤±å€¼åˆ†åœºæ™¯å¡«å……"""
    ts_data = ts_data.copy()
    discharge = ts_data["discharge_vol"].copy()
    flood_condition = ts_data["precipitation"] > ts_data["precipitation"].quantile(0.9) # 90%åˆ†ä½æ•°ä¸ºæ´ªæ°´æœŸ

    # éæ´ªæ°´æœŸç¼ºå¤±
    non_flood_nan_idx = discharge[(discharge.isna()) & (~flood_condition)].index
    if len(non_flood_nan_idx) > 0:
        rolling_mean = discharge.rolling(window=7, center=True, min_periods=1).mean() # éæ´ªæ°´æœŸ7å¤©æ»šåŠ¨å‡å€¼
        discharge.loc[non_flood_nan_idx] = rolling_mean.loc[non_flood_nan_idx]

    # æ´ªæ°´æœŸç¼ºå¤±
    flood_nan_idx = discharge[(discharge.isna()) & (flood_condition)].index
    for idx in flood_nan_idx: 
        start_idx = max(0, idx - 3)
        end_idx = min(len(ts_data), idx + 4)
        window_mask = (ts_data.index >= start_idx) & (ts_data.index < end_idx)
        window_flood_mask = window_mask & flood_condition
        
        flood_window_data = discharge[window_flood_mask]
        if not flood_window_data.empty:
            discharge.loc[idx] = flood_window_data.mean()
        else:
            discharge.loc[idx] = discharge[window_mask].mean()

    ts_data["discharge_vol"] = discharge
    return ts_data

# ======================== å•æµåŸŸå¤„ç†ä¸»å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®æ”¹å¤„ï¼‰=======================
def preprocess_single_catchment(catchment_id: int):
    catchment_output_dir = os.path.join(OUTPUT_DIR, str(catchment_id))
    os.makedirs(catchment_output_dir, exist_ok=True)
    print(f"\n{'='*50} å¼€å§‹å¤„ç†æµåŸŸ {catchment_id} {'='*50}")

    # ---------------------- æ­¥éª¤1ï¼šè¯»å–æ—¶åºæ•°æ® ----------------------
    ts_filename = f"CAMELS_GB_hydromet_timeseries_{catchment_id}_19701001-20150930.csv"
    ts_file_path = os.path.join(TS_DATA_DIR, ts_filename)
    if not os.path.exists(ts_file_path):
        print(f"âŒ æµåŸŸ{catchment_id}æ—¶åºæ–‡ä»¶ä¸å­˜åœ¨ï¼š{ts_file_path}ï¼Œè·³è¿‡")
        return

    ts_df = pd.read_csv(ts_file_path)
    required_ts_cols = ["date", "precipitation", "peti", "temperature", "discharge_vol"]
    missing_cols = [col for col in required_ts_cols[1:] if col not in ts_df.columns]
    if missing_cols:
        print(f"âš ï¸  æ—¶åºæ–‡ä»¶ç¼ºå°‘æŒ‡æ ‡ï¼š{missing_cols}ï¼Œä»…ç”¨ç°æœ‰æŒ‡æ ‡")
    used_ts_cols = [col for col in required_ts_cols if col in ts_df.columns]
    ts_data = ts_df[used_ts_cols].copy()

    # æ—¥æœŸæ ¼å¼åŒ–+å»é‡
    ts_data["date"] = pd.to_datetime(ts_data["date"], errors="coerce")
    ts_data = ts_data.dropna(subset=["date"]).drop_duplicates("date").reset_index(drop=True)
    print(f"âœ… æ—¶åºæ•°æ®è¯»å–å®Œæˆï¼š{len(ts_data)} æ¡è®°å½•")

    # ---------------------- æ­¥éª¤2ï¼šæ—¶åºæ•°æ®é¢„å¤„ç†ï¼ˆç¼ºå¤±å€¼+å¼‚å¸¸å€¼ï¼‰----------------------
    # å®šä¹‰æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡ï¼ˆ90%ä»¥ä¸Šé™æ°´ä¸ºæç«¯äº‹ä»¶ï¼‰
    extreme_prec_thresh = ts_data["precipitation"].dropna().quantile(0.9) if "precipitation" in ts_data.columns else 0
    is_extreme_context = ts_data["precipitation"] >= extreme_prec_thresh if "precipitation" in ts_data.columns else pd.Series(False, index=ts_data.index)

    # ç¼ºå¤±å€¼å¡«å……ï¼ˆå…ˆå¡«å……åŸºç¡€æ—¶åºæŒ‡æ ‡ï¼‰
    for col in ["precipitation", "peti", "temperature"]:
        if col in ts_data.columns:
            ts_data[col] = fill_timeseries_nan(ts_data[col], is_extreme_context)
            print(f"âœ… {col}ç¼ºå¤±å€¼å¡«å……å®Œæˆ")

    # æµé‡ç¼ºå¤±å€¼å¡«å……ï¼ˆSSIåŸºäºæµé‡è®¡ç®—ï¼Œå¿…é¡»å…ˆå¡«å……ï¼‰
    if "discharge_vol" in ts_data.columns:
        ts_data = fill_discharge_nan(ts_data)
        print(f"âœ… æµé‡ç¼ºå¤±å€¼å¡«å……å®Œæˆ")

    # å¼‚å¸¸å€¼å¤„ç†ï¼ˆåŸºç¡€æ—¶åºæŒ‡æ ‡ï¼‰
    ts_num_cols = [col for col in used_ts_cols if col != "date"]
    for col in ts_num_cols:
        if col in TS_PHYSICAL_CONSTRAINTS:
            ts_data[col] = handle_timeseries_outliers(ts_data[col], col, catchment_id)
    print(f"âœ… åŸºç¡€æ—¶åºæŒ‡æ ‡å¼‚å¸¸å€¼å¤„ç†å®Œæˆ")

    # ---------------------- æ­¥éª¤3ï¼šè®¡ç®—4ä¸ªè¡ç”ŸæŒ‡æ ‡ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰----------------------
    dynamic_cols = []
    if "precipitation" in ts_data.columns:
        # ä¼ å…¥æµåŸŸIDï¼Œä¾¿äºæ—¥å¿—å’Œé˜ˆå€¼é€‚é…
        ts_data = calculate_dynamic_features(ts_data, catchment_id)
        dynamic_cols = ["high_prec_running_days", "low_prec_running_days", "prec_7day_sum", "prec_30day_sum"]
        dynamic_cols = [col for col in dynamic_cols if col in ts_data.columns]  # è¿‡æ»¤å®é™…å­˜åœ¨çš„è¡ç”Ÿåˆ—
        
        # è¡ç”ŸæŒ‡æ ‡å¼‚å¸¸å€¼å¤„ç†ï¼ˆæ–°å¢ï¼‰
        for col in dynamic_cols:
            if col in TS_PHYSICAL_CONSTRAINTS:
                ts_data[col] = handle_timeseries_outliers(ts_data[col], col, catchment_id)
        print(f"âœ… è¡ç”ŸæŒ‡æ ‡è®¡ç®—+å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼š{dynamic_cols}")
    else:
        print(f"âš ï¸  æ— é™æ°´æ•°æ®ï¼Œæœªè®¡ç®—è¡ç”ŸæŒ‡æ ‡")

    # ---------------------- æ­¥éª¤4ï¼šè®¡ç®—SSIæ—±æ¶åœºæ™¯ ----------------------
    if "discharge_vol" in ts_data.columns:
        # ç”¨é¢„å¤„ç†åçš„åŸå§‹æµé‡ï¼ˆå·²å¡«å……ç¼ºå¤±å€¼ã€å¤„ç†å¼‚å¸¸å€¼ï¼‰è®¡ç®—SSI
        ssi = get_ssi(ts_data["discharge_vol"], window=30)
        ts_data["ssi"] = ssi  # æ·»åŠ SSIåˆ—ï¼ˆå€¼ä¸º0/0.5/1ï¼ŒåŸºäºåŸå§‹æµé‡ï¼‰
        dynamic_cols.append("ssi")  # å°†SSIçº³å…¥æ—¶åºç‰¹å¾ï¼Œåç»­ä¸€èµ·å½’ä¸€åŒ–
        print(f"âœ… SSIæ—±æ¶åœºæ™¯è®¡ç®—å®Œæˆï¼ˆåŸºäºåŸå§‹æµé‡ï¼‰ï¼š0=æ—±æœŸï¼Œ0.5=æ­£å¸¸æœŸï¼Œ1=æ¶æœŸ")
    else:
        print(f"âš ï¸  æ— æµé‡æ•°æ®ï¼Œæœªè®¡ç®—SSI")

    # ---------------------- æ­¥éª¤5ï¼šæ—¶åºæŒ‡æ ‡å½’ä¸€åŒ–ï¼ˆåŒ…å«è¡ç”ŸæŒ‡æ ‡+SSIï¼‰----------------------
    # æ‰€æœ‰æ—¶åºç‰¹å¾ï¼šåŸºç¡€æŒ‡æ ‡ + è¡ç”ŸæŒ‡æ ‡ + SSI
    all_ts_cols = list(set(ts_num_cols + dynamic_cols))  # å»é‡
    all_ts_cols = [col for col in all_ts_cols if col in ts_data.columns]  # ç¡®ä¿åˆ—å­˜åœ¨
    ts_scaler_params = {}
    ts_normalized = pd.DataFrame(index=ts_data.index)  # ä¿æŒç´¢å¼•ä¸€è‡´

    for col in all_ts_cols:
        min_val = ts_data[col].min()
        max_val = ts_data[col].max()
        ts_scaler_params[col] = {"min": float(min_val), "max": float(max_val)}
        # Min-Maxå½’ä¸€åŒ–ï¼ˆä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰
        if max_val - min_val < 1e-8:
            ts_normalized[col] = 0.5
        else:
            ts_normalized[col] = (ts_data[col] - min_val) / (max_val - min_val)

    # ä¿å­˜æ—¶åºç¼©æ”¾å™¨ï¼ˆåŒ…å«è¡ç”ŸæŒ‡æ ‡å’ŒSSIçš„ç¼©æ”¾å‚æ•°ï¼‰
    ts_scaler_path = os.path.join(catchment_output_dir, f"ts_scaler_{catchment_id}.json")
    with open(ts_scaler_path, "w") as f:
        json.dump(ts_scaler_params, f, indent=2)
    print(f"âœ… æ—¶åºå½’ä¸€åŒ–å®Œæˆï¼ˆå«{len(all_ts_cols)}ä¸ªæŒ‡æ ‡ï¼šåŸºç¡€+è¡ç”Ÿ+SSIï¼‰ï¼Œç¼©æ”¾å™¨ä¿å­˜è‡³ï¼š{ts_scaler_path}")

    # ---------------------- æ­¥éª¤6ï¼šé™æ€æ•°æ®é¢„å¤„ç† ----------------------
    static_raw = load_static_data_complete(catchment_id)
    static_cols = list(STATIC_REF_RANGE.keys())
    static_processed = pd.DataFrame(index=[0])  # æ˜ç¡®ç´¢å¼•

    # é™æ€æŒ‡æ ‡å½’ä¸€åŒ–ï¼ˆç¡®ä¿æ— é‡å¤åˆ—ï¼‰
    for col in static_cols:
        if col not in static_processed.columns:  # é¿å…é‡å¤æ·»åŠ 
            if col in static_raw.columns:
                raw_val = static_raw[col].iloc[0]
                norm_val = normalize_static_feature(raw_val, col)
                static_processed[col] = [norm_val]
            else:
                static_processed[col] = [np.nan]

    # ä¿å­˜é™æ€å‚è€ƒèŒƒå›´
    static_scaler_path = os.path.join(catchment_output_dir, f"static_scaler_{catchment_id}.json")
    with open(static_scaler_path, "w") as f:
        json.dump(STATIC_REF_RANGE, f, indent=2)
    print(f"âœ… é™æ€æ•°æ®å½’ä¸€åŒ–å®Œæˆï¼Œå‚è€ƒèŒƒå›´ä¿å­˜è‡³ï¼š{static_scaler_path}")

    # ---------------------- æ­¥éª¤7ï¼šæ•°æ®èåˆ ----------------------
    # 1. æ”¶é›†æ‰€æœ‰è¦æ‹¼æ¥çš„DataFrame
    date_df = ts_data[["date"]].copy()
    catchment_df = pd.DataFrame({"catchment_id": [catchment_id]*len(ts_data)})

    # 2. æ£€æŸ¥æ‰€æœ‰åˆ—åæ˜¯å¦é‡å¤ï¼ˆé¿å…æ—¶åºåˆ—ä¸é™æ€åˆ—å†²çªï¼‰
    all_cols = (
        date_df.columns.tolist() +
        ts_normalized.columns.tolist() +
        static_processed.columns.tolist() +
        catchment_df.columns.tolist()
    )
    duplicate_cols = [col for col in set(all_cols) if all_cols.count(col) > 1]
    if duplicate_cols:
        print(f"âš ï¸  å‘ç°é‡å¤åˆ—åï¼š{duplicate_cols}ï¼Œè‡ªåŠ¨ç»™é™æ€åˆ—æ·»åŠ å‰ç¼€")
        # é‡å‘½åé™æ€åˆ—ï¼ˆæ·»åŠ "static_"å‰ç¼€ï¼Œé¿å…ä¸æ—¶åºåˆ—å†²çªï¼‰
        static_processed = static_processed.rename(columns={col: f"static_{col}" for col in duplicate_cols if col in static_processed.columns})

    # 3. é™æ€æ•°æ®é‡å¤åˆ°æ—¶åºé•¿åº¦ï¼ˆä¿æŒç´¢å¼•ä¸€è‡´ï¼‰
    static_repeated = pd.DataFrame(
        np.tile(static_processed.values, (len(ts_data), 1)),
        columns=static_processed.columns,
        index=ts_data.index
    )

    # 4. æ‹¼æ¥æ‰€æœ‰æ•°æ®
    try:
        final_data = pd.concat([
            date_df,
            ts_normalized,  # åŒ…å«å½’ä¸€åŒ–åçš„åŸºç¡€æ—¶åºã€è¡ç”ŸæŒ‡æ ‡ã€SSI
            static_repeated,  # å½’ä¸€åŒ–åçš„é™æ€æŒ‡æ ‡ï¼ˆå¸¦å‰ç¼€ï¼‰
            catchment_df
        ], axis=1, verify_integrity=True)
    except ValueError as e:
        print(f"âŒ æ‹¼æ¥å¤±è´¥ï¼š{str(e)}")
        print(f"å„éƒ¨åˆ†åˆ—åï¼š")
        print(f"- æ—¥æœŸåˆ—ï¼š{date_df.columns.tolist()}")
        print(f"- æ—¶åºåˆ—ï¼ˆå«è¡ç”Ÿ+SSIï¼‰ï¼š{ts_normalized.columns.tolist()}")
        print(f"- é™æ€åˆ—ï¼š{static_repeated.columns.tolist()}")
        print(f"- æµåŸŸIDåˆ—ï¼š{catchment_df.columns.tolist()}")
        raise e

    # ---------------------- æ­¥éª¤8ï¼šä¿å­˜æœ€ç»ˆç»“æœ ----------------------
    final_output_path = os.path.join(catchment_output_dir, f"model_input_{catchment_id}.csv")
    final_data.to_csv(final_output_path, index=False, na_rep="NaN")
    print(f"âœ… æœ€ç»ˆæ•°æ®ä¿å­˜è‡³ï¼š{final_output_path}")
    print(f"ğŸ“Š æ•°æ®ç»´åº¦ï¼š{final_data.shape}ï¼ˆæ—¶é—´æ­¥Ã—æŒ‡æ ‡æ•°ï¼‰")
    print(f"ğŸ“‹ åŒ…å«æŒ‡æ ‡ï¼š{len(ts_normalized.columns)}ä¸ªæ—¶åºæŒ‡æ ‡ï¼ˆ{len(ts_num_cols)}åŸºç¡€+{len(dynamic_cols)}è¡ç”Ÿ/SSIï¼‰ + {len(static_repeated.columns)}ä¸ªé™æ€æŒ‡æ ‡")

    print(f"{'='*50} æµåŸŸ {catchment_id} å¤„ç†å®Œæˆ {'='*50}\n")

# ======================== ä¸»æµç¨‹ï¼ˆæ‰¹é‡å¤„ç†ï¼‰========================
if __name__ == "__main__":
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"å¼€å§‹å¤„ç† {len(CATCHMENT_IDS)} ä¸ªæµåŸŸ...\n")

    for catchment_id in tqdm(CATCHMENT_IDS, desc="æ•´ä½“å¤„ç†è¿›åº¦", unit="æµåŸŸ"):
        try:
            preprocess_single_catchment(catchment_id)
        except Exception as e:
            print(f"âŒ æµåŸŸ{catchment_id}å¤„ç†å¼‚å¸¸ï¼š{str(e)}ï¼Œè·³è¿‡")
            continue

    print(f"\næ‰€æœ‰æµåŸŸå¤„ç†å®Œæˆï¼æœ€ç»ˆæ•°æ®å­˜æ”¾äºï¼š{OUTPUT_DIR}")