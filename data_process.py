import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import json

# ======================== æ ¸å¿ƒé…ç½® ========================
TS_DATA_DIR = "./datasets/CAMELS_GB/CAMELS_GB_timeseries/"
STATIC_DATA_DIR = "./datasets/CAMELS_GB/CAMELS_GB_static/"
OUTPUT_DIR = "./model_input_data/"
CATCHMENT_IDS = [10002, 10003, 22001, 22006, 32006, 39025, 42003, 45003, 51001, 54017, 75003, 79005]

# é™æ€æ•°æ®æ–‡ä»¶åç§°
STATIC_FILES = {
    "topo": "CAMELS_GB_topographic_attributes.csv",
    "clim": "CAMELS_GB_climatic_attributes.csv",
    "land": "CAMELS_GB_landcover_attributes.csv",
    "soil": "CAMELS_GB_soil_attributes.csv",
    "hydro": "CAMELS_GB_hydrologic_attributes.csv",
    "human": "CAMELS_GB_humaninfluence_attributes.csv"
}

# é™æ€æŒ‡æ ‡ç‰©ç†å‚è€ƒèŒƒå›´
STATIC_REF_RANGE = {
    "area": (0, 10000),           # kmÂ²
    "dpsbar": (0, 500),           # m/km
    "elev_mean": (0, 700),        # m
    "aridity": (0, 1),            # å¹²æ—±æŒ‡æ•°
    "p_seasonality": (-1, 1),     # é™æ°´å­£èŠ‚æ€§
    "tawc": (0, 250),             # mm
    "porosity_cosby": (0, 100),   # å­”éš™åº¦ 
    "baseflow_index": (0, 1),     # åŸºæµæŒ‡æ•°
    "dwood_perc": (0, 100),       # è½å¶æ—å æ¯” ï¼ˆ%ï¼‰
    "ewood_perc": (0, 100),       # å¸¸ç»¿æ—å æ¯” ï¼ˆ%ï¼‰
    "grass_perc": (0, 100),       # è‰åœ°å æ¯” ï¼ˆ%ï¼‰
    "urban_perc": (0, 100),       # åŸé•‡å æ¯” ï¼ˆ%ï¼‰
    "inwater_perc": (0, 100),     # æ°´åŸŸå æ¯” ï¼ˆ%ï¼‰
    "benchmark_catch": (0, 1),    # åŸºå‡†æµåŸŸï¼ˆ0/1ï¼‰
    "reservoir_cap": (0, 1e8)     # æ°´åº“åº“å®¹ï¼ˆmÂ³ï¼‰
}

# æ—¶åºæŒ‡æ ‡ç‰©ç†ç¡¬çº¦æŸ(ç»Ÿè®¡æ•°æ®é€‚ç”¨ CAMELS_GB æ•°æ®é›†ï¼Œè‹±å›½åœ°åŒº)
TS_PHYSICAL_CONSTRAINTS = {
    "precipitation": (0, 200),   # æ—¥é™æ°´0-200mmï¼ˆæ²¿æµ·å¯æ”¾å®½è‡³800ï¼‰
    "peti": (0, 15),             # æ—¥è’¸æ•£å‘0-15mm   
    "temperature": (-10, 40),    # æ—¥æ¸©-10~40â„ƒ
    "discharge_vol": (0, 1000)     # æµé‡0-1000mÂ³/s
}

# ======================== æ ¸å¿ƒå·¥å…·å‡½æ•° ========================
def fill_timeseries_nan(series: pd.Series, is_extreme_context: pd.Series = None) -> pd.Series:
    """æ—¶åºæŒ‡æ ‡ç¼ºå¤±å€¼å¡«å……"""
    series_filled = series.copy()
    nan_mask = series_filled.isna()
    if nan_mask.sum() == 0:
        return series_filled

    # æç«¯äº‹ä»¶å‰åç¼ºå¤±ï¼šç”¨æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå‡å€¼Ã—0.8å¡«å……
    if is_extreme_context is not None:
        extreme_nan_idx = nan_mask & is_extreme_context
        if extreme_nan_idx.sum() > 0:
            extreme_valid_val = series_filled[is_extreme_context & (~nan_mask)].mean() # æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå‡å€¼
            if not pd.isna(extreme_valid_val):
                series_filled.loc[extreme_nan_idx] = extreme_valid_val * 0.8 # æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆå‡å€¼Ã—0.8
                nan_mask = series_filled.isna()
                if nan_mask.sum() == 0:
                    return series_filled

    # å•ä¸ªç¼ºå¤±å€¼ï¼šå‰åä¸€å¤©å‡å€¼
    single_nan_idx = []
    for idx in series_filled[nan_mask].index:
        prev_valid = (idx > 0) and (~pd.isna(series_filled.iloc[idx-1]))
        next_valid = (idx < len(series_filled)-1) and (~pd.isna(series_filled.iloc[idx+1]))
        if prev_valid and next_valid:
            single_nan_idx.append(idx)
    
    if single_nan_idx:
        series_filled.loc[single_nan_idx] = (
            series_filled.iloc[[i-1 for i in single_nan_idx]].values +
            series_filled.iloc[[i+1 for i in single_nan_idx]].values
        ) / 2
        nan_mask = series_filled.isna()
        if nan_mask.sum() == 0:
            return series_filled

    # å‰©ä½™è¿ç»­ç¼ºå¤±ï¼š7å¤©æ»šåŠ¨å‡å€¼
    rolling_mean = series_filled.rolling(window=7, center=True, min_periods=3).mean()
    series_filled.loc[nan_mask] = rolling_mean.loc[nan_mask]

    # æç«¯æƒ…å†µï¼šå…¨å±€å‡å€¼å…œåº•
    final_nan_mask = series_filled.isna()
    if final_nan_mask.sum() > 0:
        series_filled.loc[final_nan_mask] = series_filled.dropna().mean()

    return series_filled

def handle_timeseries_outliers(series: pd.Series, indicator: str, catchment_id: int) -> pd.Series:
    """æ—¶åºæŒ‡æ ‡å¼‚å¸¸å€¼å¤„ç†ï¼ˆä¿ç•™çœŸå®æç«¯å€¼ï¼‰"""
    series_clean = series.copy()
    valid_data = series_clean.dropna()
    if len(valid_data) < 10: # æ•°æ®ä¸è¶³10å¤©ï¼Œä¸å¤„ç†
        return series_clean

    # ç‰©ç†ç¡¬çº¦æŸ
    min_phys, max_phys = TS_PHYSICAL_CONSTRAINTS[indicator]
    series_clean = np.clip(series_clean, min_phys, max_phys)

    # æµåŸŸè‡ªé€‚åº”é˜ˆå€¼ï¼ˆå†å²æœ€å¤§Ã—1.2ï¼‰
    historical_max = valid_data.max()
    adaptive_thresh = historical_max * 1.2
    series_clean = np.where(
        (series_clean > adaptive_thresh) & (series_clean <= max_phys),
        adaptive_thresh,
        series_clean
    )

    print(f"æµåŸŸ{catchment_id}-{indicator}ï¼šå†å²æœ€å¤§={historical_max:.2f}ï¼Œè‡ªé€‚åº”é˜ˆå€¼={adaptive_thresh:.2f}")
    return series_clean

def calculate_dynamic_features(ts_data: pd.DataFrame) -> pd.DataFrame:
    """è®¡ç®—æ—¶åºè¡ç”ŸæŒ‡æ ‡"""
    ts_data = ts_data.copy()
    # é¿å…é‡å¤è®¡ç®—è¡ç”Ÿåˆ—ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰
    dynamic_cols = ["high_prec_running_days", "low_prec_running_days", "prec_7day_sum", "prec_30day_sum"]

    # é«˜é™æ°´æŒç»­å¤©æ•°
    daily_mean_prec = ts_data["precipitation"].dropna().mean()
    high_prec_thresh = daily_mean_prec * 3 # é«˜é™æ°´é˜ˆå€¼ï¼ˆ3å€æ—¥é™æ°´å‡å€¼ï¼‰
    ts_data["is_high_prec"] = (ts_data["precipitation"] >= high_prec_thresh).astype(int)
    ts_data["high_prec_running_days"] = ts_data["is_high_prec"].groupby(
        (ts_data["is_high_prec"] != ts_data["is_high_prec"].shift()).cumsum()
    ).cumsum() * ts_data["is_high_prec"]

    # å¹²æ—±æŒç»­å¤©æ•°
    ts_data["is_low_prec"] = (ts_data["precipitation"] < 1).astype(int) # å¹²æ—±é˜ˆå€¼ï¼ˆ1mmï¼‰
    ts_data["low_prec_running_days"] = ts_data["is_low_prec"].groupby(
        (ts_data["is_low_prec"] != ts_data["is_low_prec"].shift()).cumsum()
    ).cumsum() * ts_data["is_low_prec"]

    # ç´¯è®¡é™æ°´
    ts_data["prec_7day_sum"] = ts_data["precipitation"].rolling(window=7, min_periods=1).sum()
    ts_data["prec_30day_sum"] = ts_data["precipitation"].rolling(window=30, min_periods=7).sum()

    # åˆ é™¤ä¸­é—´åˆ—
    ts_data = ts_data.drop(columns=["is_high_prec", "is_low_prec"], errors="ignore")
    return ts_data

def normalize_static_feature(value: float, feature_name: str) -> float:
    """é™æ€æŒ‡æ ‡å½’ä¸€åŒ–"""
    if pd.isna(value):
        return np.nan
    min_ref, max_ref = STATIC_REF_RANGE[feature_name]
    if max_ref - min_ref < 1e-8:
        return 0.5
    value_clipped = np.clip(value, min_ref, max_ref)
    value_norm = (value_clipped - min_ref) / (max_ref - min_ref)
    return round(value_norm, 6)

def load_static_data_complete(catchment_id: int) -> pd.DataFrame:
    """å®Œæ•´åŠ è½½15ä¸ªé™æ€æŒ‡æ ‡"""
    static_data = pd.DataFrame({"gauge_id": [catchment_id]})

    # 1. åœ°å½¢æŒ‡æ ‡ï¼ˆarea/dpsbar/elev_meanï¼‰
    topo_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["topo"])
    if os.path.exists(topo_path):
        topo_df = pd.read_csv(topo_path)
        topo_df["gauge_id"] = pd.to_numeric(topo_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in topo_df["gauge_id"].values:
            row = topo_df[topo_df["gauge_id"] == catchment_id].iloc[0]
            static_data["area"] = row.get("area", np.nan)
            static_data["dpsbar"] = row.get("dpsbar", np.nan)
            static_data["elev_mean"] = row.get("elev_mean", np.nan)
        else:
            static_data["area"] = static_data["dpsbar"] = static_data["elev_mean"] = np.nan
    else:
        static_data["area"] = static_data["dpsbar"] = static_data["elev_mean"] = np.nan

    # 2. æ°”å€™æŒ‡æ ‡ï¼ˆaridity/p_seasonalityï¼‰
    clim_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["clim"])
    if os.path.exists(clim_path):
        clim_df = pd.read_csv(clim_path)
        clim_df["gauge_id"] = pd.to_numeric(clim_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in clim_df["gauge_id"].values:
            row = clim_df[clim_df["gauge_id"] == catchment_id].iloc[0]
            static_data["aridity"] = row.get("aridity", np.nan)
            static_data["p_seasonality"] = row.get("p_seasonality", np.nan)
        else:
            static_data["aridity"] = static_data["p_seasonality"] = np.nan
    else:
        static_data["aridity"] = static_data["p_seasonality"] = np.nan

    # 3. åœŸå£¤æŒ‡æ ‡ï¼ˆtawc/porosity_cosbyï¼‰
    soil_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["soil"])
    if os.path.exists(soil_path):
        soil_df = pd.read_csv(soil_path)
        soil_df["gauge_id"] = pd.to_numeric(soil_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in soil_df["gauge_id"].values:
            row = soil_df[soil_df["gauge_id"] == catchment_id].iloc[0]
            static_data["tawc"] = row.get("tawc", np.nan)
            static_data["porosity_cosby"] = row.get("porosity_cosby", np.nan)
        else:
            static_data["tawc"] = static_data["porosity_cosby"] = np.nan
    else:
        static_data["tawc"] = static_data["porosity_cosby"] = np.nan

    # 4. æ°´æ–‡æŒ‡æ ‡ï¼ˆbaseflow_indexï¼‰
    hydro_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["hydro"])
    if os.path.exists(hydro_path):
        hydro_df = pd.read_csv(hydro_path)
        hydro_df["gauge_id"] = pd.to_numeric(hydro_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in hydro_df["gauge_id"].values:
            row = hydro_df[hydro_df["gauge_id"] == catchment_id].iloc[0]
            static_data["baseflow_index"] = row.get("baseflow_index", np.nan)
        else:
            static_data["baseflow_index"] = np.nan
    else:
        static_data["baseflow_index"] = np.nan

    # 5. åœŸåœ°è¦†ç›–æŒ‡æ ‡ï¼ˆå„å æ¯”ï¼‰
    land_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["land"])
    if os.path.exists(land_path):
        land_df = pd.read_csv(land_path)
        land_df["gauge_id"] = pd.to_numeric(land_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in land_df["gauge_id"].values:
            row = land_df[land_df["gauge_id"] == catchment_id].iloc[0]
            static_data["dwood_perc"] = row.get("dwood_perc", np.nan)
            static_data["ewood_perc"] = row.get("ewood_perc", np.nan)
            static_data["grass_perc"] = row.get("grass_perc", np.nan)
            static_data["urban_perc"] = row.get("urban_perc", np.nan)
            static_data["inwater_perc"] = row.get("inwater_perc", np.nan)
        else:
            land_cols = ["dwood_perc", "ewood_perc", "grass_perc", "urban_perc", "inwater_perc"]
            static_data[land_cols] = np.nan
    else:
        land_cols = ["dwood_perc", "ewood_perc", "grass_perc", "urban_perc", "inwater_perc"]
        static_data[land_cols] = np.nan

    # 6. äººç±»å½±å“æŒ‡æ ‡ï¼ˆbenchmark_catch/reservoir_capï¼‰
    human_path = os.path.join(STATIC_DATA_DIR, STATIC_FILES["human"])
    if os.path.exists(human_path):
        human_df = pd.read_csv(human_path)
        human_df["gauge_id"] = pd.to_numeric(human_df["gauge_id"], errors="coerce").fillna(-1).astype(int)
        if catchment_id in human_df["gauge_id"].values:
            row = human_df[human_df["gauge_id"] == catchment_id].iloc[0]
            bench_val = row.get("benchmark_catch", np.nan)
            static_data["benchmark_catch"] = 1 if bench_val == "Y" else 0 if bench_val == "N" else np.nan
            static_data["reservoir_cap"] = row.get("reservoir_cap", np.nan)
        else:
            static_data["benchmark_catch"] = static_data["reservoir_cap"] = np.nan
    else:
        static_data["benchmark_catch"] = static_data["reservoir_cap"] = np.nan

    # ç¡®ä¿æ— é‡å¤åˆ—å
    static_data = static_data.loc[:, ~static_data.columns.duplicated()]
    return static_data

def fill_discharge_nan(ts_data: pd.DataFrame) -> pd.DataFrame:
    """æµé‡ç¼ºå¤±å€¼åˆ†åœºæ™¯å¡«å……"""
    ts_data = ts_data.copy()
    discharge = ts_data["discharge_vol"].copy()
    flood_condition = ts_data["precipitation"] > ts_data["precipitation"].quantile(0.9) # 90%åˆ†ä½æ•°ä¸ºæ´ªæ°´æœŸ

    # éæ´ªæ°´æœŸç¼ºå¤±
    non_flood_nan_idx = discharge[(discharge.isna()) & (~flood_condition)].index
    if len(non_flood_nan_idx) > 0:
        rolling_mean = discharge.rolling(window=7, center=True, min_periods=1).mean() # éæ´ªæ°´æœŸ7å¤©æ»šåŠ¨å‡å€¼
        discharge.loc[non_flood_nan_idx] = rolling_mean.loc[non_flood_nan_idx]

    # æ´ªæ°´æœŸç¼ºå¤±
    flood_nan_idx = discharge[(discharge.isna()) & (flood_condition)].index
    for idx in flood_nan_idx: 
        start_idx = max(0, idx - 3)
        end_idx = min(len(ts_data), idx + 4)
        window_mask = (ts_data.index >= start_idx) & (ts_data.index < end_idx)
        window_flood_mask = window_mask & flood_condition
        
        flood_window_data = discharge[window_flood_mask]
        if not flood_window_data.empty:
            discharge.loc[idx] = flood_window_data.mean()
        else:
            discharge.loc[idx] = discharge[window_mask].mean()

    ts_data["discharge_vol"] = discharge
    return ts_data

# ======================== å•æµåŸŸå¤„ç†ä¸»å‡½æ•° ========================
def preprocess_single_catchment(catchment_id: int):
    catchment_output_dir = os.path.join(OUTPUT_DIR, str(catchment_id))
    os.makedirs(catchment_output_dir, exist_ok=True)
    print(f"\n{'='*50} å¼€å§‹å¤„ç†æµåŸŸ {catchment_id} {'='*50}")

    # ---------------------- æ­¥éª¤1ï¼šè¯»å–æ—¶åºæ•°æ® ----------------------
    ts_filename = f"CAMELS_GB_hydromet_timeseries_{catchment_id}_19701001-20150930.csv"
    ts_file_path = os.path.join(TS_DATA_DIR, ts_filename)
    if not os.path.exists(ts_file_path):
        print(f"âŒ æµåŸŸ{catchment_id}æ—¶åºæ–‡ä»¶ä¸å­˜åœ¨ï¼š{ts_file_path}ï¼Œè·³è¿‡")
        return

    ts_df = pd.read_csv(ts_file_path)
    required_ts_cols = ["date", "precipitation", "peti", "temperature", "discharge_vol"]
    missing_cols = [col for col in required_ts_cols[1:] if col not in ts_df.columns]
    if missing_cols:
        print(f"âš ï¸  æ—¶åºæ–‡ä»¶ç¼ºå°‘æŒ‡æ ‡ï¼š{missing_cols}ï¼Œä»…ç”¨ç°æœ‰æŒ‡æ ‡")
    used_ts_cols = [col for col in required_ts_cols if col in ts_df.columns]
    ts_data = ts_df[used_ts_cols].copy()

    # æ—¥æœŸæ ¼å¼åŒ–+
    ts_data["date"] = pd.to_datetime(ts_data["date"], errors="coerce")
    ts_data = ts_data.dropna(subset=["date"]).drop_duplicates("date").reset_index(drop=True)
    print(f"âœ… æ—¶åºæ•°æ®è¯»å–å®Œæˆï¼š{len(ts_data)} æ¡è®°å½•")

    # ---------------------- æ­¥éª¤2ï¼šæ—¶åºæ•°æ®é¢„å¤„ç† ----------------------
    # å®šä¹‰æç«¯äº‹ä»¶ä¸Šä¸‹æ–‡ï¼ˆ90%ä»¥ä¸Šé™æ°´ä¸ºæç«¯äº‹ä»¶ï¼‰
    extreme_prec_thresh = ts_data["precipitation"].dropna().quantile(0.9) if "precipitation" in ts_data.columns else 0
    is_extreme_context = ts_data["precipitation"] >= extreme_prec_thresh if "precipitation" in ts_data.columns else pd.Series(False, index=ts_data.index)

    # ç¼ºå¤±å€¼å¡«å……
    for col in ["precipitation", "peti", "temperature"]:
        if col in ts_data.columns:
            ts_data[col] = fill_timeseries_nan(ts_data[col], is_extreme_context)
            print(f"âœ… {col}ç¼ºå¤±å€¼å¡«å……å®Œæˆ")

    # æµé‡ç¼ºå¤±å€¼å¡«å……
    if "discharge_vol" in ts_data.columns:
        ts_data = fill_discharge_nan(ts_data)
        print(f"âœ… æµé‡ç¼ºå¤±å€¼å¡«å……å®Œæˆ")

    # å¼‚å¸¸å€¼å¤„ç†
    ts_num_cols = [col for col in used_ts_cols if col != "date"]
    for col in ts_num_cols:
        if col in TS_PHYSICAL_CONSTRAINTS:
            ts_data[col] = handle_timeseries_outliers(ts_data[col], col, catchment_id)
    print(f"âœ… æ—¶åºå¼‚å¸¸å€¼å¤„ç†å®Œæˆ")

    # ---------------------- æ­¥éª¤3ï¼šè®¡ç®—æ—¶åºè¡ç”ŸæŒ‡æ ‡ ----------------------
    dynamic_cols = []
    if "precipitation" in ts_data.columns:
        ts_data = calculate_dynamic_features(ts_data)
        dynamic_cols = ["high_prec_running_days", "low_prec_running_days", "prec_7day_sum", "prec_30day_sum"]
        dynamic_cols = [col for col in dynamic_cols if col in ts_data.columns]  # è¿‡æ»¤å®é™…å­˜åœ¨çš„è¡ç”Ÿåˆ—
        print(f"âœ… è¡ç”ŸæŒ‡æ ‡è®¡ç®—å®Œæˆï¼š{dynamic_cols}")
    else:
        print(f"âš ï¸  æ— é™æ°´æ•°æ®ï¼Œæœªè®¡ç®—è¡ç”ŸæŒ‡æ ‡")

    # ---------------------- æ­¥éª¤4ï¼šæ—¶åºæŒ‡æ ‡å½’ä¸€åŒ– ----------------------
    all_ts_cols = list(set(ts_num_cols + dynamic_cols))  # å»é‡
    all_ts_cols = [col for col in all_ts_cols if col in ts_data.columns]  # ç¡®ä¿åˆ—å­˜åœ¨
    ts_scaler_params = {}
    ts_normalized = pd.DataFrame(index=ts_data.index)  # æ˜ç¡®ç´¢å¼•

    for col in all_ts_cols:
        min_val = ts_data[col].min()
        max_val = ts_data[col].max()
        ts_scaler_params[col] = {"min": float(min_val), "max": float(max_val)}
        # å½’ä¸€åŒ–
        if max_val - min_val < 1e-8:
            ts_normalized[col] = 0.5
        else:
            ts_normalized[col] = (ts_data[col] - min_val) / (max_val - min_val)

    # ä¿å­˜æ—¶åºç¼©æ”¾å™¨
    ts_scaler_path = os.path.join(catchment_output_dir, f"ts_scaler_{catchment_id}.json")
    with open(ts_scaler_path, "w") as f:
        json.dump(ts_scaler_params, f, indent=2)
    print(f"âœ… æ—¶åºå½’ä¸€åŒ–å®Œæˆï¼Œç¼©æ”¾å™¨ä¿å­˜è‡³ï¼š{ts_scaler_path}")

    # ---------------------- æ­¥éª¤5ï¼šé™æ€æ•°æ®é¢„å¤„ç† ----------------------
    static_raw = load_static_data_complete(catchment_id)
    static_cols = list(STATIC_REF_RANGE.keys())
    static_processed = pd.DataFrame(index=[0])  # æ˜ç¡®ç´¢å¼•

    # é™æ€æŒ‡æ ‡å½’ä¸€åŒ–ï¼ˆç¡®ä¿æ— é‡å¤åˆ—ï¼‰
    for col in static_cols:
        if col not in static_processed.columns:  # é¿å…é‡å¤æ·»åŠ 
            if col in static_raw.columns:
                raw_val = static_raw[col].iloc[0]
                norm_val = normalize_static_feature(raw_val, col)
                static_processed[col] = [norm_val]
            else:
                static_processed[col] = [np.nan]

    # ä¿å­˜é™æ€å‚è€ƒèŒƒå›´
    static_scaler_path = os.path.join(catchment_output_dir, f"static_scaler_{catchment_id}.json")
    with open(static_scaler_path, "w") as f:
        json.dump(STATIC_REF_RANGE, f, indent=2)
    print(f"âœ… é™æ€æ•°æ®å½’ä¸€åŒ–å®Œæˆï¼Œå‚è€ƒèŒƒå›´ä¿å­˜è‡³ï¼š{static_scaler_path}")

    # ---------------------- æ­¥éª¤6ï¼šæ•°æ®èåˆï¼ˆå…³é”®ä¿®å¤ï¼šé¿å…åˆ—åé‡å¤ï¼‰ ----------------------
    # 1. æ”¶é›†æ‰€æœ‰è¦æ‹¼æ¥çš„DataFrame
    date_df = ts_data[["date"]].copy()
    catchment_df = pd.DataFrame({"catchment_id": [catchment_id]*len(ts_data)})

    # 2. æ£€æŸ¥æ‰€æœ‰åˆ—åæ˜¯å¦é‡å¤
    all_cols = (
        date_df.columns.tolist() +
        ts_normalized.columns.tolist() +
        static_processed.columns.tolist() +
        catchment_df.columns.tolist()
    )
    duplicate_cols = [col for col in set(all_cols) if all_cols.count(col) > 1]
    if duplicate_cols:
        print(f"âš ï¸  å‘ç°é‡å¤åˆ—åï¼š{duplicate_cols}ï¼Œè‡ªåŠ¨é‡å‘½å")
        # é‡å‘½åé‡å¤åˆ—ï¼ˆç»™é™æ€åˆ—æ·»åŠ å‰ç¼€"static_"ï¼‰
        static_processed = static_processed.rename(columns={col: f"static_{col}" for col in duplicate_cols if col in static_processed.columns})
        # ç»™æ—¶åºåˆ—æ·»åŠ å‰ç¼€"ts_"ï¼ˆå¦‚æœä»æœ‰é‡å¤ï¼‰
        remaining_dups = [col for col in set(
            date_df.columns.tolist() + ts_normalized.columns.tolist() + static_processed.columns.tolist() + catchment_df.columns.tolist()
        ) if all_cols.count(col) > 1]
        if remaining_dups:
            ts_normalized = ts_normalized.rename(columns={col: f"ts_{col}" for col in remaining_dups if col in ts_normalized.columns})

    # 3. é™æ€æ•°æ®é‡å¤åˆ°æ—¶åºé•¿åº¦
    static_repeated = pd.DataFrame(
        np.tile(static_processed.values, (len(ts_data), 1)),
        columns=static_processed.columns,
        index=ts_data.index  # ç¡®ä¿ç´¢å¼•ä¸€è‡´
    )

    # 4. æ‹¼æ¥æ‰€æœ‰æ•°æ®ï¼ˆæ·»åŠ verify_integrity=Trueä¾¿äºè°ƒè¯•ï¼‰
    try:
        final_data = pd.concat([
            date_df,
            ts_normalized,
            static_repeated,
            catchment_df
        ], axis=1, verify_integrity=True)
    except ValueError as e:
        print(f"âŒ æ‹¼æ¥å¤±è´¥ï¼š{str(e)}")
        print(f"å½“å‰å„éƒ¨åˆ†åˆ—åï¼š")
        print(f"- æ—¥æœŸåˆ—ï¼š{date_df.columns.tolist()}")
        print(f"- æ—¶åºåˆ—ï¼š{ts_normalized.columns.tolist()}")
        print(f"- é™æ€åˆ—ï¼š{static_repeated.columns.tolist()}")
        print(f"- æµåŸŸIDåˆ—ï¼š{catchment_df.columns.tolist()}")
        raise e

    # ---------------------- æ­¥éª¤7ï¼šä¿å­˜æœ€ç»ˆç»“æœ ----------------------
    final_output_path = os.path.join(catchment_output_dir, f"model_input_{catchment_id}.csv")
    final_data.to_csv(final_output_path, index=False, na_rep="NaN")
    print(f"âœ… æœ€ç»ˆæ•°æ®ä¿å­˜è‡³ï¼š{final_output_path}")
    print(f"ğŸ“Š æ•°æ®ç»´åº¦ï¼š{final_data.shape}ï¼ˆæ—¶é—´æ­¥Ã—æŒ‡æ ‡æ•°ï¼‰")
    print(f"ğŸ“‹ åŒ…å«æŒ‡æ ‡ï¼š{len(ts_normalized.columns)}ä¸ªæ—¶åºæŒ‡æ ‡ + {len(static_repeated.columns)}ä¸ªé™æ€æŒ‡æ ‡")

    print(f"{'='*50} æµåŸŸ {catchment_id} å¤„ç†å®Œæˆ {'='*50}\n")

# ======================== ä¸»æµç¨‹ï¼ˆæ‰¹é‡å¤„ç†ï¼‰========================
if __name__ == "__main__":
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"å¼€å§‹å¤„ç† {len(CATCHMENT_IDS)} ä¸ªæµåŸŸ...\n")

    for catchment_id in tqdm(CATCHMENT_IDS, desc="æ•´ä½“å¤„ç†è¿›åº¦", unit="æµåŸŸ"):
        try:
            preprocess_single_catchment(catchment_id)
        except Exception as e:
            print(f"âŒ æµåŸŸ{catchment_id}å¤„ç†å¼‚å¸¸ï¼š{str(e)}ï¼Œè·³è¿‡")
            continue

    print(f"\næ‰€æœ‰æµåŸŸå¤„ç†å®Œæˆï¼æœ€ç»ˆæ•°æ®å­˜æ”¾äºï¼š{OUTPUT_DIR}")